{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0d59ccc",
   "metadata": {},
   "source": [
    "# Homework 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8502d7",
   "metadata": {},
   "source": [
    "## Imports and Utilities\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b784e863",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import (Callable, Iterable, List, Sequence, Tuple, Dict, Optional,\n",
    "                    Any, Union)\n",
    "\n",
    "from abc import abstractmethod\n",
    "import collections\n",
    "import itertools\n",
    "import functools\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "########## Graph-Search-Related Utilities and Class Definitions ##########\n",
    "\n",
    "State = Any\n",
    "Action = Any\n",
    "\n",
    "StateSeq = List[State]\n",
    "ActionSeq = List[State]\n",
    "CostSeq = RewardSeq = List[float]\n",
    "\n",
    "\n",
    "class Problem(object):\n",
    "  \"\"\"The abstract base class for either a path cost problem or a reward problem.\"\"\"\n",
    "\n",
    "  def __init__(self, initial: State):\n",
    "    self.initial = initial\n",
    "\n",
    "  @abstractmethod\n",
    "  def actions(self, state: State) -> Iterable[Action]:\n",
    "    \"\"\"Returns the allowed actions in a given state. \n",
    "\n",
    "    The result would typically be a list. But if there are many actions, \n",
    "    consider yielding them one at a time in an iterator, \n",
    "    rather than building them all at once.\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "  @abstractmethod\n",
    "  def step(self, state: State, action: Action) -> State:\n",
    "    \"\"\"Returns the next state when executing a given action in a given state. \n",
    "\n",
    "    The action must be one of self.actions(state).\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "\n",
    "class PathCostProblem(Problem):\n",
    "  \"\"\"An abstract class for a path cost problem, based on AIMA.\n",
    "\n",
    "  To formalize a path cost problem, you should subclass from this and implement \n",
    "  the abstract methods. \n",
    "  Then you will create instances of your subclass and solve them with the \n",
    "  various search functions.\n",
    "  \"\"\"\n",
    "\n",
    "  @abstractmethod\n",
    "  def goal_test(self, state: State) -> bool:\n",
    "    \"\"\"Checks if the state is a goal.\"\"\"\n",
    "    ...\n",
    "\n",
    "  @abstractmethod\n",
    "  def step_cost(self, state1: State, action: Action, state2: State) -> float:\n",
    "    \"\"\"Returns the cost incurred at state2 from state1 via action.\"\"\"\n",
    "    ...\n",
    "\n",
    "  def h(self, state: State) -> float:\n",
    "    \"\"\"Returns the heuristic value, a lower bound on the distance to goal.\"\"\"\n",
    "    return 0\n",
    "\n",
    "\n",
    "class RewardProblem(Problem):\n",
    "  \"\"\"An abstract class for a finite-horizon reward problem, based on AIMA.\n",
    "\n",
    "  To formalize a reward problem, you should subclass from this and implement \n",
    "  the abstract methods. \n",
    "  Then you will create instances of your subclass and solve them with the \n",
    "  various search functions.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, initial: State, horizon: int):\n",
    "    self.initial = initial\n",
    "    self.horizon = horizon\n",
    "\n",
    "  @abstractmethod\n",
    "  def reward(self, state1: State, action: Action, state2: State) -> float:\n",
    "    \"\"\"Returns the reward given at state2 from state1 via action.\n",
    "\n",
    "    A reward at each step must be no greater than `self.rmax`.\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "  @property\n",
    "  @abstractmethod\n",
    "  def rmax(self) -> float:\n",
    "    \"\"\"Returns the maximum reward per step.\"\"\"\n",
    "    ...\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6a5195",
   "metadata": {},
   "source": [
    "## Best-first Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1903cad",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec93d1be",
   "metadata": {},
   "source": [
    "\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eaf09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GridProblem(PathCostProblem):\n",
    "  \"\"\"A grid problem.\"\"\"\n",
    "\n",
    "  def __init__(self, initial=(0, 0), goal=(4, 4)):\n",
    "    super().__init__(initial)\n",
    "    self.goal = goal\n",
    "    self.all_grid_actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "    self.grid_act_to_delta = {\n",
    "        \"up\": (-1, 0),\n",
    "        \"down\": (1, 0),\n",
    "        \"left\": (0, -1),\n",
    "        \"right\": (0, 1)\n",
    "    }\n",
    "    # Somewhat unusual cost structure, depends on s', which is determined by s,a\n",
    "    self.grid_arrival_costs = np.array(\n",
    "        [\n",
    "            [1, 1, 8, 1, 1],\n",
    "            [1, 8, 1, 1, 1],\n",
    "            [1, 8, 1, 1, 1],\n",
    "            [1, 1, 1, 8, 1],\n",
    "            [1, 1, 2, 1, 1],\n",
    "        ],\n",
    "        dtype=int,\n",
    "    )\n",
    "\n",
    "  def actions(self, state):\n",
    "    (r, c) = state\n",
    "    actions = []\n",
    "    for act in self.all_grid_actions:\n",
    "      dr, dc = self.grid_act_to_delta[act]\n",
    "      new_r, new_c = r + dr, c + dc\n",
    "      # Check if in bounds\n",
    "      if (0 <= new_r < self.grid_arrival_costs.shape[0] and\n",
    "          0 <= new_c < self.grid_arrival_costs.shape[1]):\n",
    "        actions.append(act)\n",
    "    return actions\n",
    "\n",
    "  def step(self, state, action):\n",
    "    (r, c) = state\n",
    "    dr, dc = self.grid_act_to_delta[action]\n",
    "    return (r + dr, c + dc)\n",
    "\n",
    "  def goal_test(self, state):\n",
    "    return state == self.goal\n",
    "\n",
    "  def step_cost(self, state1, action, state2):\n",
    "    return self.grid_arrival_costs[state2]\n",
    "\n",
    "  def h(self, state):\n",
    "    \"\"\"Manhattan distance.\"\"\"\n",
    "    return abs(state[0] - self.goal[0]) + abs(state[1] - self.goal[1])\n",
    "\n",
    "\n",
    "import contextlib\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def count_method_calls(problem: Problem, *meths: str):\n",
    "  \"\"\"Track number of method invocations to a problem.\n",
    "\n",
    "  Args:\n",
    "    problem: an instance of a Problem.\n",
    "    meths: a sequence of names for the methods to track call counts for.\n",
    "\n",
    "  Example:\n",
    "    >>> problem = GridProblem()\n",
    "    >>> with count_method_calls(problem, \"step\") as counters:\n",
    "    ...   problem.step((0, 0), \"down\")\n",
    "    ...   problem.step((1, 1), \"up\")\n",
    "    ...   assert counters[\"step\"][((0, 0), \"down\")] == counters[\"step\"][((1, 1), \"up\")]  == 1\n",
    "    ...   assert counters[\"step\"][\"total\"] == 2\n",
    "  \"\"\"\n",
    "  counters = {meth: collections.Counter() for meth in meths}\n",
    "\n",
    "  orig_problem_methods = {meth: getattr(problem, meth) for meth in meths}\n",
    "\n",
    "  def meth_helper(attr, *args):\n",
    "    counters[attr][args] += 1\n",
    "    counters[attr][\"total\"] += 1\n",
    "    return orig_problem_methods[attr](*args)\n",
    "\n",
    "  for meth in meths:\n",
    "    orig_problem_methods[meth] = getattr(problem, meth)\n",
    "    setattr(problem, meth, functools.partial(meth_helper, meth))\n",
    "\n",
    "  try:\n",
    "    yield counters\n",
    "  finally:\n",
    "    for meth in meths:\n",
    "      setattr(problem, meth, orig_problem_methods[meth])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30194326",
   "metadata": {},
   "source": [
    "\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962e11e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import heapq as hq  # Can use this as a priority queue\n",
    "\n",
    "# A useful data structure for best-first search\n",
    "Node = collections.namedtuple(\"Node\",\n",
    "                              [\"state\", \"parent\", \"action\", \"cost\", \"g\"])\n",
    "\n",
    "\n",
    "class SearchFailed(ValueError):\n",
    "  \"\"\"Raise this exception whenever a search must fail.\"\"\"\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89366c76",
   "metadata": {},
   "source": [
    "### Question\n",
    "Complete an implementation of the best-first search, encompassing A*, GBFS, or UCS. \n",
    "      You can assume any heuristics are consistent.\n",
    "      You should follow the psuedocode given in lecture closely. \n",
    "      In particular, your implementation should prune redundant paths by remembering the reached states.\n",
    "\n",
    "For reference, our solution is **55** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f9eeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_best_first_search(\n",
    "    problem: PathCostProblem,\n",
    "    get_priority: Callable[[Node], float],\n",
    "    step_budget: int = 1000) -> Tuple[StateSeq, ActionSeq, CostSeq, int]:\n",
    "  \"\"\"A generic heuristic search implementation.\n",
    "\n",
    "  Depending on `get_priority`, can implement A*, GBFS, or UCS.\n",
    "\n",
    "  The `get_priority` function here should determine the order\n",
    "  in which nodes are expanded. For example, if you want to\n",
    "  use path cost as part of this determination, then the\n",
    "  path cost (node.g) should appear inside of get_priority,\n",
    "  rather than in this implementation of `run_best_first_search`.\n",
    "\n",
    "  Important: for determinism (and to make sure our tests pass),\n",
    "  please break ties using the state itself. For example,\n",
    "  if you would've otherwise sorted by `get_priority(node)`, you\n",
    "  should now sort by `(get_priority(node), node.state)`.\n",
    "\n",
    "  Args:\n",
    "    problem: a path cost problem.\n",
    "    get_priority: a callable taking in a search Node and returns the priority\n",
    "    step_budget: maximum number of `problem.step` before giving up.\n",
    "\n",
    "  Returns:\n",
    "    state_sequence: A list of states.\n",
    "    action_sequence: A list of actions.\n",
    "    cost_sequence: A list of costs.\n",
    "    num_steps: number of taken `problem.step`s. Must be less than or equal to `step_budget`.\n",
    "\n",
    "  Raises:\n",
    "    error: SearchFailed, if no plan is found.\n",
    "  \"\"\"\n",
    "  raise SearchFailed(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0b4b4b",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f665a06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will test this implementation more thoroughly with the\n",
    "# specific heuristic search algorithms that follow\n",
    "grid_problem = GridProblem()\n",
    "get_priority_fn = lambda node: 0\n",
    "result = run_best_first_search(grid_problem, get_priority_fn)\n",
    "assert len(result) == 4\n",
    "\n",
    "\n",
    "def best_first_search_test2():\n",
    "  # We will test this implementation more thoroughly with the\n",
    "  # specific heuristic search algorithms that follow\n",
    "  grid_problem = GridProblem()\n",
    "  get_priority_fn = lambda node: 0\n",
    "  with count_method_calls(grid_problem, \"step\", \"actions\") as counters:\n",
    "    state_sequence, action_sequence, cost_sequence, num_steps = run_best_first_search(\n",
    "        grid_problem, get_priority_fn)\n",
    "    assert (counters[\"step\"].pop(\"total\") == num_steps\n",
    "           ), \"Incorrect report of number of `problem.step`s\"\n",
    "\n",
    "  # Textbook implementation\n",
    "  try:\n",
    "    assert state_sequence == [(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (1, 4),\n",
    "                              (2, 4), (3, 4), (4, 4)]\n",
    "    assert action_sequence == [\n",
    "        'right', 'right', 'right', 'right', 'down', 'down', 'down', 'down'\n",
    "    ]\n",
    "    assert cost_sequence == [1.0, 8.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "  # Alternative implementation that tracks best-cost-to-nodes\n",
    "  except AssertionError:\n",
    "    assert state_sequence == [(0, 0), (1, 0), (2, 0), (3, 0), (3, 1), (3, 2),\n",
    "                              (4, 2), (4, 3), (4, 4)]\n",
    "    assert action_sequence == [\n",
    "        'down', 'down', 'down', 'right', 'right', 'down', 'right', 'right'\n",
    "    ]\n",
    "    assert cost_sequence == [1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0]\n",
    "  assert num_steps <= 252\n",
    "\n",
    "best_first_search_test2()\n",
    "\n",
    "\n",
    "def best_first_search_test3():\n",
    "  \"\"\"If your results do not match the expected ones, make sure that you are \n",
    "  tie-breaking as described in the docstring for `run_best_first_search`.\"\"\"\n",
    "  grid_problem = GridProblem()\n",
    "  get_priority_fn = lambda node: node.g\n",
    "  with count_method_calls(grid_problem, \"step\", \"actions\") as counters:\n",
    "    state_sequence, action_sequence, cost_sequence, num_steps = run_best_first_search(\n",
    "        grid_problem, get_priority_fn)\n",
    "    assert (counters[\"step\"].pop(\"total\") == num_steps\n",
    "           ), \"Incorrect report of number of `problem.step`s\"\n",
    "  assert state_sequence == [(0, 0), (1, 0), (2, 0), (3, 0), (3, 1), (3, 2),\n",
    "                            (4, 2), (4, 3), (4, 4)]\n",
    "  assert action_sequence == [\n",
    "      'down', 'down', 'down', 'right', 'right', 'down', 'right', 'right'\n",
    "  ]\n",
    "  assert cost_sequence == [1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0]\n",
    "  assert num_steps <= 70\n",
    "\n",
    "best_first_search_test3()\n",
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff0f5e2",
   "metadata": {},
   "source": [
    "## Uniform Cost Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134ad054",
   "metadata": {},
   "source": [
    "### Question\n",
    "Use your implementation of `run_best_first_search` to implement uniform cost search.\n",
    "\n",
    "For reference, our solution is **3** line(s) of code.\n",
    "\n",
    "In addition to all of the utilities defined at the top of the colab notebook, the following functions are available in this question environment: `run_best_first_search`. You may not need to use all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756dcc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_uniform_cost_search(problem: PathCostProblem, step_budget: int = 1000):\n",
    "  \"\"\"Uniform-cost search.\n",
    "\n",
    "  Use your implementation of `run_best_first_search`.\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e369c9",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8f75b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ucs_test1():\n",
    "  # If your results do not match the expected ones, make sure that you are tiebreaking\n",
    "  # as described in the docstring for `run_best_first_search`.\n",
    "  grid_problem = GridProblem()\n",
    "  state_sequence, action_sequence, cost_sequence, num_steps = run_uniform_cost_search(\n",
    "      grid_problem)\n",
    "  assert state_sequence == [(0, 0), (1, 0), (2, 0), (3, 0), (3, 1), (3, 2),\n",
    "                            (4, 2), (4, 3), (4, 4)]\n",
    "  assert action_sequence == [\n",
    "      'down', 'down', 'down', 'right', 'right', 'down', 'right', 'right'\n",
    "  ]\n",
    "  assert cost_sequence == [1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0]\n",
    "  assert num_steps <= 70\n",
    "\n",
    "ucs_test1()\n",
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8070d2",
   "metadata": {},
   "source": [
    "## A* Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b27896",
   "metadata": {},
   "source": [
    "### Question\n",
    "Use your implementation of `run_best_first_search` to implement A* search.\n",
    "\n",
    "For reference, our solution is **3** line(s) of code.\n",
    "\n",
    "In addition to all of the utilities defined at the top of the colab notebook, the following functions are available in this question environment: `run_best_first_search`. You may not need to use all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b3f403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_astar_search(problem: PathCostProblem, step_budget: int = 1000):\n",
    "  \"\"\"A* search.\n",
    "\n",
    "  Use your implementation of `run_best_first_search`.\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc46e50",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af88edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def astar_test1():\n",
    "  \"\"\"If your results do not match the expected ones, make sure that you are tiebreaking \n",
    "  as described in the docstring for `run_best_first_search`.\"\"\"\n",
    "  grid_problem = GridProblem()\n",
    "  state_sequence, action_sequence, cost_sequence, num_steps = run_astar_search(\n",
    "      grid_problem)\n",
    "  assert state_sequence == [(0, 0), (1, 0), (2, 0), (3, 0), (3, 1), (3, 2),\n",
    "                            (4, 2), (4, 3), (4, 4)]\n",
    "  assert action_sequence == [\n",
    "      'down', 'down', 'down', 'right', 'right', 'down', 'right', 'right'\n",
    "  ]\n",
    "  assert cost_sequence == [1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0]\n",
    "  assert num_steps <= 36\n",
    "\n",
    "astar_test1()\n",
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbbc5bd",
   "metadata": {},
   "source": [
    "## Greedy Best-First Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3797f1fb",
   "metadata": {},
   "source": [
    "### Question\n",
    "Use your implementation of `run_best_first_search` to implement GBFS.\n",
    "\n",
    "For reference, our solution is **3** line(s) of code.\n",
    "\n",
    "In addition to all of the utilities defined at the top of the colab notebook, the following functions are available in this question environment: `run_best_first_search`. You may not need to use all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb92525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_greedy_best_first_search(problem: PathCostProblem,\n",
    "                                 step_budget: int = 1000):\n",
    "  \"\"\"GBFS.\n",
    "\n",
    "  Use your implementation of `run_best_first_search`.\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f033c4a",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0a8251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbfs_test1():\n",
    "  \"\"\"If your results do not match the expected ones, make sure that you are tiebreaking\n",
    "  as described in the docstring for `run_best_first_search`.\"\"\"\n",
    "  problem = GridProblem()\n",
    "  state_sequence, action_sequence, cost_sequence, num_steps = run_greedy_best_first_search(\n",
    "      problem)\n",
    "  assert state_sequence == [(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (1, 4),\n",
    "                            (2, 4), (3, 4), (4, 4)]\n",
    "  assert action_sequence == [\n",
    "      'right', 'right', 'right', 'right', 'down', 'down', 'down', 'down'\n",
    "  ]\n",
    "  assert abs(num_steps - 22) <= 1\n",
    "\n",
    "gbfs_test1()\n",
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde96e4d",
   "metadata": {},
   "source": [
    "## Visualize Reward Fields\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83493e3",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71116d34",
   "metadata": {},
   "source": [
    "The Fractal Problem and different reward fields. \n",
    "          You shoud read the code below to get rough idea what each reward field looks like.\n",
    "\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28941e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FractalProblemState = collections.namedtuple(\"FractalProblemState\",\n",
    "                                             [\"x\", \"y\", \"t\"])\n",
    "\n",
    "\n",
    "class FractalProblem(RewardProblem):\n",
    "  \"\"\"A base class for \"fractal problem\".\n",
    "\n",
    "  Briefly, a fractal problem is as follows:\n",
    "    - The state space is the entire 2D Euclidean space\n",
    "    - The initial state is (0, 0)\n",
    "    - At each step $t$ the agent is allowed to move along one of the directions of size\n",
    "      $$step_scale^{-t}$$.\n",
    "    - At each step the agent receives a scalar reward based on a reward field.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               action_directions=\"8-neighbors\",\n",
    "               step_scale=0.5,\n",
    "               horizon=6):\n",
    "    \"\"\"A base constructor for a fractal problem. \n",
    "\n",
    "    Args:\n",
    "      action_directions: allowed directions to move at each step. \n",
    "      step_scale: each step length is scaled down by this amount.\n",
    "    \"\"\"\n",
    "    super().__init__(FractalProblemState(0, 0, 1), horizon)\n",
    "    if action_directions == \"8-neighbors\":\n",
    "      self.action_directions = [\n",
    "          (x, y)\n",
    "          for x, y in itertools.product([-1, 0, 1], [-1, 0, 1])\n",
    "          if (x, y) != (0, 0)\n",
    "      ]\n",
    "    elif action_directions == \"4-neighbors\":\n",
    "      self.action_directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    else:\n",
    "      self.action_directions = action_directions\n",
    "    self.step_scale = step_scale\n",
    "    self.field_of_view = self._compute_field_of_view()\n",
    "\n",
    "  def _compute_field_of_view(self,\n",
    "                             precision: float = 1.) -> Tuple[Tuple[float]]:\n",
    "    \"\"\"Computes a minimum rectangle that encloses all reachable states within horizon.\n",
    "\n",
    "    Args:\n",
    "      precision: the returned rectangle will have vertices that are integer \n",
    "        multiples of `precision`. Defaults to 1., which means that the rectangle\n",
    "        has integral vertices.\n",
    "\n",
    "    Returns:\n",
    "      the lower-left and upper-right vertices that represents the rectangle.\n",
    "    \"\"\"\n",
    "    extent = sum([self.step_scale**(t + 1) for t in range(self.horizon)])\n",
    "    dir_extent = np.array(self.action_directions) * extent\n",
    "    return (\n",
    "        tuple(np.floor(np.min(dir_extent, axis=0) / precision) * precision),\n",
    "        tuple(np.ceil(np.max(dir_extent, axis=0) / precision) * precision),\n",
    "    )\n",
    "\n",
    "  @property\n",
    "  def field_of_view_size(self):\n",
    "    return (self.field_of_view[1][0] - self.field_of_view[0][0],\n",
    "            self.field_of_view[1][1] - self.field_of_view[0][1])\n",
    "\n",
    "  def actions(self, state):\n",
    "    scale = self.step_scale**state.t\n",
    "    return [(x * scale, y * scale) for x, y in self.action_directions]\n",
    "\n",
    "  def step(self, state, action):\n",
    "    ax, ay = action\n",
    "    return FractalProblemState(state.x + ax, state.y + ay, state.t + 1)\n",
    "\n",
    "  def reward(self, state, action, next_state):\n",
    "    coord = np.array([next_state.x, next_state.y])\n",
    "    return self.reward_field(coord).item()\n",
    "\n",
    "  def reward_field(self, coords: np.ndarray):\n",
    "    \"\"\"The reward(s) at `coords`. Handle a batch of coordinates for efficiency of plotting.\n",
    "\n",
    "    Subclass must override to provide a meaningful reward field.\n",
    "\n",
    "    Args:\n",
    "      coords: an array with shape (2,) or (...batch, 2) denoting the coordinate(s)\n",
    "        to compute the reward field.\n",
    "\n",
    "    Returns:\n",
    "      A scalar or an array of shape (...batch,)\n",
    "    \"\"\"\n",
    "    return np.zeros(coords.shape[:-1])\n",
    "\n",
    "  @property\n",
    "  def rmax(self):\n",
    "    return 1.\n",
    "\n",
    "  def visualize_reward_field(self,\n",
    "                             fov: Tuple[Tuple[float]] = None,\n",
    "                             resolution=(1000, 1000),\n",
    "                             show=False):\n",
    "    \"\"\"Visualize a reward field.\n",
    "\n",
    "    Args:\n",
    "      fov: an optional field of view. If None, use self.field_of_view.\n",
    "      resolution: number of pixels (w, h) to discretize the field of view. \n",
    "      show: whether to display the plot immediately.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    if fov is None:\n",
    "      fov = self.field_of_view\n",
    "    plt.xlim(fov[0][0], fov[1][0])\n",
    "    plt.ylim(fov[0][1], fov[1][1])\n",
    "    coords = np.dstack(\n",
    "        np.meshgrid(\n",
    "            np.linspace(fov[0][0], fov[1][0], resolution[0]),\n",
    "            np.linspace(fov[0][1], fov[1][1], resolution[1]),\n",
    "        )).reshape(-1, 2)\n",
    "    vals = self.reward_field(coords).reshape(resolution)\n",
    "    plt.imshow(vals,\n",
    "               origin=\"lower\",\n",
    "               extent=(fov[0][0], fov[1][0], fov[0][1], fov[1][1]))\n",
    "    plt.colorbar()\n",
    "    if show:\n",
    "      plt.show()\n",
    "    return self\n",
    "\n",
    "  def visualize_plan(self,\n",
    "                     state_sequence: StateSeq,\n",
    "                     fov: Tuple[Tuple[float]] = None,\n",
    "                     show=False,\n",
    "                     **arrow_kwargs):\n",
    "    \"\"\"Visualize a plan on the reward field.\n",
    "\n",
    "    Visualizes a plan on top of the reward field.\n",
    "    Usually, one calls `visualize_reward_field` first to draw the reward \n",
    "    field in the back.\n",
    "    One then chain multiple `visualize_plan` calls, and pass `show=True` to the \n",
    "    last call to visualize multiple plans on the reward field.\n",
    "\n",
    "    Args:\n",
    "      state_sequence: A sequence state moving in the fractal problem.\n",
    "      fov: an optional field of view. If None, use self.field_of_view.\n",
    "      arrow_kwargs: passed to `plt.arrow`.\n",
    "      show: whether to display the plot immediately.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    if fov is None:\n",
    "      fov = self.field_of_view\n",
    "    plt.xlim(fov[0][0], fov[1][0])\n",
    "    plt.ylim(fov[0][1], fov[1][1])\n",
    "    for s1, s2 in zip(state_sequence[:-1], state_sequence[1:]):\n",
    "      x1, y1, d = s1\n",
    "      x2, y2, _ = s2\n",
    "      dx, dy = x2 - x1, y2 - y1\n",
    "      plt.arrow(x1,\n",
    "                y1,\n",
    "                dx,\n",
    "                dy,\n",
    "                length_includes_head=True,\n",
    "                width=0.01 / d**0.5,\n",
    "                fill=True,\n",
    "                **arrow_kwargs)\n",
    "    if show:\n",
    "      plt.show()\n",
    "    return self\n",
    "\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "\n",
    "class GradientRewardFieldProblem(FractalProblem):\n",
    "  \"\"\"A fractal problem with a gradient reward field by adding multiple (scaled) \n",
    "  gaussian distributions.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               locs: Sequence[Tuple[int, int]] = ((0, 0),),\n",
    "               covs: Union[float, Sequence[Union[float, np.ndarray]]] = 0.1,\n",
    "               strengths: Union[float, Sequence[float]] = 1.,\n",
    "               **kwargs):\n",
    "    \"\"\"A reward field with a mixture of guassian gradients.\n",
    "\n",
    "    Args:\n",
    "      locs: the centers of the gaussians\n",
    "      covs: a scalar, a sequence of scalars, or a sequence of matrices \n",
    "        for the covariances of the gaussians.\n",
    "      strenghts: a scalar or a sequence of scalars for the scalaring factors \n",
    "        for each guassian. \n",
    "    \"\"\"\n",
    "    super().__init__(**kwargs)\n",
    "    self.locs = locs\n",
    "    if np.isscalar(covs):\n",
    "      covs = [covs] * len(self.locs)\n",
    "    covs = [np.eye(2) * cov if np.isscalar(cov) else cov for cov in covs]\n",
    "    self.covs = covs\n",
    "    if np.isscalar(strengths):\n",
    "      strengths = [strengths] * len(self.locs)\n",
    "    self.strengths = strengths\n",
    "\n",
    "  def reward_field(self, coords):\n",
    "    return sum([\n",
    "        multivariate_normal.pdf(coords, mean=loc, cov=cov) * strength\n",
    "        for loc, cov, strength in zip(self.locs, self.covs, self.strengths)\n",
    "    ])\n",
    "\n",
    "\n",
    "class NoisyRewardFieldProblem(FractalProblem):\n",
    "  \"\"\"A fractal problem with a reward field sampled from iid Beta distributions \n",
    "  in a discretized grid.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, seed=0, bin_size=5e-2, beta_params=(1., 2.), **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    # Compute number of bins required for the field of view\n",
    "    self.nbins = tuple(int(s / bin_size) for s in self.field_of_view_size)\n",
    "    # Initialize the random rewards within the field of view\n",
    "    rng_state = np.random.RandomState(seed)\n",
    "    self.random_locs = rng_state.beta(*beta_params, size=self.nbins)\n",
    "    self.random_locs = np.pad(self.random_locs, [(1, 1), (1, 1)],\n",
    "                              constant_values=0.)\n",
    "\n",
    "  def reward_field(self, coords):\n",
    "    binX, binY = tuple(\n",
    "        np.digitize(\n",
    "            coords[..., i],\n",
    "            np.linspace(self.field_of_view[0][i], self.field_of_view[1][i],\n",
    "                        self.nbins[i] + 1)) for i in range(2))\n",
    "    return self.random_locs[binX, binY]\n",
    "\n",
    "\n",
    "def get_fractal_problems() -> Dict[str, FractalProblem]:\n",
    "  \"\"\"Three fractal problems with reward fields.\n",
    "\n",
    "  DO NOT CHANGE THIS FUNCTION --- it exists to protect you from accidentally \n",
    "  changing the problems.\n",
    "  \"\"\"\n",
    "  return {\n",
    "      \"reward-field-1\":\n",
    "          GradientRewardFieldProblem(locs=[(-1, -1), (-1, 1), (1, -1), (1, 1)],\n",
    "                                     covs=[.3, .3, .3, .3],\n",
    "                                     strengths=[1.5, 1.5, 1.5, 1.5]),\n",
    "      \"reward-field-2\":\n",
    "          GradientRewardFieldProblem(locs=[(-1, -1), (-1, 1), (1, -1), (1, 1)],\n",
    "                                     covs=[.25, .25, .25, .2],\n",
    "                                     strengths=[0.7, 0.7, 0.7, 0.8]),\n",
    "      \"reward-field-3\":\n",
    "          NoisyRewardFieldProblem(seed=42, bin_size=5e-2,\n",
    "                                  beta_params=(0.2, 2.)),\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe04269",
   "metadata": {},
   "source": [
    "### Question\n",
    "We have defined for you three fractal problems in `get_fractal_problems()`.\n",
    "          Below you can visualize the reward fields corresponding to these problems. \n",
    "          We have also provided you with code in the Colab notebook to recreate these visualizations.\n",
    "          You are encouraged to inspect the relevant class definitions for these problems to understand them better. \n",
    "          **You do not need to submit any code for this subsection.**\n",
    "          Instead, we will ask you questions in the following problems to confirm your understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf63549",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644da690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_fractal_problems():\n",
    "  \"\"\"Visualize the fractal problems with different rewards fields.\"\"\"\n",
    "  import matplotlib.pyplot as plt\n",
    "  for name, problem in get_fractal_problems().items():\n",
    "    plt.title(name)\n",
    "    problem.visualize_reward_field(show=True)\n",
    "    plt.clf()\n",
    "\n",
    "visualize_fractal_problems()\n",
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f865b5",
   "metadata": {},
   "source": [
    "## Path-cost Problem from Reward Problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec96501",
   "metadata": {},
   "source": [
    "### Question\n",
    "Implement the reduction from a reward problem to a path-cost problem **as in lecture**.\n",
    "\n",
    "For reference, our solution is **31** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed21a4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_cost_problem_from_reward_problem(\n",
    "    reward_problem: RewardProblem) -> PathCostProblem:\n",
    "  \"\"\"Reduce a reward maximization problem into a path search problem.\n",
    "\n",
    "  You should take a close look that the class definition of `RewardProblem`, \n",
    "  since they will be handy. \n",
    "  Especially note that the horizon value is inclusive -- for a horizon value of $H$, \n",
    "  the agent should be allowed to step exactly $H$ number of steps.\n",
    "\n",
    "  Args:\n",
    "    problem: a RewardProblem.\n",
    "\n",
    "  Returns:\n",
    "    A PathCostProblem\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bd015f",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dab77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_reward_reduction_test(path_cost_problem_from_reward_problem,\n",
    "                               loc,\n",
    "                               horizon: int = 4):\n",
    "  problem = GradientRewardFieldProblem(locs=[loc],\n",
    "                                       covs=0.3,\n",
    "                                       strengths=[1.],\n",
    "                                       horizon=horizon)\n",
    "  path_problem = path_cost_problem_from_reward_problem(problem)\n",
    "  state_sequence, action_sequence, cost_sequence, num_steps = run_uniform_cost_search(\n",
    "      path_problem, step_budget=100000)\n",
    "  assert ((len(state_sequence) - 1) == len(action_sequence) ==\n",
    "          len(cost_sequence) == horizon)\n",
    "  assert np.all(np.sign(np.array(action_sequence)) == np.sign(\n",
    "      loc)), f\"Actions {action_sequence} does not move towards {loc}\"\n",
    "  cost_sequence = np.array(cost_sequence)\n",
    "  assert np.all(cost_sequence <= problem.rmax), \"Costs should not exceed Rmax\"\n",
    "  assert np.all(cost_sequence >= 0), \"Costs should be positive\"\n",
    "  assert np.all(np.diff(cost_sequence) <= 0), \"Costs should be decreasing\"\n",
    "\n",
    "path_reward_reduction_test(path_cost_problem_from_reward_problem, (1, 1), 2)\n",
    "\n",
    "def path_reward_reduction_test(path_cost_problem_from_reward_problem,\n",
    "                               loc,\n",
    "                               horizon: int = 4):\n",
    "  problem = GradientRewardFieldProblem(locs=[loc],\n",
    "                                       covs=0.3,\n",
    "                                       strengths=[1.],\n",
    "                                       horizon=horizon)\n",
    "  path_problem = path_cost_problem_from_reward_problem(problem)\n",
    "  state_sequence, action_sequence, cost_sequence, num_steps = run_uniform_cost_search(\n",
    "      path_problem, step_budget=100000)\n",
    "  assert ((len(state_sequence) - 1) == len(action_sequence) ==\n",
    "          len(cost_sequence) == horizon)\n",
    "  assert np.all(np.sign(np.array(action_sequence)) == np.sign(\n",
    "      loc)), f\"Actions {action_sequence} does not move towards {loc}\"\n",
    "  cost_sequence = np.array(cost_sequence)\n",
    "  assert np.all(cost_sequence <= problem.rmax), \"Costs should not exceed Rmax\"\n",
    "  assert np.all(cost_sequence >= 0), \"Costs should be positive\"\n",
    "  assert np.all(np.diff(cost_sequence) <= 0), \"Costs should be decreasing\"\n",
    "\n",
    "path_reward_reduction_test(path_cost_problem_from_reward_problem, (1, 1), 3)\n",
    "\n",
    "def path_reward_reduction_test(path_cost_problem_from_reward_problem,\n",
    "                               loc,\n",
    "                               horizon: int = 4):\n",
    "  problem = GradientRewardFieldProblem(locs=[loc],\n",
    "                                       covs=0.3,\n",
    "                                       strengths=[1.],\n",
    "                                       horizon=horizon)\n",
    "  path_problem = path_cost_problem_from_reward_problem(problem)\n",
    "  state_sequence, action_sequence, cost_sequence, num_steps = run_uniform_cost_search(\n",
    "      path_problem, step_budget=100000)\n",
    "  assert ((len(state_sequence) - 1) == len(action_sequence) ==\n",
    "          len(cost_sequence) == horizon)\n",
    "  assert np.all(np.sign(np.array(action_sequence)) == np.sign(\n",
    "      loc)), f\"Actions {action_sequence} does not move towards {loc}\"\n",
    "  cost_sequence = np.array(cost_sequence)\n",
    "  assert np.all(cost_sequence <= problem.rmax), \"Costs should not exceed Rmax\"\n",
    "  assert np.all(cost_sequence >= 0), \"Costs should be positive\"\n",
    "  assert np.all(np.diff(cost_sequence) <= 0), \"Costs should be decreasing\"\n",
    "\n",
    "path_reward_reduction_test(path_cost_problem_from_reward_problem, (1, 1), 5)\n",
    "\n",
    "def path_reward_reduction_test(path_cost_problem_from_reward_problem,\n",
    "                               loc,\n",
    "                               horizon: int = 4):\n",
    "  problem = GradientRewardFieldProblem(locs=[loc],\n",
    "                                       covs=0.3,\n",
    "                                       strengths=[1.],\n",
    "                                       horizon=horizon)\n",
    "  path_problem = path_cost_problem_from_reward_problem(problem)\n",
    "  state_sequence, action_sequence, cost_sequence, num_steps = run_uniform_cost_search(\n",
    "      path_problem, step_budget=100000)\n",
    "  assert ((len(state_sequence) - 1) == len(action_sequence) ==\n",
    "          len(cost_sequence) == horizon)\n",
    "  assert np.all(np.sign(np.array(action_sequence)) == np.sign(\n",
    "      loc)), f\"Actions {action_sequence} does not move towards {loc}\"\n",
    "  cost_sequence = np.array(cost_sequence)\n",
    "  assert np.all(cost_sequence <= problem.rmax), \"Costs should not exceed Rmax\"\n",
    "  assert np.all(cost_sequence >= 0), \"Costs should be positive\"\n",
    "  assert np.all(np.diff(cost_sequence) <= 0), \"Costs should be decreasing\"\n",
    "\n",
    "path_reward_reduction_test(path_cost_problem_from_reward_problem, (-1, -1), 2)\n",
    "\n",
    "def path_reward_reduction_test(path_cost_problem_from_reward_problem,\n",
    "                               loc,\n",
    "                               horizon: int = 4):\n",
    "  problem = GradientRewardFieldProblem(locs=[loc],\n",
    "                                       covs=0.3,\n",
    "                                       strengths=[1.],\n",
    "                                       horizon=horizon)\n",
    "  path_problem = path_cost_problem_from_reward_problem(problem)\n",
    "  state_sequence, action_sequence, cost_sequence, num_steps = run_uniform_cost_search(\n",
    "      path_problem, step_budget=100000)\n",
    "  assert ((len(state_sequence) - 1) == len(action_sequence) ==\n",
    "          len(cost_sequence) == horizon)\n",
    "  assert np.all(np.sign(np.array(action_sequence)) == np.sign(\n",
    "      loc)), f\"Actions {action_sequence} does not move towards {loc}\"\n",
    "  cost_sequence = np.array(cost_sequence)\n",
    "  assert np.all(cost_sequence <= problem.rmax), \"Costs should not exceed Rmax\"\n",
    "  assert np.all(cost_sequence >= 0), \"Costs should be positive\"\n",
    "  assert np.all(np.diff(cost_sequence) <= 0), \"Costs should be decreasing\"\n",
    "\n",
    "path_reward_reduction_test(path_cost_problem_from_reward_problem, (-1, -1), 3)\n",
    "\n",
    "def path_reward_reduction_test(path_cost_problem_from_reward_problem,\n",
    "                               loc,\n",
    "                               horizon: int = 4):\n",
    "  problem = GradientRewardFieldProblem(locs=[loc],\n",
    "                                       covs=0.3,\n",
    "                                       strengths=[1.],\n",
    "                                       horizon=horizon)\n",
    "  path_problem = path_cost_problem_from_reward_problem(problem)\n",
    "  state_sequence, action_sequence, cost_sequence, num_steps = run_uniform_cost_search(\n",
    "      path_problem, step_budget=100000)\n",
    "  assert ((len(state_sequence) - 1) == len(action_sequence) ==\n",
    "          len(cost_sequence) == horizon)\n",
    "  assert np.all(np.sign(np.array(action_sequence)) == np.sign(\n",
    "      loc)), f\"Actions {action_sequence} does not move towards {loc}\"\n",
    "  cost_sequence = np.array(cost_sequence)\n",
    "  assert np.all(cost_sequence <= problem.rmax), \"Costs should not exceed Rmax\"\n",
    "  assert np.all(cost_sequence >= 0), \"Costs should be positive\"\n",
    "  assert np.all(np.diff(cost_sequence) <= 0), \"Costs should be decreasing\"\n",
    "\n",
    "path_reward_reduction_test(path_cost_problem_from_reward_problem, (-1, -1), 5)\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc12b91d",
   "metadata": {},
   "source": [
    "## MCTS vs. UCS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d4c986",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0da5cc",
   "metadata": {},
   "source": [
    "Our implementation of MCTS.\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d883f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dataclasses\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=False, eq=False)\n",
    "class MCTNode:\n",
    "  \"\"\"Node in the Monte Carlo search tree, keeps track of the children states.\"\"\"\n",
    "  state: State\n",
    "  U: float\n",
    "  N: int\n",
    "  horizon: int\n",
    "  parent: Optional['MCTNode']\n",
    "  children: Dict['MCTNode', Action] = dataclasses.field(default_factory=dict)\n",
    "\n",
    "\n",
    "def ucb(n: MCTNode, C: float = 1.4) -> float:\n",
    "  \"\"\"UCB for a node, note the C argument\"\"\"\n",
    "  return np.inf if n.N == 0 else (n.U / n.N +\n",
    "                                  C * np.sqrt(np.log(n.parent.N) / n.N))\n",
    "\n",
    "\n",
    "def run_mcts_search(problem: RewardProblem,\n",
    "                    C: float = 1.4,\n",
    "                    iteration_budget: int = 1000,\n",
    "                    step_budget: int = np.inf):\n",
    "  \"\"\"A generic MCTS search implementation.\n",
    "\n",
    "  Args:\n",
    "    problem: a reward problem.\n",
    "    C: the UCB parameter.\n",
    "    iteration_budget: maximum iterations to run the search.\n",
    "    step_budget: maximum number of allowed `problem.step`s.\n",
    "\n",
    "  Returns:\n",
    "    state_sequence: A list of states.\n",
    "    action_sequence: A list of actions.\n",
    "    reward_sequence: A list of rewards\n",
    "  \"\"\"\n",
    "  if min(iteration_budget, step_budget) == np.inf:\n",
    "    raise ValueError(\"Must provide at least one budget\")\n",
    "\n",
    "  problem_step_count = 0\n",
    "\n",
    "  class BudgetExceeded(Exception):\n",
    "    pass\n",
    "\n",
    "  def step_helper(state, action):\n",
    "    \"\"\"helper to track the problem's step count.\"\"\"\n",
    "    nonlocal problem_step_count\n",
    "    problem_step_count += 1\n",
    "    if problem_step_count > step_budget:\n",
    "      raise BudgetExceeded(\"step budget exceeded\")\n",
    "    return problem.step(state, action)\n",
    "\n",
    "  ucb_fixed_C = functools.partial(ucb, C=C)\n",
    "\n",
    "  def select(n: MCTNode) -> MCTNode:\n",
    "    \"\"\"select a leaf node in the tree\"\"\"\n",
    "    if n.children:\n",
    "      ucb_pick = max(n.children.keys(), key=ucb_fixed_C)\n",
    "      return select(ucb_pick)\n",
    "    return n\n",
    "\n",
    "  def expand(n: MCTNode) -> MCTNode:\n",
    "    \"\"\"expand the leaf node by adding all its children states\"\"\"\n",
    "    assert not n.children\n",
    "    if n.horizon == 0:\n",
    "      return n\n",
    "    for action in problem.actions(n.state):\n",
    "      child_state = step_helper(n.state, action)\n",
    "      new_node = MCTNode(state=child_state,\n",
    "                         horizon=n.horizon - 1,\n",
    "                         parent=n,\n",
    "                         U=0,\n",
    "                         N=0)\n",
    "      n.children[new_node] = action\n",
    "    child = random.choice(list(n.children.keys()))\n",
    "    return child\n",
    "\n",
    "  def simulate(node: MCTNode) -> float:\n",
    "    \"\"\"simulate the utility of current state by randomly picking a step\"\"\"\n",
    "    state = node.state\n",
    "    total_reward = 0\n",
    "    for h in range(node.horizon, 0, -1):\n",
    "      action = random.choice(problem.actions(state))\n",
    "      child_state = step_helper(state, action)\n",
    "      reward = problem.reward(state, action, child_state)\n",
    "      total_reward += reward\n",
    "      state = child_state\n",
    "    return total_reward\n",
    "\n",
    "  def backup(n: MCTNode, value: float) -> None:\n",
    "    \"\"\"passing the utility back to all parent nodes\"\"\"\n",
    "    if n.parent:\n",
    "      # Need to include the reward on the action *into* n\n",
    "      a = n.parent.children[n]\n",
    "      r = problem.reward(n.parent.state, a, n.state)\n",
    "      n.U += value + r\n",
    "      n.N += 1\n",
    "      backup(n.parent, value + r)\n",
    "    else:\n",
    "      n.N += 1\n",
    "\n",
    "  root = MCTNode(state=problem.initial,\n",
    "                 horizon=problem.horizon,\n",
    "                 parent=None,\n",
    "                 U=0,\n",
    "                 N=0)\n",
    "\n",
    "  try:\n",
    "    i = 0\n",
    "    while i < iteration_budget:\n",
    "      leaf = select(root)\n",
    "      child = expand(leaf)\n",
    "      value = simulate(child)\n",
    "      backup(child, value)\n",
    "      i += 1\n",
    "  except BudgetExceeded:\n",
    "    pass\n",
    "\n",
    "  return finish_mcts_plan(problem, root)\n",
    "\n",
    "\n",
    "def finish_mcts_plan(problem: RewardProblem, node: MCTNode):\n",
    "  \"\"\"Helper for run_mcts_search. Recover the plan. \"\"\"\n",
    "  state_sequence = [node.state]\n",
    "  action_sequence = []\n",
    "  reward_sequence = []\n",
    "\n",
    "  while node.children:\n",
    "    max_node = max(node.children, key=lambda p: p.N)\n",
    "    max_action = node.children.get(max_node)\n",
    "    action_sequence.append(max_action)\n",
    "    state_sequence.append(max_node.state)\n",
    "    reward_sequence.append(\n",
    "        problem.reward(node.state, max_action, max_node.state))\n",
    "    node = max_node\n",
    "\n",
    "  return state_sequence, action_sequence, reward_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43d7e45",
   "metadata": {},
   "source": [
    "### Question\n",
    "We have provided you an implementation of MCTS for reward problems in the Colab notebook.\n",
    "          You should make sure that you can run our implementation on the reward problems we have defined for you.\n",
    "          You are also encouraged to look into our code to see under how our MCTS is implemented.\n",
    "          **You do not need to submit any code for this question.**\n",
    "          Instead, we will ask you some questions to confirm your understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146d556e",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfff5aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mcts():\n",
    "  \"\"\"Solve one of the fractal problems and visualize the plan.\n",
    "\n",
    "  This function may take a few moments to run. \n",
    "  \"\"\"\n",
    "  # You may change this to visualize other reward fields.\n",
    "  problem = get_fractal_problems()[\"reward-field-1\"]\n",
    "\n",
    "  plan = run_mcts_search(problem, iteration_budget=10000)\n",
    "  problem.visualize_reward_field().visualize_plan(plan[0], show=True)\n",
    "\n",
    "visualize_mcts()\n",
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703aeb52",
   "metadata": {},
   "source": [
    "## Warming up with Pyperplan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af37a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "from abc import abstractmethod\n",
    "import dataclasses\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import time\n",
    "import tempfile\n",
    "from pyperplan.pddl.parser import Parser\n",
    "from pyperplan import grounding, planner\n",
    "\n",
    "# This uses TYPING\n",
    "BLOCKS_DOMAIN = '''(define (domain blocks)\n",
    "    (:requirements :strips :typing)\n",
    "    (:types block)\n",
    "    (:predicates \n",
    "        (on ?x - block ?y - block)\n",
    "        (ontable ?x - block)\n",
    "        (clear ?x - block)\n",
    "        (handempty)\n",
    "        (holding ?x - block)\n",
    "    )\n",
    "\n",
    "    (:action pick-up\n",
    "        :parameters (?x - block)\n",
    "        :precondition (and\n",
    "            (clear ?x) \n",
    "            (ontable ?x) \n",
    "            (handempty)\n",
    "        )\n",
    "        :effect (and\n",
    "            (not (ontable ?x))\n",
    "            (not (clear ?x))\n",
    "            (not (handempty))\n",
    "            (holding ?x)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    (:action put-down\n",
    "        :parameters (?x - block)\n",
    "        :precondition (and \n",
    "            (holding ?x)\n",
    "        )\n",
    "        :effect (and \n",
    "            (not (holding ?x))\n",
    "            (clear ?x)\n",
    "            (handempty)\n",
    "            (ontable ?x))\n",
    "        )\n",
    "\n",
    "    (:action stack\n",
    "        :parameters (?x - block ?y - block)\n",
    "        :precondition (and\n",
    "            (holding ?x) \n",
    "            (clear ?y)\n",
    "        )\n",
    "        :effect (and \n",
    "            (not (holding ?x))\n",
    "            (not (clear ?y))\n",
    "            (clear ?x)\n",
    "            (handempty)\n",
    "            (on ?x ?y)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    (:action unstack\n",
    "        :parameters (?x - block ?y - block)\n",
    "        :precondition (and\n",
    "            (on ?x ?y)\n",
    "            (clear ?x)\n",
    "            (handempty)\n",
    "        )\n",
    "        :effect (and \n",
    "            (holding ?x)\n",
    "            (clear ?y)\n",
    "            (not (clear ?x))\n",
    "            (not (handempty))\n",
    "            (not (on ?x ?y))\n",
    "        )\n",
    "    )\n",
    ")\n",
    "'''\n",
    "\n",
    "# This uses TYPING\n",
    "BLOCKS_PROBLEM = '''(define (problem blocks)\n",
    "    (:domain blocks)\n",
    "    (:objects \n",
    "        d - block\n",
    "        b - block\n",
    "        a - block\n",
    "        c - block\n",
    "    )\n",
    "    (:init \n",
    "        (clear a) \n",
    "        (on a b) \n",
    "        (on b c)\n",
    "        (on c d)\n",
    "        (ontable d) \n",
    "        (handempty)\n",
    "    )\n",
    "    (:goal (and (on d c) (on c b) (on b a)))\n",
    ")\n",
    "'''\n",
    "\n",
    "def get_task_definition_str(domain_pddl_str, problem_pddl_str):\n",
    "    '''Get Pyperplan task definition from PDDL domain and problem.\n",
    "\n",
    "    This function is a lightweight wrapper around Pyperplan.\n",
    "\n",
    "    Args:\n",
    "      domain_pddl_str: A str, the contents of a domain.pddl file.\n",
    "      problem_pddl_str: A str, the contents of a problem.pddl file.\n",
    "\n",
    "    Returns:\n",
    "      task: a structure defining the problem\n",
    "    '''\n",
    "    # Parsing the PDDL\n",
    "    domain_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "    problem_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "    with open(domain_file.name, 'w') as f:\n",
    "        f.write(domain_pddl_str)\n",
    "    with open(problem_file.name, 'w') as f:\n",
    "        f.write(problem_pddl_str)\n",
    "    parser = Parser(domain_file.name, problem_file.name)\n",
    "    domain = parser.parse_domain()\n",
    "    problem = parser.parse_problem(domain)\n",
    "    os.remove(domain_file.name)\n",
    "    os.remove(problem_file.name)\n",
    "\n",
    "    # Ground the PDDL\n",
    "    task = grounding.ground(problem)\n",
    "    return task\n",
    "\n",
    "def run_planning(domain_pddl_str,\n",
    "                 problem_pddl_str,\n",
    "                 search_alg_name,\n",
    "                 heuristic_name=None,\n",
    "                 return_time=False):\n",
    "    '''Plan a sequence of actions to solve the given PDDL problem.\n",
    "\n",
    "    This function is a lightweight wrapper around pyperplan.\n",
    "\n",
    "    Args:\n",
    "      domain_pddl_str: A str, the contents of a domain.pddl file.\n",
    "      problem_pddl_str: A str, the contents of a problem.pddl file.\n",
    "      search_alg_name: A str, the name of a search algorithm in\n",
    "        pyperplan. Options: astar, wastar, gbf, bfs, ehs, ids, sat.\n",
    "      heuristic_name: A str, the name of a heuristic in pyperplan.\n",
    "        Options: blind, hadd, hmax, hsa, hff, lmcut, landmark.\n",
    "      return_time:  Bool. Set to `True` to return the planning time.\n",
    "\n",
    "    Returns:\n",
    "      plan: A list of actions; each action is a pyperplan Operator.\n",
    "    '''\n",
    "    # Ground the PDDL\n",
    "    task = get_task_definition_str(domain_pddl_str, problem_pddl_str)\n",
    "\n",
    "    # Get the search alg\n",
    "    search_alg = planner.SEARCHES[search_alg_name]\n",
    "\n",
    "    if heuristic_name is None:\n",
    "        if not return_time:\n",
    "            return search_alg(task)\n",
    "        start_time = time.time()\n",
    "        plan = search_alg(task)\n",
    "        plan_time = time.time() - start_time\n",
    "        return plan, plan_time\n",
    "\n",
    "    # Get the heuristic\n",
    "    heuristic = planner.HEURISTICS[heuristic_name](task)\n",
    "\n",
    "    # Run planning\n",
    "    start_time = time.time()\n",
    "    plan = search_alg(task, heuristic)\n",
    "    plan_time = time.time() - start_time\n",
    "\n",
    "    if return_time:\n",
    "        return plan, plan_time\n",
    "    return plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf2c911",
   "metadata": {},
   "source": [
    "### Let's Make a Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbc30c1",
   "metadata": {},
   "source": [
    "Use run_planning to find a plan for the blocks problem defined at the top of the colab file (BLOCKS_DOMAIN, BLOCKS_PROBLEM).\n",
    "\n",
    "The run_planning function takes in a PDDL domain string, a PDDL problem string, the name of a search algorithm, and the name of a heuristic (if the search algorithm is informed) or a customized heuristic class. It then uses the Python planning library pyperplan to find a plan.\n",
    "\n",
    "The plan returned by run_planning is a list of pyperplan Operators. You should not need to manipulate these data structures directly in this homework, but if you are curious about the definition, see here.\n",
    "\n",
    "The search algs available in pyperplan are: astar, wastar, gbf, bfs, ehs, ids, sat. The heuristics available in pyperplan are: blind, hadd, hmax, hsa, hff, lmcut, landmark.\n",
    "\n",
    "For this question, use the astar search algorithm with the hadd heuristic.\n",
    "\n",
    "For reference, our solution is 2 line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84e585d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def planning_warmup():\n",
    "    '''Use run_planning to find a plan for the blocks problem defined at the\n",
    "    top of the colab file (BLOCKS_DOMAIN, BLOCKS_PROBLEM).\n",
    "\n",
    "      Use the astar search algorithm with the hadd heuristic.\n",
    "\n",
    "    Returns:\n",
    "      plan: A list of actions; each action is a pyperplan Operator.\n",
    "    '''\n",
    "    raise NotImplementedError(\"Implement me!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30cc999",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e9c57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_test1():\n",
    "    plan = planning_warmup()\n",
    "    assert len(plan) == 8\n",
    "    assert plan[0].name == '(unstack a b)'\n",
    "\n",
    "warmup_test1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5649c380",
   "metadata": {},
   "source": [
    "## Fill in the Blanks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cf6d6b",
   "metadata": {},
   "source": [
    "You've received PDDL domain and problem strings from your boss and you need to make a plan, pronto! Unfortunately, some of the PDDL is missing.\n",
    "\n",
    "Here's what you know. What you're trying to model is a newspaper delivery robot. The robot starts out at a \"home base\" where there are papers that it can pick up. The robot can hold arbitrarily many papers at once. It can then move around to different locations and deliver papers.\n",
    "\n",
    "Not all locations want a paper -- the goal is to satisfy all the locations that do want a paper.\n",
    "\n",
    "You also know:\n",
    "\n",
    "There are 6 locations in addition to 1 for the homebase. Locations 1, 2, 3, and 4 want paper; locations 5 and 6 do not.\n",
    "There are 8 papers at the homebase.\n",
    "The robot is initially at the homebase with no papers packed.\n",
    "Use this description to complete the PDDL domain and problem. You can assume the PDDL planner will find the optimal solution.\n",
    "\n",
    "If you are running into issues writing or debugging the PDDL you can check out this online PDDL editor, which comes with a built-in planner: editor.planning.domains. To use the editor, you can create two files, one for the domain and one for the problem. You can then click \"Solve\" at the top to use the built-in planner.\n",
    "\n",
    "For a general reference on PDDL, check out planning.wiki. Note that the PDDL features supported by pyperplan are very limited: types and constants are supported, but that's about it. If you want to make a domain that involves more advanced features, you can try the built-in planner at editor.planning.domains, or you can use any other PDDL planner of your choosing.\n",
    "\n",
    "Debugging PDDL can be painful. The online editor at editor.planning.domains is helpful: pay attention to the syntax highlighting and to the line numbers in error messages that result from trying to Solve. To debug, you can also comment out multiple lines of your files by highlighting and using command+/. Common issues to watch out for include:\n",
    "\n",
    "- A predicate is not defined in the domain file\n",
    "- A variable name does not start with a question mark\n",
    "- An illegal character is used (we recommended sticking with alphanumeric characters, dashes, or underscores; and don't start any names with numbers)\n",
    "- An operator is missing a parameter (in general, the parameters should be exactly the variables that are used anywhere in the operator's preconditions or effects)\n",
    "- An operator is missing a necessary precondition or effect\n",
    "- Using negated preconditions, which are not allowed in basic Strips\n",
    "\n",
    "If you get stuck debugging your PDDL file for more than 10-15 min, please reach out and we'll help!\n",
    "\n",
    "For reference, our solution is 89 line(s) of code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbc0a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pddl_warmup():\n",
    "    '''Creates a PDDL domain and problem strs for newspaper delivery (uses\n",
    "    TYPING).\n",
    "\n",
    "    Returns:\n",
    "      domain: str\n",
    "      problem: str\n",
    "    '''\n",
    "    domain_str = '''(define (domain newspapers)\n",
    "    (:requirements :strips :typing)\n",
    "    (:types loc paper)\n",
    "    (:predicates \n",
    "      (isHomeBase ?loc - loc)\n",
    "      ; TODO: Add missing predicates!\n",
    "    )\n",
    "\n",
    "    (:action pick-up\n",
    "      :parameters ()    ; TODO: Add missing parameters!\n",
    "      :precondition (and\n",
    "        (at ?loc)\n",
    "        (isHomeBase ?loc)\n",
    "        (unpacked ?paper)\n",
    "      )\n",
    "      :effect (and\n",
    "        (not (unpacked ?paper))\n",
    "        (carrying ?paper)\n",
    "      )\n",
    "    )\n",
    "\n",
    "    (:action move\n",
    "      :parameters (?from - loc ?to - loc)\n",
    "      :precondition (and\n",
    "        (at ?from) \n",
    "      )\n",
    "      :effect (and\n",
    "        (not (at ?from))\n",
    "        (at ?to)\n",
    "      )\n",
    "    )\n",
    "\n",
    "    (:action deliver\n",
    "      :parameters (?paper - paper ?loc - loc)\n",
    "      :precondition (and\n",
    "        ; TODO: Add missing preconditions!\n",
    "      )\n",
    "      :effect (and\n",
    "        ; TODO: Add missing effects!\n",
    "      )\n",
    "    )\n",
    "\n",
    ")'''\n",
    "\n",
    "    problem_str = '''(define (problem newspapers1) (:domain newspapers)\n",
    "  (:objects\n",
    "    loc-0 - loc\n",
    "    loc-1 - loc\n",
    "    loc-2 - loc\n",
    "    loc-3 - loc\n",
    "    loc-4 - loc\n",
    "    loc-5 - loc\n",
    "    loc-6 - loc\n",
    "    paper-0 - paper\n",
    "    paper-1 - paper\n",
    "    paper-2 - paper\n",
    "    paper-3 - paper\n",
    "    paper-4 - paper\n",
    "    paper-5 - paper\n",
    "    paper-6 - paper\n",
    "    paper-7 - paper\n",
    "  )\n",
    "  (:init \n",
    "    (at loc-0)\n",
    "    (unpacked paper-0)\n",
    "    ; TODO: Add missing initial atoms!\n",
    "  )\n",
    "  (:goal (and\n",
    "    (satisfied loc-1)\n",
    "    (satisfied loc-2)\n",
    "    (satisfied loc-3)\n",
    "    (satisfied loc-4)\n",
    "  ))\n",
    ")'''\n",
    "\n",
    "    return domain_str, problem_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bf25af",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261f6ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_test2():\n",
    "    domain, problem = pddl_warmup()\n",
    "    plan = run_planning(domain, problem, \"gbf\", \"hadd\")\n",
    "    assert plan, \"Failed to find a plan.\"\n",
    "    picked_up_papers = set()\n",
    "    satisfied_locs = set()\n",
    "    for op in plan:\n",
    "        if \"pickup\" in op.name:\n",
    "            _, _, paper, _ = op.name.split(\" \")\n",
    "            assert paper not in picked_up_papers, \\\n",
    "                \"Should not pick up the same paper twice\"\n",
    "            picked_up_papers.add(paper)\n",
    "        elif \"deliver\" in op.name:\n",
    "            _, loc = op.name.rsplit(\" \", 1)\n",
    "            assert loc.endswith(\")\")\n",
    "            loc = loc[:-1]\n",
    "            assert loc not in satisfied_locs, \\\n",
    "                \"Should not deliver to the same place twice\"\n",
    "            satisfied_locs.add(loc)\n",
    "    assert satisfied_locs == {\"loc-1\", \"loc-2\", \"loc-3\", \"loc-4\"}\n",
    "\n",
    "warmup_test2()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw06.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "411",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
