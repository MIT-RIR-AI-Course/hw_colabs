{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b877ea39",
      "metadata": {
        "id": "b877ea39"
      },
      "source": [
        "# Homework 9"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea16128c",
      "metadata": {},
      "source": [
        "## Value from Q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d0e31e0",
      "metadata": {},
      "source": [
        "### Mountain Car MDP and Regressors\n",
        "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3335235",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "class MountainCar:\n",
        "    '''\n",
        "    Mountain Car MDP.\n",
        "\n",
        "    The state is a tuple of two floats (x, v), denoting the position and \n",
        "    velocity of the car.\n",
        "    '''\n",
        "    def __init__(self, goal_velocity=0, difficulty='hard', rng=np.random.default_rng(42)):\n",
        "        self.difficulty = difficulty\n",
        "        self.min_position = -1.2\n",
        "        self.max_position = 0.6\n",
        "        self.max_speed = 0.07\n",
        "        self.goal_position = 0.5\n",
        "        self.goal_velocity = goal_velocity\n",
        "\n",
        "        self.force = 0.001\n",
        "        self.gravity = 0.0025\n",
        "        self.force_noise = 0.0002\n",
        "\n",
        "        self.low = np.array([self.min_position, -self.max_speed], dtype=np.float32)\n",
        "        self.high = np.array([self.max_position, self.max_speed], dtype=np.float32)\n",
        "        self.actions = (0, 1, 2)          # (left_acc, none, right_acc)\n",
        "        self.discount_factor = 1.0\n",
        "        self.rng = rng\n",
        "\n",
        "        self.time_factor = 10.\n",
        "\n",
        "        self.init_state()\n",
        "\n",
        "\n",
        "    def init_state(self):\n",
        "        if self.difficulty == 'hard':\n",
        "            self.state = (self.rng.uniform(-0.6, -0.4), 0.0)\n",
        "        else:\n",
        "            self.state = (self.rng.uniform(0.0, 0.5), 0.0)\n",
        "        return self.state\n",
        "\n",
        "    def terminal(self, s):\n",
        "        position, velocity = s\n",
        "        return bool(position >= self.goal_position and velocity >= self.goal_velocity)\n",
        "\n",
        "    def sim_transition(self, action: int):\n",
        "        '''\n",
        "        Args:\n",
        "        - action : {0, 1, 2}, indicating left, none, right\n",
        "        Returns:\n",
        "        - reward : float\n",
        "        - state : (x_position : float, velocity : float)\n",
        "        '''\n",
        "        position, velocity = self.state\n",
        "        velocity += ((action - 1) * self.force + np.cos(3 * position) * (-self.gravity) + self.rng.normal(scale=self.force_noise)) * self.time_factor\n",
        "        velocity = np.clip(velocity, -self.max_speed, self.max_speed)\n",
        "        position += velocity * self.time_factor\n",
        "        position = np.clip(position, self.min_position, self.max_position)\n",
        "        if position == self.min_position and velocity < 0:\n",
        "            velocity = 0\n",
        "        reward = -1.0 \n",
        "\n",
        "        self.state = (position, velocity)\n",
        "\n",
        "        return reward, self.state\n",
        "\n",
        "\n",
        "    def sim_episode(self, policy, max_iters = 20):\n",
        "        traj = []\n",
        "        s = self.init_state()\n",
        "        for i in range(max_iters):\n",
        "            if self.terminal(s):\n",
        "                for a in self.actions:\n",
        "                    traj.append((s, a, 0, s))\n",
        "                return traj\n",
        "            a = policy(s)\n",
        "            (r, s_prime) = self.sim_transition(a)\n",
        "            traj.append((s, a, r, s_prime))\n",
        "            s = s_prime\n",
        "        return traj\n",
        "\n",
        "    def evaluate(self, n_play, traj_length, policy):\n",
        "        score = 0\n",
        "        for i in range(n_play):\n",
        "            score += sum(x[2] for x in self.sim_episode(policy=policy, max_iters=traj_length)) # reward\n",
        "        return score/n_play\n",
        "\n",
        "\n",
        "class QFRegressor:\n",
        "    def __init__(self, mdp, rng=np.random.default_rng(42)):\n",
        "        self.mdp = mdp\n",
        "        self.fitted = False\n",
        "        self.rng = rng\n",
        "    def fq_q_value(self, s, a):\n",
        "        raise NotImplementedError('Override me')\n",
        "\n",
        "    def fq_value(self, s):\n",
        "        return compute_value_from_q(self.mdp.actions, self.fq_q_value, s)\n",
        "\n",
        "    def fq_greedy(self, s):\n",
        "        if not self.fitted:\n",
        "            return self.rng.choice(self.mdp.actions)\n",
        "        return greedy_policy_from_q(self.mdp.actions, self.fq_q_value, s)\n",
        "\n",
        "    def fq_epsilon_greedy(self, s, eps):\n",
        "        if not self.fitted:\n",
        "            return self.rng.choice(self.mdp.actions)\n",
        "        return epsilon_greedy_policy_from_q(self.mdp.actions, self.fq_q_value, s, eps, self.rng)\n",
        "\n",
        "class NeuralRegressor(QFRegressor):\n",
        "    def initialize(self, max_iter=1000, hidden_layer_sizes=(40,40)):\n",
        "        from sklearn.neural_network import MLPRegressor\n",
        "        self.fq_models = {\n",
        "            a: MLPRegressor(hidden_layer_sizes=hidden_layer_sizes,\n",
        "                            max_iter=max_iter, learning_rate_init=0.03)\n",
        "            for a in self.mdp.actions\n",
        "        }\n",
        "        self.fitted = False\n",
        "    def fq_q_value(self, s, a):\n",
        "        return self.fq_models[a].predict(np.array(s).reshape(1,-1))[0]\n",
        "\n",
        "    def fit(self, a, X, Y):\n",
        "        self.fq_models[a].fit(X, Y)\n",
        "\n",
        "class KNNRegressor(QFRegressor):\n",
        "    def initialize(self, n_neighbors=3):\n",
        "        from sklearn.neighbors import KNeighborsRegressor\n",
        "        self.fq_models = {\n",
        "            a: KNeighborsRegressor(n_neighbors=n_neighbors)\n",
        "            for a in self.mdp.actions\n",
        "        }\n",
        "        self.fitted = False\n",
        "\n",
        "    def fq_q_value(self, s, a):\n",
        "        return self.fq_models[a].predict(np.array(s).reshape(1,-1))[0]\n",
        "\n",
        "    def fit(self, a, X, Y):\n",
        "        self.fq_models[a].fit(X, Y)\n",
        "\n",
        "def get_state_grid(x_divisions, v_divisions, mdp):\n",
        "    '''\n",
        "    Returns a list of points that subdivides the MountainCar state space\n",
        "    evenly into a grid.\n",
        "\n",
        "    Args:\n",
        "    - x_divisions : int\n",
        "    - v_divisions : int\n",
        "    - mdp : MountainCars\n",
        "    Returns:\n",
        "    - grid : list(state)\n",
        "    '''\n",
        "    grid = []\n",
        "    for pos in np.linspace(mdp.min_position, mdp.max_position, x_divisions):\n",
        "        for vel in np.linspace(-mdp.max_speed, mdp.max_speed, v_divisions):\n",
        "            grid.append((pos, vel))\n",
        "    return grid"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05a4e0c0",
      "metadata": {},
      "source": [
        "### Question\n",
        "Compute the value function from the Q function.\n",
        "\n",
        "For reference, our solution is **3** line(s) of code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23d93751",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_value_from_q(action_space, q_function, state):\n",
        "    \"\"\"Given the action space, a q_function and a state, compute the value \n",
        "    for this state using the q_function.\n",
        "    Args:\n",
        "    - action_space : tuple of actions\n",
        "    - q_function : (state, action) -> q_value : float\n",
        "    - state : state\n",
        "    Returns:\n",
        "    - value : float - the value of this state\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"Implement me!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49b0892d",
      "metadata": {},
      "source": [
        "### Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7827171",
      "metadata": {},
      "outputs": [],
      "source": [
        "def value_from_q_problem_test():\n",
        "    actions = (0, 1, 2)\n",
        "    action_scores = [-3, 1, 5.]\n",
        "    test_state = 3\n",
        "    def q(state, action):\n",
        "        assert(state == test_state)\n",
        "        assert action in actions\n",
        "        return action_scores[action]\n",
        "    def rotate(arr):\n",
        "        arr.append(arr[0])\n",
        "        arr.pop(0)\n",
        "    assert(compute_value_from_q(actions, q, test_state) == 5.)\n",
        "    rotate(action_scores)\n",
        "    assert(compute_value_from_q(actions, q, test_state) == 5.)\n",
        "    rotate(action_scores)\n",
        "    assert(compute_value_from_q(actions, q, test_state) == 5.)\n",
        "\n",
        "value_from_q_problem_test()\n",
        "\n",
        "print('Tests passed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23530364",
      "metadata": {},
      "source": [
        "## Greedy Policy from Q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a789555a",
      "metadata": {},
      "source": [
        "### Question\n",
        "Write the greedy policy given a Q function.\n",
        "\n",
        "For reference, our solution is **6** line(s) of code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d762139a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def greedy_policy_from_q(action_space, q_function, state):\n",
        "    \"\"\"Given the action space, a q_function and a state, compute the action\n",
        "    taken by the greedy policy at this state, under the q_function.\n",
        "\n",
        "    Args:\n",
        "    - action_space : tuple of actions\n",
        "    - q_function : (state, action) -> q_value : float\n",
        "    - state : state\n",
        "\n",
        "    Return:\n",
        "    - action\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"Implement me!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "737a1f54",
      "metadata": {},
      "source": [
        "### Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09356044",
      "metadata": {},
      "outputs": [],
      "source": [
        "def greedy_policy_from_q_test():\n",
        "    actions = (0, 1, 2)\n",
        "    action_scores = [-3, 1, 5.]\n",
        "    test_state = 3\n",
        "    def q(state, action):\n",
        "        assert(state == test_state)\n",
        "        assert action in actions\n",
        "        return action_scores[action]\n",
        "    def rotate(arr):\n",
        "        arr.append(arr[0])\n",
        "        arr.pop(0)\n",
        "    assert(greedy_policy_from_q(actions, q, test_state) == 2)\n",
        "    rotate(action_scores)\n",
        "    assert(greedy_policy_from_q(actions, q, test_state) == 1)\n",
        "    rotate(action_scores)\n",
        "    assert(greedy_policy_from_q(actions, q, test_state) == 0)\n",
        "\n",
        "greedy_policy_from_q_test()\n",
        "\n",
        "print('Tests passed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e4b099d",
      "metadata": {},
      "source": [
        "## Epsilon Greedy Policy from Q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93ee512b",
      "metadata": {},
      "source": [
        "### Question\n",
        "Write the epsilon greedy policy given a Q function. With probability epsilon, the policy should pick a random action, and with probability 1-epsilon, pick the greedy action. You may use the `greedy_policy_from_q` function. You may find `rng.choice` and `rng.random` useful.\n",
        "\n",
        "For reference, our solution is **6** line(s) of code.\n",
        "\n",
        "In addition to all the utilities defined at the top of the Colab notebook, the following functions are available in this question environment: `greedy_policy_from_q`. You may not need to use all of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2727668e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def epsilon_greedy_policy_from_q(action_space, q_function, state, eps, rng):\n",
        "    \"\"\"Given the action space, a q_function and a state, compute the action\n",
        "    taken by an epsilon-greedy policy at this state, under the q_function.\n",
        "\n",
        "    Args:\n",
        "    - action_space : tuple of actions\n",
        "    - q_function : (state, action) -> q_value : float\n",
        "    - state : state\n",
        "    - eps : [0, 1]\n",
        "    - rng : np.random.Generator\n",
        "\n",
        "    Return:\n",
        "    - action\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"Implement me!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a483b17d",
      "metadata": {},
      "source": [
        "### Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0d0886a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def epsilon_greedy_policy_from_q_test():\n",
        "    actions = (0, 1, 2)\n",
        "    action_scores = [-3, 1, 5.]\n",
        "    test_state = 3\n",
        "    rng = np.random.default_rng(42)\n",
        "    eps = 0.7\n",
        "    N = 1000\n",
        "    def q(state, action):\n",
        "        assert(state == test_state)\n",
        "        assert action in actions\n",
        "        return action_scores[action]\n",
        "    def rotate(arr):\n",
        "        arr.append(arr[0])\n",
        "        arr.pop(0)\n",
        "    runs = [epsilon_greedy_policy_from_q(actions, q, test_state, eps, rng) for _ in range(N)]\n",
        "    counts = [sum([1 for j in runs if j == i]) for i in actions]\n",
        "    def binary_std(p):\n",
        "        return (p * (1-p) * N)**.5\n",
        "    assert(np.abs(counts[0] - N*eps/3) < binary_std(eps/3) * 3)\n",
        "    assert(np.abs(counts[1] - N*eps/3) < binary_std(eps/3) * 3)\n",
        "    assert(np.abs(counts[2] - N*eps/3 - N * (1-eps)) < binary_std(eps/3) * 3)\n",
        "    assert(sum(counts) == N)\n",
        "\n",
        "epsilon_greedy_policy_from_q_test()\n",
        "\n",
        "print('Tests passed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cb24298",
      "metadata": {},
      "source": [
        "## Sampling points in grid\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "561ef11b",
      "metadata": {},
      "source": [
        "### Question\n",
        "Next, we'd want to sample points in the state space on which to perform Bellman backups. The following function samples the state space in an evenly spaced grid, and returns the sampled points and their corresponding transitions for every possible action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3619ebf",
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_grid_points(x_divisions, v_divisions, mdp):\n",
        "    '''\n",
        "    Samples x_divisions times v_divisions points in a grid across \n",
        "    the state space, defined by [mdp.min_position, mdp.max_position] x\n",
        "    [-mdp.max_speed, mdp.max_speed]. Then, for every action and every state, \n",
        "    sample their next transitions using the mdp.sim_transition method. \n",
        "    Returns a list of tuples, each tuple describing the sampled state and \n",
        "    its transition. \n",
        "\n",
        "    You may find the get_state_grid function defined in the utilities useful.\n",
        "\n",
        "    Args:\n",
        "    - mdp : MountainCar\n",
        "    Return:\n",
        "    - memory : list(tuple (state, action, reward, next_state))\n",
        "    '''\n",
        "    memory = []\n",
        "    for s in get_state_grid(x_divisions, v_divisions, mdp):\n",
        "        for a in mdp.actions:\n",
        "            if mdp.terminal(s):\n",
        "                r, s_prime = 0., s\n",
        "            else:\n",
        "                mdp.state = s   # reset state\n",
        "                r, s_prime = mdp.sim_transition(a)\n",
        "            memory.append((s, a, r, s_prime))\n",
        "    return memory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aea17faf",
      "metadata": {},
      "source": [
        "### Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0677fb59",
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_grid_points_test():\n",
        "    mc = MountainCar()\n",
        "    mc.force_noise = 0.\n",
        "    results = sample_grid_points(3, 4, mc)\n",
        "    my_results = [((-1.2, -0.07), 0, -1.0, (-1.2, 0)), ((-1.2, -0.07), 1, -1.0, (-1.2, 0)), ((-1.2, -0.07), 2, -1.0, (-1.2, 0)), ((-1.2, -0.023333333333333338), 0, -1.0, (-1.2, 0)), ((-1.2, -0.023333333333333338), 1, -1.0, (-1.2, 0)), ((-1.2, -0.023333333333333338), 2, -1.0, (-1.1091437292497965, 0.009085627075020343)), ((-1.2, 0.02333333333333333), 0, -1.0, (-0.8424770625831298, 0.035752293741687015)), ((-1.2, 0.02333333333333333), 1, -1.0, (-0.7424770625831298, 0.04575229374168702)), ((-1.2, 0.02333333333333333), 2, -1.0, (-0.6424770625831299, 0.05575229374168701)), ((-1.2, 0.07), 0, -1.0, (-0.4999999999999999, 0.07)), ((-1.2, 0.07), 1, -1.0, (-0.4999999999999999, 0.07)), ((-1.2, 0.07), 2, -1.0, (-0.4999999999999999, 0.07)), ((-0.30000000000000004, -0.07), 0, -1.0, (-1.0, -0.07)), ((-0.30000000000000004, -0.07), 1, -1.0, (-1.0, -0.07)), ((-0.30000000000000004, -0.07), 2, -1.0, (-1.0, -0.07)), ((-0.30000000000000004, -0.023333333333333338), 0, -1.0, (-0.7887358254009995, -0.04887358254009995)), ((-0.30000000000000004, -0.023333333333333338), 1, -1.0, (-0.6887358254009995, -0.03887358254009995)), ((-0.30000000000000004, -0.023333333333333338), 2, -1.0, (-0.5887358254009996, -0.028873582540099946)), ((-0.30000000000000004, 0.02333333333333333), 0, -1.0, (-0.32206915873433284, -0.002206915873433281)), ((-0.30000000000000004, 0.02333333333333333), 1, -1.0, (-0.22206915873433283, 0.007793084126566721)), ((-0.30000000000000004, 0.02333333333333333), 2, -1.0, (-0.12206915873433283, 0.017793084126566723)), ((-0.30000000000000004, 0.07), 0, -1.0, (0.1445975079323339, 0.044459750793233395)), ((-0.30000000000000004, 0.07), 1, -1.0, (0.24459750793233392, 0.0544597507932334)), ((-0.30000000000000004, 0.07), 2, -1.0, (0.3445975079323339, 0.06445975079323339)), ((0.6, -0.07), 0, -1.0, (-0.10000000000000009, -0.07)), ((0.6, -0.07), 1, -1.0, (-0.04319947632672838, -0.06431994763267283)), ((0.6, -0.07), 2, -1.0, (0.0568005236732716, -0.05431994763267284)), ((0.6, -0.023333333333333338), 0, -1.0, (0.3234671903399383, -0.027653280966006166)), ((0.6, -0.023333333333333338), 1, -1.0, (0.42346719033993835, -0.017653280966006164)), ((0.6, -0.023333333333333338), 2, -1.0, (0.5234671903399384, -0.007653280966006166)), ((0.6, 0.02333333333333333), 0, 0.0, (0.6, 0.02333333333333333)), ((0.6, 0.02333333333333333), 1, 0.0, (0.6, 0.02333333333333333)), ((0.6, 0.02333333333333333), 2, 0.0, (0.6, 0.02333333333333333)), ((0.6, 0.07), 0, 0.0, (0.6, 0.07)), ((0.6, 0.07), 1, 0.0, (0.6, 0.07)), ((0.6, 0.07), 2, 0.0, (0.6, 0.07))]\n",
        "    def recur_match(a, b, fn):\n",
        "        if type(a) is tuple:\n",
        "            assert type(b) is tuple\n",
        "            assert len(a) == len(b)\n",
        "            for x, y in zip(a, b):\n",
        "                if not recur_match(x, y, fn):\n",
        "                    False\n",
        "            return True\n",
        "        else:\n",
        "            return fn(a, b)\n",
        "    for r in results:\n",
        "        found = False\n",
        "        for dr in my_results:\n",
        "            if recur_match(dr, r, lambda x, y: np.abs(x - y) < 1e-6):\n",
        "              found = True\n",
        "        assert(found)\n",
        "\n",
        "sample_grid_points_test()\n",
        "\n",
        "print('Tests passed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b8e0770",
      "metadata": {},
      "source": [
        "## Sampling points from policy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "852d6f17",
      "metadata": {},
      "source": [
        "### Question\n",
        "Another way to sample points is to collect points by rolling out trajectories from a policy. Implement a function that samples several trajectories from a given policy, and concatenate the trajectories together to produce a list of (state, action, reward, next_state) tuples.\n",
        "\n",
        "For reference, our solution is **8** line(s) of code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8d64a08",
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_policy_points(policy, traj_length, num_traj, mdp):\n",
        "    \"\"\"Produce samples in the state space by rolling out a policy for\n",
        "    traj_length steps for num_traj rollouts. Use `mdp.sim_episode`\n",
        "    to obtain rollouts.\n",
        "\n",
        "    Args:\n",
        "    - policy : state -> action\n",
        "    - traj_length : int  - length of rollout\n",
        "    - num_traj : int - number of trajectories to rollout\n",
        "    - mdp : MountainCar\n",
        "    Return:\n",
        "    - memory : [tuple (state, action, reward, next_state)]\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"Implement me!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41124e53",
      "metadata": {},
      "source": [
        "### Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "860a663c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_policy_points_test():\n",
        "    rng = np.random.default_rng(3)\n",
        "    mc = MountainCar(rng=rng)\n",
        "    mc.state = 0.\n",
        "    policy = lambda s : 1+np.sign(s[1])\n",
        "    results = sample_policy_points(policy, 5, 2, mc)\n",
        "    my_results = [((-1.2, -0.07), 0, -1.0, (-1.2, 0)), ((-1.2, -0.07), 1, -1.0, (-1.2, 0)), ((-1.2, -0.07), 2, -1.0, (-1.2, 0)), ((-1.2, -0.023333333333333338), 0, -1.0, (-1.2, 0)), ((-1.2, -0.023333333333333338), 1, -1.0, (-1.2, 0)), ((-1.2, -0.023333333333333338), 2, -1.0, (-1.1091437292497965, 0.009085627075020343)), ((-1.2, 0.02333333333333333), 0, -1.0, (-0.8424770625831298, 0.035752293741687015)), ((-1.2, 0.02333333333333333), 1, -1.0, (-0.7424770625831298, 0.04575229374168702)), ((-1.2, 0.02333333333333333), 2, -1.0, (-0.6424770625831299, 0.05575229374168701)), ((-1.2, 0.07), 0, -1.0, (-0.4999999999999999, 0.07)), ((-1.2, 0.07), 1, -1.0, (-0.4999999999999999, 0.07)), ((-1.2, 0.07), 2, -1.0, (-0.4999999999999999, 0.07)), ((-0.30000000000000004, -0.07), 0, -1.0, (-1.0, -0.07)), ((-0.30000000000000004, -0.07), 1, -1.0, (-1.0, -0.07)), ((-0.30000000000000004, -0.07), 2, -1.0, (-1.0, -0.07)), ((-0.30000000000000004, -0.023333333333333338), 0, -1.0, (-0.7887358254009995, -0.04887358254009995)), ((-0.30000000000000004, -0.023333333333333338), 1, -1.0, (-0.6887358254009995, -0.03887358254009995)), ((-0.30000000000000004, -0.023333333333333338), 2, -1.0, (-0.5887358254009996, -0.028873582540099946)), ((-0.30000000000000004, 0.02333333333333333), 0, -1.0, (-0.32206915873433284, -0.002206915873433281)), ((-0.30000000000000004, 0.02333333333333333), 1, -1.0, (-0.22206915873433283, 0.007793084126566721)), ((-0.30000000000000004, 0.02333333333333333), 2, -1.0, (-0.12206915873433283, 0.017793084126566723)), ((-0.30000000000000004, 0.07), 0, -1.0, (0.1445975079323339, 0.044459750793233395)), ((-0.30000000000000004, 0.07), 1, -1.0, (0.24459750793233392, 0.0544597507932334)), ((-0.30000000000000004, 0.07), 2, -1.0, (0.3445975079323339, 0.06445975079323339)), ((0.6, -0.07), 0, -1.0, (-0.10000000000000009, -0.07)), ((0.6, -0.07), 1, -1.0, (-0.04319947632672838, -0.06431994763267283)), ((0.6, -0.07), 2, -1.0, (0.0568005236732716, -0.05431994763267284)), ((0.6, -0.023333333333333338), 0, -1.0, (0.3234671903399383, -0.027653280966006166)), ((0.6, -0.023333333333333338), 1, -1.0, (0.42346719033993835, -0.017653280966006164)), ((0.6, -0.023333333333333338), 2, -1.0, (0.5234671903399384, -0.007653280966006166)), ((0.6, 0.02333333333333333), 0, 0.0, (0.6, 0.02333333333333333)), ((0.6, 0.02333333333333333), 1, 0.0, (0.6, 0.02333333333333333)), ((0.6, 0.02333333333333333), 2, 0.0, (0.6, 0.02333333333333333)), ((0.6, 0.07), 0, 0.0, (0.6, 0.07)), ((0.6, 0.07), 1, 0.0, (0.6, 0.07)), ((0.6, 0.07), 2, 0.0, (0.6, 0.07))]\n",
        "    def recur_match(a, b, fn):\n",
        "        if type(a) is tuple:\n",
        "            assert type(b) is tuple\n",
        "            assert len(a) == len(b)\n",
        "            for x, y in zip(a, b):\n",
        "                if not recur_match(x, y, fn):\n",
        "                    False\n",
        "            return True\n",
        "        else:\n",
        "            return fn(a, b)\n",
        "    for r in results:\n",
        "        found = False\n",
        "        for dr in my_results:\n",
        "            if recur_match(dr, r, lambda x, y: np.abs(x - y) < 1e-6):\n",
        "              found = True\n",
        "        assert(found)\n",
        "\n",
        "sample_policy_points_test()\n",
        "\n",
        "print('Tests passed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67e7fa70",
      "metadata": {},
      "source": [
        "## Fitted Q Visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23972296",
      "metadata": {},
      "source": [
        "### Question\n",
        "Complete the `fitted_Q_learn` function given in the colab notebook. You're now ready to conduct fitted Q learning on the Mountain Car Problem! You may use either sampling method (grid or policy), and either regression method (`KNNRegressor` or `NeuralRegressor`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b011232",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython import display as display\n",
        "matplotlib.rc('animation', html='jshtml')\n",
        "\n",
        "def visualize_qf(qf_regressor):\n",
        "    min_x, max_x = qf_regressor.mdp.min_position, qf_regressor.mdp.max_position\n",
        "    min_v, max_v = -qf_regressor.mdp.max_speed, qf_regressor.mdp.max_speed\n",
        "    vf = np.array([\n",
        "          [\n",
        "              qf_regressor.fq_value((x, v))\n",
        "            for x in np.linspace(min_x, max_x, 50)\n",
        "          ]\n",
        "        for v in np.linspace(min_v, max_v, 50)\n",
        "    ])\n",
        "    im = plt.imshow(vf, extent=(min_x, max_x, min_v, max_v), aspect='auto')\n",
        "    plt.colorbar(im)\n",
        "\n",
        "def visualize_traj(traj):\n",
        "    '''\n",
        "    Visualizes a trajectory. Call with the output of MountainCar.sim_episode\n",
        "\n",
        "    Args:\n",
        "    - traj : [tuple (state, action, reward, next_state)]\n",
        "    '''\n",
        "    # based off https://github.com/mpatacchiola/dissecting-reinforcement-learning/blob/master/environments/mountain_car.py#L105\n",
        "    mode = 'jupyter'\n",
        "    file_path='./mountain_car.mp4'\n",
        "    # Plot init\n",
        "\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111, autoscale_on=False, xlim=(-1.2, 0.6), ylim=(-1.1, 1.1))\n",
        "    ax.grid(False)  # disable the grid\n",
        "    x_sin = np.linspace(start=-1.2, stop=0.6, num=100)\n",
        "    y_sin = np.sin(3 * x_sin)\n",
        "    # plt.plot(x, y)\n",
        "    ax.plot(x_sin, y_sin)  # plot the sine wave\n",
        "    # line, _ = ax.plot(x, y, 'o-', lw=2)\n",
        "    dot, = ax.plot([], [], 'ro')\n",
        "    time_text = ax.text(0.05, 0.9, '', transform=ax.transAxes)\n",
        "    _position_list = [s[0][0] for s in traj]\n",
        "    _delta_t = .6\n",
        "\n",
        "    def _init():\n",
        "        dot.set_data([], [])\n",
        "        time_text.set_text('')\n",
        "        return dot, time_text\n",
        "\n",
        "    def _animate(i):\n",
        "        x = _position_list[i]\n",
        "        y = np.sin(3 * x)\n",
        "        dot.set_data(x, y)\n",
        "        time_text.set_text(\"Time: \" + str(np.round(i*_delta_t, 1)) + \"s\" + '\\n' + \"Frame: \" + str(i))\n",
        "        return dot, time_text\n",
        "\n",
        "    ani = animation.FuncAnimation(fig, _animate, np.arange(1, len(_position_list)),\n",
        "                                    blit=True, init_func=_init, repeat=True, interval=_delta_t * 1000)\n",
        "\n",
        "    if mode == 'gif':\n",
        "        ani.save(file_path, writer='imagemagick', fps=int(1/_delta_t))\n",
        "    elif mode == 'mp4':\n",
        "        ani.save(file_path, fps=int(1/_delta_t), writer='avconv', codec='libx264')\n",
        "    elif mode == 'jupyter':\n",
        "        video = ani.to_jshtml()\n",
        "        html = display.HTML(video)\n",
        "        display.display(html)\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "def fitted_Q_learn(mdp, sampler, qf_regressor, iters):\n",
        "    '''\n",
        "    Takes in a MountainCar instance, a sampling method, a fitted q\n",
        "    regression method, and the number of iterations. Runs fitted Q\n",
        "    learning with that many iterations.\n",
        "\n",
        "    Args:\n",
        "    - mdp : MountainCar\n",
        "    - sampler : (mdp : MountainCar) -> memory : [tuple (state, action, reward, next_state)]\n",
        "    - qf_regressor : QFRegressor\n",
        "    - iters : int\n",
        "    '''\n",
        "    PRINT_EPOCH = 6\n",
        "    qf_regressor.initialize()\n",
        "\n",
        "    for it in range(iters):\n",
        "        Xd = dict([(a, []) for a in mdp.actions])\n",
        "        Yd = dict([(a, []) for a in mdp.actions])\n",
        "        memory = sampler(mdp)\n",
        "        for (s, a, r, s_prime) in memory:\n",
        "            if it == 0 or mdp.terminal(s):\n",
        "                # TODO: IMPLEMENT ME\n",
        "                raise NotImplementedError('Set v = something here')\n",
        "            else:\n",
        "                # TODO: IMPLEMENT ME. You may find mdp.discount_factor \n",
        "                # and qf_regressor.fq_value useful.\n",
        "                raise NotImplementedError('Set v = something here')\n",
        "            Xd[a].append(s)\n",
        "            Yd[a].append(np.array([v]))\n",
        "        for a in mdp.actions:\n",
        "            X = np.vstack(Xd[a])\n",
        "            Y = np.vstack(Yd[a])\n",
        "            Y = Y[:, 0]\n",
        "            qf_regressor.fit(a, X, Y)\n",
        "        qf_regressor.fitted = True\n",
        "        print(f'Iteration {it}:  {mdp.evaluate(n_play=10, traj_length=100, policy=qf_regressor.fq_greedy)}')\n",
        "        if it % PRINT_EPOCH == PRINT_EPOCH-1:\n",
        "            visualize_qf(qf_regressor)\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a10d86bf",
      "metadata": {},
      "source": [
        "This shows an example of running fitted Q learning using KNNRegressor with policy sampling. Try it with different different regressors, sampling methods and sampling parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9265ba36",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_fitted_q_example_1():\n",
        "    NUM_ITERS = 10\n",
        "    TRAJ_LENGTH = 40\n",
        "    NUM_ROLLOUTS = 20\n",
        "    EPSILON = 0.4\n",
        "    mc = MountainCar()\n",
        "    qf_regressor = KNNRegressor(mc)\n",
        "    sampler = partial(sample_policy_points, lambda s: qf_regressor.fq_epsilon_greedy(s, EPSILON), TRAJ_LENGTH, NUM_ROLLOUTS)\n",
        "    fitted_Q_learn(\n",
        "        mdp=mc, \n",
        "        sampler=sampler,\n",
        "        qf_regressor=qf_regressor,\n",
        "        iters=NUM_ITERS)\n",
        "    print('expected reward =', mc.evaluate(100, 100, qf_regressor.fq_greedy))\n",
        "    visualize_traj(mc.sim_episode(policy=qf_regressor.fq_greedy, max_iters=100))\n",
        "    plt.show()\n",
        "    visualize_qf(qf_regressor)\n",
        "    plt.show()\n",
        "def run_fitted_q_example_2():\n",
        "    NUM_ITERS = 10\n",
        "    X_DIVISIONS = 15\n",
        "    V_DIVISIONS = 15\n",
        "    mc = MountainCar()\n",
        "    qf_regressor = KNNRegressor(mc)\n",
        "    sampler = partial(sample_grid_points, X_DIVISIONS, V_DIVISIONS)\n",
        "    fitted_Q_learn(\n",
        "        mdp=mc, \n",
        "        sampler=sampler,\n",
        "        qf_regressor=qf_regressor,\n",
        "        iters=NUM_ITERS)\n",
        "    print('expected reward =', mc.evaluate(100, 100, qf_regressor.fq_greedy))\n",
        "    visualize_traj(mc.sim_episode(policy=qf_regressor.fq_greedy, max_iters=100))\n",
        "    plt.show()\n",
        "    visualize_qf(qf_regressor)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bb5082b",
      "metadata": {
        "id": "1bb5082b"
      },
      "source": [
        "## Imports and Utilities\n",
        "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a5684c4",
      "metadata": {
        "id": "9a5684c4"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from math import sqrt, log\n",
        "import abc\n",
        "import numpy as np\n",
        "import functools\n",
        "\n",
        "\n",
        "class MDP:\n",
        "    \"\"\"A Markov Decision Process.\"\"\"\n",
        "\n",
        "    @property\n",
        "    @abc.abstractmethod\n",
        "    def state_space(self):\n",
        "        \"\"\"Representation of the MDP state set.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Override me\")\n",
        "\n",
        "    @property\n",
        "    @abc.abstractmethod\n",
        "    def action_space(self):\n",
        "        \"\"\"Representation of the MDP action set.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Override me\")\n",
        "\n",
        "    @property\n",
        "    def temporal_discount_factor(self):\n",
        "        \"\"\"Gamma, defaults to 1.\n",
        "        \"\"\"\n",
        "        return 1.\n",
        "\n",
        "    @property\n",
        "    def horizon(self):\n",
        "        \"\"\"H, defaults to inf.\n",
        "        \"\"\"\n",
        "        return float(\"inf\")\n",
        "\n",
        "    def state_is_terminal(self, state):\n",
        "        \"\"\"Designate certain states as terminal (done) states.\n",
        "\n",
        "        Defaults to False.\n",
        "\n",
        "        Args:\n",
        "            state: A state.\n",
        "\n",
        "        Returns:\n",
        "            state_is_terminal : A bool.\n",
        "        \"\"\"\n",
        "        return False\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def get_reward(self, state, action, next_state=None):\n",
        "        \"\"\"Return (deterministic) reward for executing action\n",
        "        in state.\n",
        "\n",
        "        Args:\n",
        "            state: A current state.\n",
        "            action: An action.\n",
        "            next_state: Optional. A next state.\n",
        "\n",
        "        Returns:\n",
        "            reward : Single time step reward.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Override me\")\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def get_transition_distribution(self, state, action):\n",
        "        \"\"\"Return a distribution over next states.\n",
        "\n",
        "        The form of this distribution will vary, e.g., depending\n",
        "        on whether the MDP has discrete or continuous states.\n",
        "\n",
        "        Args:\n",
        "            state: A current state.\n",
        "            action: An action.\n",
        "\n",
        "        Returns:\n",
        "            next_state_distribution: Distribution over next states.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Override me\")\n",
        "\n",
        "    def sample_next_state(self, state, action, rng=np.random):\n",
        "        \"\"\"Sample a next state from the transition distribution.\n",
        "\n",
        "        This function may be overwritten by subclasses when the explicit\n",
        "        distribution is too large to enumerate.\n",
        "\n",
        "        Args:\n",
        "            state: A state from the state space.\n",
        "            action: An action from the action space.\n",
        "            rng: A random number generator.\n",
        "\n",
        "        Returns:\n",
        "            next_state: A sampled next state from the state space.\n",
        "        \"\"\"\n",
        "        next_state_dist = self.get_transition_distribution(state, action)\n",
        "        next_states, probs = zip(*next_state_dist.items())\n",
        "        next_state_index = rng.choice(len(next_states), p=probs)\n",
        "        next_state = next_states[next_state_index]\n",
        "        return next_state\n",
        "\n",
        "\n",
        "class POMDP(MDP):\n",
        "    \"\"\"A partially observable Markov decision process (POMDP).\"\"\"\n",
        "\n",
        "    @property\n",
        "    @abc.abstractmethod\n",
        "    def observation_space(self):\n",
        "        \"\"\"Representation of the POMDP observation space.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Override me\")\n",
        "\n",
        "    @abc.abstractclassmethod\n",
        "    def get_observation_distribution(self, next_state, action):\n",
        "        \"\"\"Return a distribution over the observations.\n",
        "\n",
        "        The form of this distribution will vary, e.g., depending\n",
        "        on whether the MDP has discrete or continuous observation\n",
        "        spaces.\n",
        "\n",
        "        Args:\n",
        "            next_state: The next state.\n",
        "            action: The action taken.\n",
        "\n",
        "        Returns:\n",
        "            observation_distribution: Distribution over the observation.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Override me\")\n",
        "\n",
        "\n",
        "class LambdaMDP(MDP):\n",
        "    \"\"\"A helper class that creates a MDP class based on a set of functions.\n",
        "    See the constructor for details.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, state_space, action_space, state_is_terminal_fn, get_reward_fn, get_transition_distribution_fn, temporal_discount_factor=1.0):\n",
        "        \"\"\"\n",
        "        Construct a MDP class based on a set of function definitions.\n",
        "\n",
        "        Args:\n",
        "            state_space: The set of possible states.\n",
        "            action_space: The set of possible actions.\n",
        "            state_is_terminal_fn: A callable function: state_is_terminal_fn(state) -> bool,\n",
        "                mapping a state to a boolean value indicating whether\n",
        "                the state is a terminal state.\n",
        "            get_reward_fn: A callable function: get_reward_fn(state, action, next_state) -> float,\n",
        "                mapping a (s, a, s') tuple to a float reward value.\n",
        "            get_transition_distribution_fn: A callable function:\n",
        "                get_transition_distribution_fn(state, action) -> distribution of the next state.\n",
        "                Note that the return value for this function must be a discrete distribution.\n",
        "            temporal_discount_factor: A float number, the temporal discount factor of the MDP.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.state_space_v = state_space\n",
        "        self.action_space_v = action_space\n",
        "        self.state_is_terminal = state_is_terminal_fn\n",
        "        self.get_reward = get_reward_fn\n",
        "        self.get_transition_distribution = get_transition_distribution_fn\n",
        "        self.temporal_discount_factor_v = temporal_discount_factor\n",
        "\n",
        "    @property\n",
        "    def state_space(self):\n",
        "        return self.state_space_v\n",
        "\n",
        "    @property\n",
        "    def action_space(self):\n",
        "        return self.action_space_v\n",
        "\n",
        "    @property\n",
        "    def temporal_discount_factor(self):\n",
        "        return self.temporal_discount_factor_v\n",
        "\n",
        "\n",
        "class DiscreteDistribution(object):\n",
        "    \"\"\"A discrete distribution, represneted as a dictionary.\"\"\"\n",
        "\n",
        "    eps = 1e-6\n",
        "\n",
        "    def __init__(self, prob_dict):\n",
        "        \"\"\"Construct a discrete distribution based on a probability dictionary.\n",
        "        The dictionary might be \"sparse\", in which case the omitted entries are\n",
        "        treated as zero-probability values.\n",
        "\n",
        "        Note that, even if the random varaible takes values from a continuous space,\n",
        "        (e.g., all real numbers), we can still define a \"discrete distribution\",\n",
        "        that is, a distribution only has mass on a finite set of points.\n",
        "        For example, we can define a distribution on R: {0: 0.5, 1: 0.5}.\n",
        "        Implicitly, all values not in the prob_dict will be treated as\n",
        "        zero-probability.\n",
        "\n",
        "        Example:\n",
        "\n",
        "        ```\n",
        "        p = DiscreteDistribution({'x': 0.0, 'y': 0.6, 'z': 0.4})\n",
        "        print(p.p('x'))  # 0.0\n",
        "        for x in p:  # iterate over the set of possible values.\n",
        "            print(x, p.p(x))  # should print y 0.6 z 0.4\n",
        "        for x, p_x in p.items():  # just like iterating over a Python dict.\n",
        "            print(x, p_x)  # should print y 0.6 z 0.4\n",
        "        ```\n",
        "        Note that, during iteration, zero-probability values will be omitted.\n",
        "\n",
        "        Args:\n",
        "            prob_dict: A dictionary, mapping elements in the domain to a float\n",
        "                number. The dictionary might be sparse. It should always\n",
        "                sum up to one (thus being a valid distribution.)\n",
        "        \"\"\"\n",
        "        self.prob_dict = prob_dict\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Iterate over the support set.\"\"\"\n",
        "        yield from self.support()\n",
        "\n",
        "    def support(self):\n",
        "        \"\"\"Itearte over the support set of the distribution. That is,\n",
        "        values with a non-zero probability mass.\n",
        "        \"\"\"\n",
        "        for k, v in self.prob_dict.items():\n",
        "            if v > 0:\n",
        "                yield k\n",
        "\n",
        "    def items(self):\n",
        "        \"\"\"Iterate over the distribution. Generates a list of (x, p(x)) pairs.\n",
        "        This function will ignore zero-probability values in the prob_dict.\n",
        "        \"\"\"\n",
        "        for k, v in self.prob_dict.items():\n",
        "            if v > 0:\n",
        "                yield k, v\n",
        "\n",
        "    def p(self, value):\n",
        "        \"\"\"Evaluate the proabbility of a value in the support set.\n",
        "\n",
        "        Args:\n",
        "            value: An object in the domain of the distribution.\n",
        "\n",
        "        Returns:\n",
        "            p: A float, indicating p(value). For values not in the support (prob_dict),\n",
        "                the probability is assumed to be zero.\n",
        "        \"\"\"\n",
        "        return self.prob_dict.get(value, 0.)\n",
        "\n",
        "    def renormalize(self):\n",
        "        \"\"\"Renormalize the distribution to ensure that the probabilities sum up to 1.\n",
        "\n",
        "        Returns:\n",
        "            self\n",
        "        \"\"\"\n",
        "        z = sum(self.prob_dict.values())\n",
        "        assert z > 0, 'Degenerated probability distribution.'\n",
        "        self.prob_dict = {k: v / z for k, v in self.prob_dict.items()}\n",
        "        return self\n",
        "\n",
        "    def check_normalization(self):\n",
        "        \"\"\"Check if the prob dict is correctly normalized (i.e., should sum up to 1).\"\"\"\n",
        "        assert 1 - type(self).eps < sum(self.prob_dict.values()) < 1 + type(self).eps\n",
        "\n",
        "    def max(self):\n",
        "        \"\"\"Return argmax_x p(x).\n",
        "\n",
        "        Returns:\n",
        "            arg_max: An object in the support, argmax_x p(x).\n",
        "        \"\"\"\n",
        "        return max(self.prob_dict, key=lambda x: (self.prob_dict[x], x))\n",
        "\n",
        "    def draw(self, rng=None):\n",
        "        if rng is None:\n",
        "            rng = np.random\n",
        "        keys = list(self.prob_dict.keys())\n",
        "        probs = [self.prob_dict[k] for k in keys]\n",
        "        return keys[rng.choice(len(keys), p=probs)]\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.prob_dict)\n",
        "\n",
        "    def as_tuple(self):\n",
        "        return tuple((self.prob_dict.get(k), k) for k in sorted(self.support()))\n",
        "\n",
        "    def __lt__(self, other):\n",
        "        return self.as_tuple() < other.as_tuple()\n",
        "\n",
        "    def __gt__(self, other):\n",
        "        return self.as_tuple() > other.as_tuple()\n",
        "\n",
        "\n",
        "def OnehotDiscreteDistribution(obj):\n",
        "    \"\"\"Create a DiscreteDistribution of p(obj) = 1.\"\"\"\n",
        "    return DiscreteDistribution({obj: 1.0})\n",
        "\n",
        "\n",
        "def UniformDiscreteDistribution(support):\n",
        "    \"\"\"Create a DiscreteDistribution that is uniform. That is, for any object x, p(x) = 1 / |support|.\"\"\"\n",
        "    return DiscreteDistribution({x: 1 / len(support) for x in support})\n",
        "\n",
        "# Our RobotChargingPOMDP\n",
        "\n",
        "class RobotChargingPOMDP(POMDP):\n",
        "    DEF_MOVE_SUCCESS = 0.8\n",
        "    DEF_OBS_IF_THERE = 0.9\n",
        "    DEF_OBS_IF_NOT_THERE = 0.4\n",
        "    DEF_C_MOVE = 0.5\n",
        "    DEF_C_LOOK = 0.1\n",
        "    DEF_GAMMA = 0.9\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        p_move_success=DEF_MOVE_SUCCESS, p_obs_if_there=DEF_OBS_IF_THERE, p_obs_if_not_there=DEF_OBS_IF_NOT_THERE,\n",
        "        c_move=DEF_C_MOVE, c_look=DEF_C_LOOK,\n",
        "        gamma=DEF_GAMMA\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Create the Robot Charging POMDP.\n",
        "\n",
        "        Args:\n",
        "            p_move_success (float): the probability that a move action is successful.\n",
        "            p_obs_if_there (float): the probability of return 1 when looking at a location with the charger.\n",
        "            p_obs_if_not_there (float): the probability of return 1 when looking at a location without a charger.\n",
        "            c_move (float): the cost of a move action.\n",
        "            c_look (float): the cost of a look action.\n",
        "            gamma (float): the temporal discount factor.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.p_move_success = p_move_success\n",
        "        self.p_obs_if_there = p_obs_if_there\n",
        "        self.p_obs_if_not_there = p_obs_if_not_there\n",
        "        self.c_move = c_move\n",
        "        self.c_look = c_look\n",
        "        self.gamma = gamma\n",
        "\n",
        "    @property\n",
        "    def state_space(self):\n",
        "        \"\"\"\n",
        "        Three \"normal\" states: 0, 1, 2, indicating the position of the charger.\n",
        "        One \"terminal\" state T. Executing the \"charge\" action will reach this\n",
        "        terminal state. And the state is absorbing. The robot will deterministically\n",
        "        transition to this terminal state when we execute the c action.\n",
        "        \"\"\"\n",
        "        return {0, 1, 2, 'T'}\n",
        "\n",
        "    @property\n",
        "    def action_space(self):\n",
        "        # lx: look(x)\n",
        "        # mxy: move(start=x, target=y)\n",
        "        # c: charge\n",
        "        # nop: NOP\n",
        "        return {'l0', 'l1', 'l2', 'm01', 'm12', 'm20', 'c', 'nop'}\n",
        "\n",
        "    @property\n",
        "    def observation_space(self):\n",
        "        return {0, 1}\n",
        "\n",
        "    @property\n",
        "    def temporal_discount_factor(self):\n",
        "        return self.gamma\n",
        "\n",
        "    def state_is_terminal(self, state):\n",
        "        return state == 'T'\n",
        "\n",
        "    def get_reward(self, state, action, next_state=None):\n",
        "        if action == 'nop':\n",
        "            return 0\n",
        "        elif action == 'c':\n",
        "            if state == 0:\n",
        "                return 10\n",
        "            else:\n",
        "                return -100\n",
        "        elif action.startswith('m'):\n",
        "            return -self.c_move\n",
        "        else:  # look\n",
        "            return -self.c_look\n",
        "\n",
        "    def get_transition_distribution(self, state, action):\n",
        "        if action == 'c':\n",
        "            return OnehotDiscreteDistribution('T')\n",
        "        elif action.startswith('m'):\n",
        "            start, target = int(action[1]), int(action[2])\n",
        "            if state == start:\n",
        "                return DiscreteDistribution({target : self.p_move_success, start : 1 - self.p_move_success})\n",
        "        return OnehotDiscreteDistribution(state)\n",
        "\n",
        "    def get_observation_distribution(self, next_state, action):\n",
        "        if action.startswith('l'):\n",
        "            target = int(action[1])\n",
        "            if next_state == target:\n",
        "                return DiscreteDistribution({0: 1 - self.p_obs_if_there, 1: self.p_obs_if_there})\n",
        "            else:\n",
        "                return DiscreteDistribution({0: 1 - self.p_obs_if_not_there, 1: self.p_obs_if_not_there})\n",
        "        return OnehotDiscreteDistribution(0)\n",
        "\n",
        "\n",
        "\n",
        "def bellman_backup(s, V, mdp):\n",
        "    \"\"\"Look ahead one step and propose an update for the value of s.\n",
        "\n",
        "    You can assume that the mdp is either infinite or indefinite\n",
        "    horizon (that is, mdp.horizon is inf).\n",
        "\n",
        "    Args:\n",
        "        s: A state.\n",
        "        V: A dict, V[state] -> value.\n",
        "        mdp: An MDP.\n",
        "\n",
        "    Returns:\n",
        "        vs: new value estimate for s.\n",
        "    \"\"\"\n",
        "\n",
        "    assert mdp.horizon == float(\"inf\")\n",
        "    vs = -float(\"inf\")\n",
        "    for a in mdp.action_space:\n",
        "        qsa = 0.\n",
        "        for ns, p in mdp.get_transition_distribution(s, a).items():\n",
        "            r = mdp.get_reward(s, a, ns)\n",
        "            qsa += p * (r + mdp.temporal_discount_factor * V[ns])\n",
        "        vs = max(qsa, vs)\n",
        "    return vs\n",
        "\n",
        "\n",
        "def value_iteration(mdp, max_num_iters=1000, change_threshold=1e-4):\n",
        "    \"\"\"Run value iteration for a certain number of iterations or until\n",
        "    the max change between iterations is below a threshold.\n",
        "\n",
        "    You can assume that the mdp is either infinite or indefinite\n",
        "    horizon (that is, mdp.horizon is inf).\n",
        "\n",
        "    Args:\n",
        "        mdp: An MDP.\n",
        "        max_num_iters: An int representing the maximum number of\n",
        "        iterations to run value iteration before giving up.\n",
        "        change_threshold: A float used to determine when value iteration\n",
        "        has converged and it is safe to terminate.\n",
        "\n",
        "    Returns:\n",
        "        V:  A dict, V[state] -> value.\n",
        "        it: The number of iterations before convergence.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize V to all zeros\n",
        "    V = {s: 0. for s in mdp.state_space}\n",
        "\n",
        "    for it in range(max_num_iters):\n",
        "        next_V = {}\n",
        "        max_change = 0.\n",
        "        for s in mdp.state_space:\n",
        "            if mdp.state_is_terminal(s):\n",
        "                next_V[s] = 0.\n",
        "            else:\n",
        "                next_V[s] = bellman_backup(s, V, mdp)\n",
        "            max_change = max(abs(next_V[s] - V[s]), max_change)\n",
        "        V = next_V\n",
        "        if max_change < change_threshold:\n",
        "            break\n",
        "    return V, it\n",
        "\n",
        "\n",
        "def qsa_from_vs(mdp, V):\n",
        "    \"\"\"Compute Q(s, a) based on V(s).\n",
        "\n",
        "    Args:\n",
        "        mdp: An MDP.\n",
        "        V: A dict, V[state] -> value. Typically, this is computed by value_iteration.\n",
        "\n",
        "    Returns:\n",
        "        Q: A dict, Q[state, action] -> value.\n",
        "    \"\"\"\n",
        "\n",
        "    Q = dict()\n",
        "    for s in mdp.state_space:\n",
        "        if not mdp.state_is_terminal(s):\n",
        "            for a in mdp.action_space:\n",
        "                qsa = 0.\n",
        "                for ns, p in mdp.get_transition_distribution(s, a).items():\n",
        "                    r = mdp.get_reward(s, a, ns)\n",
        "                    qsa += p * (r + mdp.temporal_discount_factor * V[ns])\n",
        "                Q[s, a] = qsa\n",
        "        else:\n",
        "            for a in mdp.action_space:\n",
        "                Q[s, a] = 0\n",
        "    return Q\n",
        "\n",
        "\n",
        "def expectimax_search(initial_state, mdp, horizon, return_Q=False):\n",
        "    \"\"\"Use expectimax search to determine a next action.\n",
        "\n",
        "    Note that we're just computing the single next action to\n",
        "    take, we do not need to store the entire partial V.\n",
        "\n",
        "    Horizon is given as a separate argument so that we can use\n",
        "    expectimax search with receding horizon control, for example,\n",
        "    even if mdp.horizon is inf.\n",
        "\n",
        "    Args:\n",
        "        initial_state: A state in the mdp.\n",
        "        mdp (MDP): An MDP.\n",
        "        horizon (int): An int horizon.\n",
        "        return_Q (bool): A boolean value. If true, also return the Q value\n",
        "            at the root instead of the action.\n",
        "\n",
        "    Returns:\n",
        "        action: An action in the mdp.\n",
        "        Q: The Q value at the root state (only when return_Q is True).\n",
        "    \"\"\"\n",
        "    A = sorted(mdp.action_space)\n",
        "    R = mdp.get_reward\n",
        "    P = mdp.get_transition_distribution\n",
        "    gm = mdp.temporal_discount_factor\n",
        "    ts = mdp.state_is_terminal\n",
        "\n",
        "    # Cache the V(s, h)'s that have been computed.\n",
        "    def V(s, h):\n",
        "        if h == horizon or ts(s):\n",
        "            return 0\n",
        "        return max(Q(s, a, h) for a in A)\n",
        "\n",
        "    def Q(s, a, h):\n",
        "        psa = P(s, a)\n",
        "        # psa is a DiscreteDistribution over beliefs.  ns is a belief.\n",
        "        return sum(psa.p(ns) * (R(s, a, ns) + gm * V(ns, h+1)) for ns in psa)\n",
        "\n",
        "    Q_values = {a: Q(initial_state, a, 0) for a in A}\n",
        "    if return_Q:\n",
        "        return max(A, key=Q_values.get), Q_values\n",
        "    return max(A, key=Q_values.get)\n",
        "\n",
        "\n",
        "def transition_update(pomdp, belief, action):\n",
        "    \"\"\"Compute p(s') from a prior distribution of p(s) based on the transition\n",
        "    distribution p(s, action, s').\n",
        "\n",
        "    Args:\n",
        "        pomdp (POMDP): A POMDP object.\n",
        "        belief (DiscreteDistribution): A distribution over the current state s.\n",
        "        action: The action to be executed.\n",
        "\n",
        "    Returns:\n",
        "        updated_belief (DiscreteDistribution): A distribution over the next state s'.\n",
        "    \"\"\"\n",
        "    updated = {x: 0.0 for x in pomdp.state_space}\n",
        "    for s in belief:\n",
        "        prob_s = belief.p(s)\n",
        "        for s_prime, transition_prob in pomdp.get_transition_distribution(s, action).items():\n",
        "            updated[s_prime] += transition_prob * prob_s\n",
        "    # Note that we don't necessarily need to renormalize the distribution (it is self-normalized!)\n",
        "    # The added `.renormalize()` part is really just for numeric stability.\n",
        "    return DiscreteDistribution(updated).renormalize()\n",
        "\n",
        "\n",
        "def observation_update(pomdp, belief, action, observation):\n",
        "    \"\"\"Compute p(s' | observation, action) following the Bayes rule.\n",
        "        p(s' | o, a) is proportional to p(s' | a) * p(o | s', a).\n",
        "\n",
        "    Args:\n",
        "        pomdp (POMDP): A POMDP object.\n",
        "        belief (DiscreteDistribution): The distribution over the next state: p(s' | a).\n",
        "            Typically, this is the output of the transition_update() function.\n",
        "        action: The action taken.\n",
        "        observation: The observation.\n",
        "\n",
        "    Returns:\n",
        "        posterior (DiscreteDistribution): The updated belief over the next state s'.\n",
        "            Normalized!\n",
        "    \"\"\"\n",
        "    posterior = {x: 0.0 for x in pomdp.state_space}\n",
        "    for s in belief:\n",
        "        posterior[s] = belief.p(s) * pomdp.get_observation_distribution(s, action).p(observation)\n",
        "    return DiscreteDistribution(posterior).renormalize()\n",
        "\n",
        "\n",
        "def belief_filter(pomdp, belief, action, observation):\n",
        "    \"\"\"Compute the updated belief over the states based on the current action and obervation.\n",
        "\n",
        "    Specifically, the process is:\n",
        "        1. the agent is at state s, and has a belief about its current state p(s).\n",
        "        2. the agent takes an action a, and has a belief about its next state p(s' | a),\n",
        "            computed by transition_update.\n",
        "        3. the agent observes o, which follows the observation model of the POMDP p(o | s', a).\n",
        "        4. the agent updates its belief over the next state p(s' | o, a), following the Bayes rule.\n",
        "\n",
        "    Args:\n",
        "        pomdp (POMDP): A POMDP object.\n",
        "        belief (DiscreteDistribution): The belief about the agent's current state.\n",
        "        action: The action taken.\n",
        "        observation: The observation.\n",
        "\n",
        "    Returns:\n",
        "        next_belief: The belief about the next state by taking into consideration the action\n",
        "            at this step and the observation.\n",
        "    \"\"\"\n",
        "    return observation_update(pomdp, transition_update(pomdp, belief, action),\n",
        "                              action=action, observation=observation)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4f6815f",
      "metadata": {
        "id": "c4f6815f"
      },
      "source": [
        "## Belief-Space MDP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d9b3041",
      "metadata": {
        "id": "6d9b3041"
      },
      "source": [
        "### Question\n",
        "In this section, you will implement a function `create_belief_mdp`, that transforms a POMDP into a belief-space MDP.\n",
        "    We have provided the basic skeleton for you. In particular, you only need to implement the get_reward and the get_transition_distribution\n",
        "    function for the Belief MDP.  You can use the function `belief_filter(pomdp, b, a, o)` which is already defined; the implementaation of that function is available in the colab.\n",
        "\n",
        "For reference, our solution is **81** line(s) of code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b92c8615",
      "metadata": {
        "id": "b92c8615"
      },
      "outputs": [],
      "source": [
        "def create_belief_mdp(pomdp):\n",
        "    \"\"\"Constructs a belief-space MDP from a POMDP.\n",
        "\n",
        "    Args:\n",
        "        pomdp: The input POMDP object.\n",
        "\n",
        "    Returns:\n",
        "        belief_mdp: The constructed belief-space MDP.\n",
        "    \"\"\"\n",
        "    def state_is_terminal(belief):\n",
        "        \"\"\"The state_is_terminal function for the belief-space MDP. It returns true iff. all possible states\n",
        "        in the belief are terminal states.\n",
        "\n",
        "        Args:\n",
        "            belief: A DiscreteDistribution of the state.\n",
        "\n",
        "        Returns:\n",
        "            is_terminal: Whether the current belief is a \"terminal\" belief.\n",
        "        \"\"\"\n",
        "        for state, p in belief.items():\n",
        "            if p > 0 and not pomdp.state_is_terminal(state):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def get_reward(belief, action, next_belief=None):\n",
        "        \"\"\"Compute the expected reward function for the belief-space MDP.\n",
        "\n",
        "        You only need to implement the case where the original reward function only\n",
        "        depends on the state and the action (but not the next state).\n",
        "\n",
        "        In this case, the reward function of the belief-space MDP will be only a function\n",
        "        of belief and action, but not next_belief.\n",
        "\n",
        "        In general (where the reward function if a function of state, action, and next_action),\n",
        "        in order to compute the expected reward, we need to also marginalize over the next state\n",
        "        distribution (which is next_belief).\n",
        "\n",
        "        Args:\n",
        "            belief: A DiscreteDistribution of the state.\n",
        "            action: An action.\n",
        "            next_belief: A DiscreteDistribution of the next state. Should be ignored (see above).\n",
        "\n",
        "        Returns:\n",
        "            reward: the expected reward at this step.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_transition_distribution(belief, action):\n",
        "        \"\"\"Compute the transition distribution for an input belief and an action.\n",
        "\n",
        "        Specifically, the output will be a distribution over beliefs. That is, a distribution over\n",
        "        distributions. Since we have restricted our observation space to be finite, the\n",
        "        possible next belief is also a finite space. Thus, we can still use a DiscreteDistribution\n",
        "        object to represent the distribution over the next belief.\n",
        "\n",
        "        Args:\n",
        "            belief: A DiscreteDistribution of the state.\n",
        "            action: An action.\n",
        "\n",
        "        Returns:\n",
        "            next_belief: A DiscreteDistribution of the next state.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    # Construct a new MDP based on the functions defined above.\n",
        "    return LambdaMDP(\n",
        "        state_space=None,  # We are not going to specify the state space explicitly (it's a continuous space).\n",
        "        action_space=pomdp.action_space,\n",
        "        state_is_terminal_fn=state_is_terminal,\n",
        "        get_reward_fn=get_reward,\n",
        "        get_transition_distribution_fn=get_transition_distribution,\n",
        "        temporal_discount_factor=pomdp.temporal_discount_factor\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d783ec49",
      "metadata": {
        "id": "d783ec49"
      },
      "source": [
        "### Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc87b5bd",
      "metadata": {
        "id": "bc87b5bd"
      },
      "outputs": [],
      "source": [
        "def test1_create_belief_mdp():\n",
        "    pomdp = RobotChargingPOMDP()\n",
        "    belief_mdp = create_belief_mdp(pomdp)\n",
        "    b4 = DiscreteDistribution({0 : .4, 1: .3, 2: .3})\n",
        "    a, Q = expectimax_search(b4, belief_mdp, 4, return_Q=True)\n",
        "\n",
        "    assert a == 'l1'\n",
        "    gt = {'l0': -0.1, 'l1': 0.255914, 'l2': -0.1, 'm12': -0.471128, 'm01': -0.5, 'm20': -0.031586, 'c': -56.0, 'nop': 0.0}\n",
        "    for k, v in gt.items():\n",
        "        assert k in Q and np.allclose(v, Q[k])\n",
        "\n",
        "test1_create_belief_mdp()\n",
        "\n",
        "print('Tests passed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13b9c199",
      "metadata": {
        "id": "13b9c199"
      },
      "source": [
        "## Receding Horizon Control\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f5dab55",
      "metadata": {
        "id": "2f5dab55"
      },
      "source": [
        "### Utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a76399a",
      "metadata": {
        "id": "4a76399a"
      },
      "source": [
        "A simple implementation of the Receding Horizon Control (RHC).\n",
        "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a0564a8",
      "metadata": {
        "id": "9a0564a8"
      },
      "outputs": [],
      "source": [
        "def receding_horizon_control(pomdp, h=4, search_algo=expectimax_search):\n",
        "    \"\"\"Receding Horizon Control (RHC).\n",
        "\n",
        "    Args:\n",
        "        pomdp (POMDP): The input POMDP problem.\n",
        "        h (int): The receding horizon.\n",
        "\n",
        "    Returns:\n",
        "        policy (Callable): A function policy(belief) -> action, mapping from a belief state to the action to take.\n",
        "    \"\"\"\n",
        "\n",
        "    belief_mdp = create_belief_mdp(pomdp)\n",
        "\n",
        "    def policy(belief):\n",
        "        \"\"\"The RHC policy. Basically, it runs a search algorithm (e.g., expectimax_search)\n",
        "            with a fixed horizon and output the optimal policy at the current belief.\n",
        "\n",
        "        Args:\n",
        "            belief (DiscreteDistribution): The current belief.\n",
        "\n",
        "        Returns:\n",
        "            action: The next action to take.\n",
        "        \"\"\"\n",
        "\n",
        "        return search_algo(belief, belief_mdp, horizon=h)\n",
        "\n",
        "    return policy\n",
        "\n",
        "\n",
        "def simulate(pomdp, initial_belief, policy, n=4, real_s=None):\n",
        "    \"\"\"Simulate a policy on a POMDP.\n",
        "\n",
        "    Specifically, the function commands the environment for n timesteps.\n",
        "    We will keep track of two variables.\n",
        "        - robot_b, which is the current belief.\n",
        "        - real_s: the \"true\" state of the world.\n",
        "\n",
        "    Args:\n",
        "        pomdp (POMDP): The input POMDP problem.\n",
        "        initial_belief (DiscreteDistribution): a distribution of the state (the initial belief).\n",
        "        n (int): The nunber of simulation steps.\n",
        "        real_s: The initial real_s, can be None, in which case it will be sampled from initial_belief.\n",
        "        policy_gen (Callable): a function that maps a belief to the next action.\n",
        "    \"\"\"\n",
        "\n",
        "    import numpy.random as npr; npr.seed(0)  # to determinize the execution.\n",
        "\n",
        "    # Create the Belief MDP.\n",
        "    bmdp = create_belief_mdp(pomdp)\n",
        "\n",
        "    robot_b = initial_belief\n",
        "    if real_s is None:\n",
        "        real_s = robot_b.draw()\n",
        "\n",
        "    print('Robot belief:', robot_b)\n",
        "    print('Real state:', real_s)\n",
        "    print('')\n",
        "\n",
        "    for t in range(n):\n",
        "        print('Step', t)\n",
        "        # search_algo returns the optimal action at the current belief.\n",
        "        a = policy(robot_b)\n",
        "\n",
        "        print('  Executing:', a)\n",
        "        real_s = pomdp.get_transition_distribution(real_s, a).draw()\n",
        "        print('  Real State:', real_s)\n",
        "        if pomdp.state_is_terminal(real_s):\n",
        "            print('Terminated.')\n",
        "            break\n",
        "        o = pomdp.get_observation_distribution(real_s, a).draw()\n",
        "        print('  Observation:', o)\n",
        "        robot_b = belief_filter(pomdp, robot_b, a, o)\n",
        "        print('  Robot belief:', robot_b)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a819a4ed",
      "metadata": {
        "id": "a819a4ed"
      },
      "source": [
        "### Question\n",
        "Use the RHC implementation provided in the colab and answer the questions in Catsoop.  **Note that the simulation function sets the random seed so that your results will be deterministic.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d55508fa",
      "metadata": {
        "id": "d55508fa"
      },
      "source": [
        "You can run the following code to visualize the execution of your RHC-Expectimax policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23773bb8",
      "metadata": {
        "id": "23773bb8"
      },
      "outputs": [],
      "source": [
        "pomdp = RobotChargingPOMDP(gamma=0.9)\n",
        "b0 = DiscreteDistribution({0: 0.03, 1: 0.07, 2: 0.9})\n",
        "policy = receding_horizon_control(pomdp, 4, search_algo=expectimax_search)\n",
        "simulate(pomdp, b0, policy=policy, n=10, real_s=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c18c3cd",
      "metadata": {
        "id": "3c18c3cd"
      },
      "source": [
        "## Receding Horizon Control with Most-Likely-Observation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddb301e7",
      "metadata": {
        "id": "ddb301e7"
      },
      "source": [
        "### Question\n",
        "Complete the RHC-MLO implementation provided in the colab and answer the questions in Catsoop.\n",
        "The only new requirement is to implement the `expectimax_search_mlo` that instead of computing the\n",
        "expected reward over all possible observations, only focuses on the most-likely observation.<br />\n",
        "**HINT**: You only need very MINIMAL changes to the original `expectimax_search` code.<br />\n",
        "**HINT**: You may find the `DiscreteDistribution.max()` function useful.\n",
        "\n",
        "After implementing `expectimax_search_mlo`, use the following code snippets (in colab) to simulate RHC-MLO, and answer questions in Catsoop.\n",
        "Specifically, we will use `gamma = 0.9` and initial belief $b = (0.4, 0.3, 0.3)$ (the same one we used in the Most Likely State problem).  Note that we are specifying the actual initial state to be 2.\n",
        "\n",
        "\n",
        "For reference, our solution is **21** line(s) of code.\n",
        "\n",
        "In addition to all the utilities defined at the top of the Colab notebook, the following functions are available in this question environment: `create_belief_mdp`. You may not need to use all of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f6180af",
      "metadata": {
        "id": "5f6180af"
      },
      "outputs": [],
      "source": [
        "def expectimax_search_mlo(initial_state, belief_mdp, horizon, return_Q=False):\n",
        "    \"\"\"Use expectimax search to determine a next action.\n",
        "\n",
        "    Note that we're just computing the single next action to\n",
        "    take, we do not need to store the entire partial V.\n",
        "\n",
        "    Horizon is given as a separate argument so that we can use\n",
        "    expectimax search with receding horizon control, for example,\n",
        "    even if belief_mdp.horizon is inf.\n",
        "\n",
        "    Args:\n",
        "        initial_state: A state in the belief_mdp.\n",
        "        belief_mdp: An MDP.\n",
        "        horizon: An int horizon.\n",
        "        return_Q: A boolean value. If true, also return the Q value\n",
        "            at the root instead of the action.\n",
        "\n",
        "    Returns:\n",
        "        action: An action in the belief_mdp.\n",
        "        Q: The Q value at the root state (only when return_Q is True).\n",
        "    \"\"\"\n",
        "    A = sorted(belief_mdp.action_space)\n",
        "    R = belief_mdp.get_reward\n",
        "    P = belief_mdp.get_transition_distribution\n",
        "    gm = belief_mdp.temporal_discount_factor\n",
        "    ts = belief_mdp.state_is_terminal\n",
        "\n",
        "    def V(s, h):\n",
        "        if h == horizon or ts(s):\n",
        "            return 0\n",
        "        return max(Q(s, a, h) for a in A)\n",
        "\n",
        "    def Q(s, a, h):\n",
        "        # TODO: Your code here.\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    Q_values = {a: Q(initial_state, a, 0) for a in A}\n",
        "    if return_Q:\n",
        "        return max(A, key=Q_values.get), Q_values\n",
        "    return max(A, key=Q_values.get)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d17d1611",
      "metadata": {
        "id": "d17d1611"
      },
      "source": [
        "### Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48579f99",
      "metadata": {
        "id": "48579f99"
      },
      "outputs": [],
      "source": [
        "def test1_mlo():\n",
        "    pomdp = RobotChargingPOMDP(gamma=0.9)\n",
        "    policy = receding_horizon_control(pomdp, 4, search_algo=expectimax_search_mlo)\n",
        "\n",
        "    b0 = DiscreteDistribution({0: 0.4, 1: 0.3, 2: 0.3})\n",
        "    assert policy(b0) == 'm20'\n",
        "    b1 = DiscreteDistribution({0: 0.64, 1: 0.3, 2: 0.05999999999999998, 'T': 0.0})\n",
        "    assert policy(b1) == 'l0'\n",
        "    b2 = DiscreteDistribution({0: 0.8, 1: 0.16666666666666666, 2: 0.03333333333333331})\n",
        "    assert policy(b2) == 'l1'\n",
        "\n",
        "test1_mlo()\n",
        "\n",
        "print('Tests passed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4336963",
      "metadata": {
        "id": "f4336963"
      },
      "source": [
        "You can run the following code to visualize the execution of your RHC-Expectimax-MLO policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5143efda",
      "metadata": {
        "id": "5143efda"
      },
      "outputs": [],
      "source": [
        "pomdp = RobotChargingPOMDP(gamma=0.9)\n",
        "b0 = DiscreteDistribution({0: 0.4, 1: 0.3, 2: 0.3})\n",
        "policy = receding_horizon_control(pomdp, 4, search_algo=expectimax_search_mlo)\n",
        "simulate(pomdp, b0, policy=policy, n=10, real_s=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2213d6d5",
      "metadata": {
        "id": "2213d6d5"
      },
      "source": [
        "## QMDP (Optional)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d1db797",
      "metadata": {
        "id": "7d1db797"
      },
      "source": [
        "### Question\n",
        "Complete the QMDP implementation provided in the colab and answer the questions in catsoop. Here you can use the value function\n",
        "computed by the `value_iteration` algorithm provided and focus on computing the policy.\n",
        "\n",
        "For reference, our solution is **21** line(s) of code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c3fcd20",
      "metadata": {
        "id": "3c3fcd20"
      },
      "outputs": [],
      "source": [
        "def qmdp(pomdp):\n",
        "    \"\"\"QMDP algorithm.\n",
        "    This function takes a POMDP as input and output a policy function that maps belief to the action to take.\n",
        "\n",
        "    Args:\n",
        "        pomdp (POMDP): An POMDP.\n",
        "\n",
        "    Returns:\n",
        "        policy (Callable): A function policy(belief) -> action, mapping from a belief state to the action to take.\n",
        "    \"\"\"\n",
        "    V, _ = value_iteration(pomdp)  # run value iteration on the underlying MDP.\n",
        "    Q = qsa_from_vs(pomdp, V)   # constructs Q values from the value function.\n",
        "\n",
        "    def policy(belief):\n",
        "        \"\"\"The QMDP policy.\n",
        "\n",
        "        Args:\n",
        "            belief (DiscreteDistribution): The belief about the current state.\n",
        "\n",
        "        Returns:\n",
        "            action: The argmax action computed based on QMDP.\n",
        "        \"\"\"\n",
        "        # TODO: Your code here.\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    return policy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0181ab1d",
      "metadata": {
        "id": "0181ab1d"
      },
      "source": [
        "### Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa92c16a",
      "metadata": {
        "id": "aa92c16a"
      },
      "outputs": [],
      "source": [
        "def test1_qmdp():\n",
        "    pomdp = RobotChargingPOMDP()\n",
        "    policy = qmdp(pomdp)\n",
        "    b0 = DiscreteDistribution({0: 0.999, 1: 0.001, 2: 0.0})\n",
        "    assert policy(b0) == 'c'\n",
        "    b0 = DiscreteDistribution({0: 0.0, 1: 0.6, 2: 0.4})\n",
        "    assert policy(b0) == 'm12'\n",
        "    b0 = DiscreteDistribution({0: 0.1, 1: 0.33, 2: 0.57})\n",
        "    assert policy(b0) == 'm20'\n",
        "\n",
        "test1_qmdp()\n",
        "\n",
        "print('Tests passed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd68867d",
      "metadata": {
        "id": "bd68867d"
      },
      "source": [
        "You can run the following code to visualize the execution of your QMDP policy (Hint: QMDP is not a good strategy for this case)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a49e53f",
      "metadata": {
        "id": "3a49e53f"
      },
      "outputs": [],
      "source": [
        "pomdp = RobotChargingPOMDP(gamma=0.9)\n",
        "b0 = DiscreteDistribution({0: 0.03, 1: 0.07, 2: 0.9})\n",
        "policy = qmdp(pomdp)\n",
        "simulate(pomdp, b0, policy=policy, n=10, real_s=2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "hw09.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
