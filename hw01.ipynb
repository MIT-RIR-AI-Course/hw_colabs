{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e53492a",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70edb1c1",
   "metadata": {},
   "source": [
    "## Imports and Utilities\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a5be83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import (Callable, Iterable, List, Sequence, Tuple, Dict, Optional,\n",
    "                    Any, Union)\n",
    "\n",
    "from abc import abstractmethod\n",
    "import collections\n",
    "import itertools\n",
    "import functools\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "########## Graph-Search-Related Utilities and Class Definitions ##########\n",
    "\n",
    "State = Any\n",
    "Action = Any\n",
    "\n",
    "StateSeq = List[State]\n",
    "ActionSeq = List[State]\n",
    "CostSeq = RewardSeq = List[float]\n",
    "\n",
    "\n",
    "class Problem(object):\n",
    "  \"\"\"The abstract base class for either a path cost problem or a reward problem.\"\"\"\n",
    "\n",
    "  def __init__(self, initial: State):\n",
    "    self.initial = initial\n",
    "\n",
    "  @abstractmethod\n",
    "  def actions(self, state: State) -> Iterable[Action]:\n",
    "    \"\"\"Returns the allowed actions in a given state. \n",
    "\n",
    "    The result would typically be a list. But if there are many actions, \n",
    "    consider yielding them one at a time in an iterator, \n",
    "    rather than building them all at once.\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "  @abstractmethod\n",
    "  def step(self, state: State, action: Action) -> State:\n",
    "    \"\"\"Returns the next state when executing a given action in a given state. \n",
    "\n",
    "    The action must be one of self.actions(state).\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "\n",
    "class PathCostProblem(Problem):\n",
    "  \"\"\"An abstract class for a path cost problem, based on AIMA.\n",
    "\n",
    "  To formalize a path cost problem, you should subclass from this and implement \n",
    "  the abstract methods. \n",
    "  Then you will create instances of your subclass and solve them with the \n",
    "  various search functions.\n",
    "  \"\"\"\n",
    "\n",
    "  @abstractmethod\n",
    "  def goal_test(self, state: State) -> bool:\n",
    "    \"\"\"Checks if the state is a goal.\"\"\"\n",
    "    ...\n",
    "\n",
    "  @abstractmethod\n",
    "  def step_cost(self, state1: State, action: Action, state2: State) -> float:\n",
    "    \"\"\"Returns the cost incurred at state2 from state1 via action.\"\"\"\n",
    "    ...\n",
    "\n",
    "  def h(self, state: State) -> float:\n",
    "    \"\"\"Returns the heuristic value, a lower bound on the distance to goal.\"\"\"\n",
    "    return 0\n",
    "\n",
    "\n",
    "class RewardProblem(Problem):\n",
    "  \"\"\"An abstract class for a finite-horizon reward problem, based on AIMA.\n",
    "\n",
    "  To formalize a reward problem, you should subclass from this and implement \n",
    "  the abstract methods. \n",
    "  Then you will create instances of your subclass and solve them with the \n",
    "  various search functions.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, initial: State, horizon: int):\n",
    "    self.initial = initial\n",
    "    self.horizon = horizon\n",
    "\n",
    "  @abstractmethod\n",
    "  def reward(self, state1: State, action: Action, state2: State) -> float:\n",
    "    \"\"\"Returns the reward given at state2 from state1 via action.\n",
    "\n",
    "    A reward at each step must be no greater than `self.rmax`.\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "  @property\n",
    "  @abstractmethod\n",
    "  def rmax(self) -> float:\n",
    "    \"\"\"Returns the maximum reward per step.\"\"\"\n",
    "    ...\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388ce01c",
   "metadata": {},
   "source": [
    "## Best-first Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a64af53",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cd1263",
   "metadata": {},
   "source": [
    "\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a361cca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GridProblem(PathCostProblem):\n",
    "  \"\"\"A grid problem.\"\"\"\n",
    "\n",
    "  def __init__(self, initial=(0, 0), goal=(4, 4)):\n",
    "    super().__init__(initial)\n",
    "    self.goal = goal\n",
    "    self.all_grid_actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "    self.grid_act_to_delta = {\n",
    "        \"up\": (-1, 0),\n",
    "        \"down\": (1, 0),\n",
    "        \"left\": (0, -1),\n",
    "        \"right\": (0, 1)\n",
    "    }\n",
    "    # Somewhat unusual cost structure, depends on s', which is determined by s,a\n",
    "    self.grid_arrival_costs = np.array(\n",
    "        [\n",
    "            [1, 1, 8, 1, 1],\n",
    "            [1, 8, 1, 1, 1],\n",
    "            [1, 8, 1, 1, 1],\n",
    "            [1, 1, 1, 8, 1],\n",
    "            [1, 1, 2, 1, 1],\n",
    "        ],\n",
    "        dtype=int,\n",
    "    )\n",
    "\n",
    "  def actions(self, state):\n",
    "    (r, c) = state\n",
    "    actions = []\n",
    "    for act in self.all_grid_actions:\n",
    "      dr, dc = self.grid_act_to_delta[act]\n",
    "      new_r, new_c = r + dr, c + dc\n",
    "      # Check if in bounds\n",
    "      if (0 <= new_r < self.grid_arrival_costs.shape[0] and\n",
    "          0 <= new_c < self.grid_arrival_costs.shape[1]):\n",
    "        actions.append(act)\n",
    "    return actions\n",
    "\n",
    "  def step(self, state, action):\n",
    "    (r, c) = state\n",
    "    dr, dc = self.grid_act_to_delta[action]\n",
    "    return (r + dr, c + dc)\n",
    "\n",
    "  def goal_test(self, state):\n",
    "    return state == self.goal\n",
    "\n",
    "  def step_cost(self, state1, action, state2):\n",
    "    return self.grid_arrival_costs[state2]\n",
    "\n",
    "  def h(self, state):\n",
    "    \"\"\"Manhattan distance.\"\"\"\n",
    "    return abs(state[0] - self.goal[0]) + abs(state[1] - self.goal[1])\n",
    "\n",
    "\n",
    "import contextlib\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def count_method_calls(problem: Problem, *meths: str):\n",
    "  \"\"\"Track number of method invocations to a problem.\n",
    "\n",
    "  Args:\n",
    "    problem: an instance of a Problem.\n",
    "    meths: a sequence of names for the methods to track call counts for.\n",
    "\n",
    "  Example:\n",
    "    >>> problem = GridProblem()\n",
    "    >>> with count_method_calls(problem, \"step\") as counters:\n",
    "    ...   problem.step((0, 0), \"down\")\n",
    "    ...   problem.step((1, 1), \"up\")\n",
    "    ...   assert counters[\"step\"][((0, 0), \"down\")] == counters[\"step\"][((1, 1), \"up\")]  == 1\n",
    "    ...   assert counters[\"step\"][\"total\"] == 2\n",
    "  \"\"\"\n",
    "  counters = {meth: collections.Counter() for meth in meths}\n",
    "\n",
    "  orig_problem_methods = {meth: getattr(problem, meth) for meth in meths}\n",
    "\n",
    "  def meth_helper(attr, *args):\n",
    "    counters[attr][args] += 1\n",
    "    counters[attr][\"total\"] += 1\n",
    "    return orig_problem_methods[attr](*args)\n",
    "\n",
    "  for meth in meths:\n",
    "    orig_problem_methods[meth] = getattr(problem, meth)\n",
    "    setattr(problem, meth, functools.partial(meth_helper, meth))\n",
    "\n",
    "  try:\n",
    "    yield counters\n",
    "  finally:\n",
    "    for meth in meths:\n",
    "      setattr(problem, meth, orig_problem_methods[meth])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34571af8",
   "metadata": {},
   "source": [
    "\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31003258",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import heapq as hq  # Can use this as a priority queue\n",
    "\n",
    "# A useful data structure for best-first search\n",
    "Node = collections.namedtuple(\"Node\",\n",
    "                              [\"state\", \"parent\", \"action\", \"cost\", \"g\"])\n",
    "\n",
    "\n",
    "class SearchFailed(ValueError):\n",
    "  \"\"\"Raise this exception whenever a search must fail.\"\"\"\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354cbd65",
   "metadata": {},
   "source": [
    "### Question\n",
    "Complete an implementation of the best-first search, encompassing A*, GBFS, or UCS. \n",
    "      You can assume any heuristics are consistent.\n",
    "      You should follow the psuedocode given in lecture closely. \n",
    "      In particular, your implementation should prune redundant paths by remembering the reached states.      \n",
    "      \n",
    "\n",
    "For reference, our solution is **54** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9ff91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_best_first_search(\n",
    "    problem: PathCostProblem,\n",
    "    get_priority: Callable[[Node], float],\n",
    "    step_budget: int = 1000) -> Tuple[StateSeq, ActionSeq, CostSeq, int]:\n",
    "  \"\"\"A generic heuristic search implementation.\n",
    "\n",
    "  Depending on `get_priority`, can implement A*, GBFS, or UCS.\n",
    "\n",
    "  The `get_priority` function here should determine the order\n",
    "  in which nodes are expanded. For example, if you want to\n",
    "  use path cost as part of this determination, then the\n",
    "  path cost (node.g) should appear inside of get_priority,\n",
    "  rather than in this implementation of `run_best_first_search`.\n",
    "\n",
    "  Important: for determinism (and to make sure our tests pass),\n",
    "  please break ties using the state itself. For example,\n",
    "  if you would've otherwise sorted by `get_priority(node)`, you\n",
    "  should now sort by `(get_priority(node), node.state)`.\n",
    "\n",
    "  Args:\n",
    "    problem: a path cost problem.\n",
    "    get_priority: a callable taking in a search Node and returns the priority\n",
    "    step_budget: maximum number of `problem.step` before giving up.\n",
    "\n",
    "  Returns:\n",
    "    state_sequence: A list of states.\n",
    "    action_sequence: A list of actions.\n",
    "    cost_sequence: A list of costs.\n",
    "    num_steps: number of taken `problem.step`s. Must be less than or equal to `step_budget`.\n",
    "\n",
    "  Raises:\n",
    "    error: SearchFailed, if no plan is found.\n",
    "  \"\"\"\n",
    "  raise SearchFailed(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73a617e",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db529403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will test this implementation more thoroughly with the\n",
    "# specific heuristic search algorithms that follow\n",
    "grid_problem = GridProblem()\n",
    "get_priority_fn = lambda node: 0\n",
    "result = run_best_first_search(grid_problem, get_priority_fn)\n",
    "assert len(result) == 4\n",
    "\n",
    "\n",
    "def best_first_search_test2():\n",
    "  # We will test this implementation more thoroughly with the\n",
    "  # specific heuristic search algorithms that follow\n",
    "  grid_problem = GridProblem()\n",
    "  get_priority_fn = lambda node: 0\n",
    "  with count_method_calls(grid_problem, \"step\", \"actions\") as counters:\n",
    "    state_sequence, action_sequence, cost_sequence, num_steps = run_best_first_search(\n",
    "        grid_problem, get_priority_fn)\n",
    "    assert (counters[\"step\"].pop(\"total\") == num_steps\n",
    "           ), \"Incorrect report of number of `problem.step`s\"\n",
    "\n",
    "  # Textbook implementation\n",
    "  try:\n",
    "    assert state_sequence == [(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (1, 4),\n",
    "                              (2, 4), (3, 4), (4, 4)]\n",
    "    assert action_sequence == [\n",
    "        'right', 'right', 'right', 'right', 'down', 'down', 'down', 'down'\n",
    "    ]\n",
    "    assert cost_sequence == [1.0, 8.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "  # Alternative implementation that tracks best-cost-to-nodes\n",
    "  except AssertionError:\n",
    "    assert state_sequence == [(0, 0), (1, 0), (2, 0), (3, 0), (3, 1), (3, 2),\n",
    "                              (4, 2), (4, 3), (4, 4)]\n",
    "    assert action_sequence == [\n",
    "        'down', 'down', 'down', 'right', 'right', 'down', 'right', 'right'\n",
    "    ]\n",
    "    assert cost_sequence == [1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0]\n",
    "  assert num_steps <= 252\n",
    "\n",
    "best_first_search_test2()\n",
    "\n",
    "\n",
    "def best_first_search_test3():\n",
    "  \"\"\"If your results do not match the expected ones, make sure that you are tiebreaking\n",
    "  as described in the docstring for `run_best_first_search`.\"\"\"\n",
    "  grid_problem = GridProblem()\n",
    "  get_priority_fn = lambda node: node.g\n",
    "  with count_method_calls(grid_problem, \"step\", \"actions\") as counters:\n",
    "    state_sequence, action_sequence, cost_sequence, num_steps = run_best_first_search(\n",
    "        grid_problem, get_priority_fn)\n",
    "    assert (counters[\"step\"].pop(\"total\") == num_steps\n",
    "           ), \"Incorrect report of number of `problem.step`s\"\n",
    "  assert state_sequence == [(0, 0), (1, 0), (2, 0), (3, 0), (3, 1), (3, 2),\n",
    "                            (4, 2), (4, 3), (4, 4)]\n",
    "  assert action_sequence == [\n",
    "      'down', 'down', 'down', 'right', 'right', 'down', 'right', 'right'\n",
    "  ]\n",
    "  assert cost_sequence == [1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0]\n",
    "  assert num_steps <= 70\n",
    "\n",
    "best_first_search_test3()\n",
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9656a5f1",
   "metadata": {},
   "source": [
    "## Uniform Cost Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aa0ac8",
   "metadata": {},
   "source": [
    "### Question\n",
    "Use your implementation of `run_best_first_search` to implement uniform cost search.\n",
    "\n",
    "For reference, our solution is **2** line(s) of code.\n",
    "\n",
    "In addition to all of the utilities defined at the top of the colab notebook, the following functions are available in this question environment: `run_best_first_search`. You may not need to use all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2576079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_uniform_cost_search(problem: PathCostProblem, step_budget: int = 1000):\n",
    "  \"\"\"Uniform-cost search.\n",
    "\n",
    "  Use your implementation of `run_best_first_search`.\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4189c03",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf2b43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ucs_test1():\n",
    "  # If your results do not match the expected ones, make sure that you are tiebreaking\n",
    "  # as described in the docstring for `run_best_first_search`.\n",
    "  grid_problem = GridProblem()\n",
    "  state_sequence, action_sequence, cost_sequence, num_steps = run_uniform_cost_search(\n",
    "      grid_problem)\n",
    "  assert state_sequence == [(0, 0), (1, 0), (2, 0), (3, 0), (3, 1), (3, 2),\n",
    "                            (4, 2), (4, 3), (4, 4)]\n",
    "  assert action_sequence == [\n",
    "      'down', 'down', 'down', 'right', 'right', 'down', 'right', 'right'\n",
    "  ]\n",
    "  assert cost_sequence == [1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0]\n",
    "  assert num_steps <= 70\n",
    "\n",
    "ucs_test1()\n",
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6918cdd",
   "metadata": {},
   "source": [
    "## A* Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03ed7e8",
   "metadata": {},
   "source": [
    "### Question\n",
    "Use your implementation of `run_best_first_search` to implement A* search.\n",
    "\n",
    "For reference, our solution is **2** line(s) of code.\n",
    "\n",
    "In addition to all of the utilities defined at the top of the colab notebook, the following functions are available in this question environment: `run_best_first_search`. You may not need to use all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a5e393",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_astar_search(problem: PathCostProblem, step_budget: int = 1000):\n",
    "  \"\"\"A* search.\n",
    "\n",
    "  Use your implementation of `run_best_first_search`.\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc026d54",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019b8e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def astar_test1():\n",
    "  \"\"\"If your results do not match the expected ones, make sure that you are tiebreaking \n",
    "  as described in the docstring for `run_best_first_search`.\"\"\"\n",
    "  grid_problem = GridProblem()\n",
    "  state_sequence, action_sequence, cost_sequence, num_steps = run_astar_search(\n",
    "      grid_problem)\n",
    "  assert state_sequence == [(0, 0), (1, 0), (2, 0), (3, 0), (3, 1), (3, 2),\n",
    "                            (4, 2), (4, 3), (4, 4)]\n",
    "  assert action_sequence == [\n",
    "      'down', 'down', 'down', 'right', 'right', 'down', 'right', 'right'\n",
    "  ]\n",
    "  assert cost_sequence == [1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0]\n",
    "  assert num_steps <= 36\n",
    "\n",
    "astar_test1()\n",
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c121a790",
   "metadata": {},
   "source": [
    "## Greedy Best-First Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a047230a",
   "metadata": {},
   "source": [
    "### Question\n",
    "Use your implementation of `run_best_first_search` to implement GBFS.\n",
    "\n",
    "For reference, our solution is **2** line(s) of code.\n",
    "\n",
    "In addition to all of the utilities defined at the top of the colab notebook, the following functions are available in this question environment: `run_best_first_search`. You may not need to use all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f3abfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_greedy_best_first_search(problem: PathCostProblem,\n",
    "                                 step_budget: int = 1000):\n",
    "  \"\"\"GBFS.\n",
    "\n",
    "  Use your implementation of `run_best_first_search`.\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656d928c",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a844f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbfs_test1():\n",
    "  \"\"\"If your results do not match the expected ones, make sure that you are tiebreaking\n",
    "  as described in the docstring for `run_best_first_search`.\"\"\"\n",
    "  problem = GridProblem()\n",
    "  state_sequence, action_sequence, cost_sequence, num_steps = run_greedy_best_first_search(\n",
    "      problem)\n",
    "  assert state_sequence == [(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (1, 4),\n",
    "                            (2, 4), (3, 4), (4, 4)]\n",
    "  assert action_sequence == [\n",
    "      'right', 'right', 'right', 'right', 'down', 'down', 'down', 'down'\n",
    "  ]\n",
    "  assert abs(num_steps - 22) <= 1\n",
    "\n",
    "gbfs_test1()\n",
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441a3141",
   "metadata": {},
   "source": [
    "## Visualize Reward Fields\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca15217",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef14737",
   "metadata": {},
   "source": [
    "The Fractal Problem and different reward fields. \n",
    "          You shoud read the code below to get rough idea what each reward field looks like.\n",
    "\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b7c7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FractalProblemState = collections.namedtuple(\"FractalProblemState\",\n",
    "                                             [\"x\", \"y\", \"t\"])\n",
    "\n",
    "\n",
    "class FractalProblem(RewardProblem):\n",
    "  \"\"\"A base class for \"fractal problem\".\n",
    "\n",
    "  Briefly, a fractal problem is as follows:\n",
    "    - The state space is the entire 2D Euclidean space\n",
    "    - The initial state is (0, 0)\n",
    "    - At each step $t$ the agent is allowed to move along one of the directions of size\n",
    "      $$step_scale^{-t}$$.\n",
    "    - At each step the agent receives a scalar reward based on a reward field.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               action_directions=\"8-neighbors\",\n",
    "               step_scale=0.5,\n",
    "               horizon=6):\n",
    "    \"\"\"A base constructor for a fractal problem. \n",
    "\n",
    "    Args:\n",
    "      action_directions: allowed directions to move at each step. \n",
    "      step_scale: each step length is scaled down by this amount.\n",
    "    \"\"\"\n",
    "    super().__init__(FractalProblemState(0, 0, 1), horizon)\n",
    "    if action_directions == \"8-neighbors\":\n",
    "      self.action_directions = [\n",
    "          (x, y)\n",
    "          for x, y in itertools.product([-1, 0, 1], [-1, 0, 1])\n",
    "          if (x, y) != (0, 0)\n",
    "      ]\n",
    "    elif action_directions == \"4-neighbors\":\n",
    "      self.action_directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    else:\n",
    "      self.action_directions = action_directions\n",
    "    self.step_scale = step_scale\n",
    "    self.field_of_view = self._compute_field_of_view()\n",
    "\n",
    "  def _compute_field_of_view(self,\n",
    "                             precision: float = 1.) -> Tuple[Tuple[float]]:\n",
    "    \"\"\"Computes a minimum rectangle that encloses all reachable states within horizon.\n",
    "\n",
    "    Args:\n",
    "      precision: the returned rectangle will have vertices that are integer \n",
    "        multiples of `precision`. Defaults to 1., which means that the rectangle\n",
    "        has integral vertices.\n",
    "\n",
    "    Returns:\n",
    "      the lower-left and upper-right vertices that represents the rectangle.\n",
    "    \"\"\"\n",
    "    extent = sum([self.step_scale**(t + 1) for t in range(self.horizon)])\n",
    "    dir_extent = np.array(self.action_directions) * extent\n",
    "    return (\n",
    "        tuple(np.floor(np.min(dir_extent, axis=0) / precision) * precision),\n",
    "        tuple(np.ceil(np.max(dir_extent, axis=0) / precision) * precision),\n",
    "    )\n",
    "\n",
    "  @property\n",
    "  def field_of_view_size(self):\n",
    "    return (self.field_of_view[1][0] - self.field_of_view[0][0],\n",
    "            self.field_of_view[1][1] - self.field_of_view[0][1])\n",
    "\n",
    "  def actions(self, state):\n",
    "    scale = self.step_scale**state.t\n",
    "    return [(x * scale, y * scale) for x, y in self.action_directions]\n",
    "\n",
    "  def step(self, state, action):\n",
    "    ax, ay = action\n",
    "    return FractalProblemState(state.x + ax, state.y + ay, state.t + 1)\n",
    "\n",
    "  def reward(self, state, action, next_state):\n",
    "    coord = np.array([next_state.x, next_state.y])\n",
    "    return self.reward_field(coord).item()\n",
    "\n",
    "  def reward_field(self, coords: np.ndarray):\n",
    "    \"\"\"The reward(s) at `coords`. Handle a batch of coordinates for efficiency of plotting.\n",
    "\n",
    "    Subclass must override to provide a meaningful reward field.\n",
    "\n",
    "    Args:\n",
    "      coords: an array with shape (2,) or (...batch, 2) denoting the coordinate(s)\n",
    "        to compute the reward field.\n",
    "\n",
    "    Returns:\n",
    "      A scalar or an array of shape (...batch,)\n",
    "    \"\"\"\n",
    "    return np.zeros(coords.shape[:-1])\n",
    "\n",
    "  @property\n",
    "  def rmax(self):\n",
    "    return 1.\n",
    "\n",
    "  def visualize_reward_field(self,\n",
    "                             fov: Tuple[Tuple[float]] = None,\n",
    "                             resolution=(1000, 1000),\n",
    "                             show=False):\n",
    "    \"\"\"Visualize a reward field.\n",
    "\n",
    "    Args:\n",
    "      fov: an optional field of view. If None, use self.field_of_view.\n",
    "      resolution: number of pixels (w, h) to discretize the field of view. \n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    if fov is None:\n",
    "      fov = self.field_of_view\n",
    "    coords = np.dstack(\n",
    "        np.meshgrid(\n",
    "            np.linspace(fov[0][0], fov[1][0], resolution[0]),\n",
    "            np.linspace(fov[0][1], fov[1][1], resolution[1]),\n",
    "        )).reshape(-1, 2)\n",
    "    vals = self.reward_field(coords).reshape(resolution)\n",
    "    plt.imshow(vals,\n",
    "               origin=\"lower\",\n",
    "               extent=(fov[0][0], fov[1][0], fov[0][1], fov[1][1]))\n",
    "    plt.colorbar()\n",
    "    if show:\n",
    "      plt.show()\n",
    "    return self\n",
    "\n",
    "  def visualize_plan(self,\n",
    "                     state_sequence: StateSeq,\n",
    "                     show=False,\n",
    "                     **arrow_kwargs):\n",
    "    \"\"\"Visualize a plan on the reward field.\n",
    "\n",
    "    Args:\n",
    "      state_sequence: A sequence state moving in the fractal problem.\n",
    "      arrow_kwargs: passed to `plt.arrow`.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    for s1, s2 in zip(state_sequence[:-1], state_sequence[1:]):\n",
    "      x1, y1, d = s1\n",
    "      x2, y2, _ = s2\n",
    "      dx, dy = x2 - x1, y2 - y1\n",
    "      plt.arrow(x1,\n",
    "                y1,\n",
    "                dx,\n",
    "                dy,\n",
    "                length_includes_head=True,\n",
    "                width=0.01 / d**0.5,\n",
    "                fill=True,\n",
    "                **arrow_kwargs)\n",
    "    if show:\n",
    "      plt.show()\n",
    "    return self\n",
    "\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "\n",
    "class GradientRewardFieldProblem(FractalProblem):\n",
    "  \"\"\"A fractal problem with a gradient reward field by adding multiple (scaled) \n",
    "  gaussian distributions.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               locs: Sequence[Tuple[int, int]] = ((0, 0),),\n",
    "               covs: Union[float, Sequence[Union[float, np.ndarray]]] = 0.1,\n",
    "               strengths: Union[float, Sequence[float]] = 1.,\n",
    "               **kwargs):\n",
    "    \"\"\"A reward field with a mixture of guassian gradients.\n",
    "\n",
    "    Args:\n",
    "      locs: the centers of the gaussians\n",
    "      covs: a scalar, a sequence of scalars, or a sequence of matrices \n",
    "        for the covariances of the gaussians.\n",
    "      strenghts: a scalar or a sequence of scalars for the scalaring factors \n",
    "        for each guassian. \n",
    "    \"\"\"\n",
    "    super().__init__(**kwargs)\n",
    "    self.locs = locs\n",
    "    if np.isscalar(covs):\n",
    "      covs = [covs] * len(self.locs)\n",
    "    covs = [np.eye(2) * cov if np.isscalar(cov) else cov for cov in covs]\n",
    "    self.covs = covs\n",
    "    if np.isscalar(strengths):\n",
    "      strengths = [strengths] * len(self.locs)\n",
    "    self.strengths = strengths\n",
    "\n",
    "  def reward_field(self, coords):\n",
    "    return sum([\n",
    "        multivariate_normal.pdf(coords, mean=loc, cov=cov) * strength\n",
    "        for loc, cov, strength in zip(self.locs, self.covs, self.strengths)\n",
    "    ])\n",
    "\n",
    "\n",
    "class NoisyRewardFieldProblem(FractalProblem):\n",
    "  \"\"\"A fractal problem with a reward field sampled from iid Beta distributions \n",
    "  in a discretized grid.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, seed=0, bin_size=5e-2, beta_params=(1., 2.), **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    # Compute number of bins required for the field of view\n",
    "    self.nbins = tuple(int(s / bin_size) for s in self.field_of_view_size)\n",
    "    # Initialize the random rewards within the field of view\n",
    "    rng_state = np.random.RandomState(seed)\n",
    "    self.random_locs = rng_state.beta(*beta_params, size=self.nbins)\n",
    "    self.random_locs = np.pad(self.random_locs, [(1, 1), (1, 1)],\n",
    "                              constant_values=0.)\n",
    "\n",
    "  def reward_field(self, coords):\n",
    "    binX, binY = tuple(\n",
    "        np.digitize(\n",
    "            coords[..., i],\n",
    "            np.linspace(self.field_of_view[0][i], self.field_of_view[1][i],\n",
    "                        self.nbins[i] + 1)) for i in range(2))\n",
    "    return self.random_locs[binX, binY]\n",
    "\n",
    "\n",
    "def get_fractal_problems() -> Dict[str, FractalProblem]:\n",
    "  \"\"\"Three fractal problems with reward fields.\n",
    "\n",
    "  DO NOT CHANGE THIS FUNCTION --- it exists to protect you from accidentally \n",
    "  changing the problems.\n",
    "  \"\"\"\n",
    "  return {\n",
    "      \"reward-field-1\":\n",
    "          GradientRewardFieldProblem(locs=[(-1, -1), (-1, 1), (1, -1), (1, 1)],\n",
    "                                     covs=[.3, .3, .3, .3],\n",
    "                                     strengths=[1.5, 1.5, 1.5, 1.5]),\n",
    "      \"reward-field-2\":\n",
    "          GradientRewardFieldProblem(locs=[(-1, -1), (-1, 1), (1, -1), (1, 1)],\n",
    "                                     covs=[.25, .25, .25, .2],\n",
    "                                     strengths=[0.7, 0.7, 0.7, 0.8]),\n",
    "      \"reward-field-3\":\n",
    "          NoisyRewardFieldProblem(seed=42, bin_size=5e-2,\n",
    "                                  beta_params=(0.2, 2.)),\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26abfef",
   "metadata": {},
   "source": [
    "### Question\n",
    "We have defined for you three fractal problems in `get_fractal_problems()`.\n",
    "          Below you can see the visualizations of the reward fields corresponding to these problems. \n",
    "          We have also provided you code in the Colab notebook to recreate these visualizations.\n",
    "          You are encouraged to inspect the relevant class definitions for the these problems to better understand them. \n",
    "          **You do not need to submit any code for this subsection.**\n",
    "          Instead, we will ask you questions in the following problems to confirm your understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f254c4",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75222cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_fractal_problems():\n",
    "  \"\"\"Visualize the fractal problems with different rewards fields.\"\"\"\n",
    "  import matplotlib.pyplot as plt\n",
    "  for name, problem in get_fractal_problems().items():\n",
    "    plt.title(name)\n",
    "    problem.visualize_reward_field(show=True)\n",
    "    plt.clf()\n",
    "\n",
    "visualize_fractal_problems()\n",
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db38b95d",
   "metadata": {},
   "source": [
    "## Path-cost Problem from Reward Problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0665d983",
   "metadata": {},
   "source": [
    "### Question\n",
    "Implement the reduction from a reward problem to a path-cost problem **as in lecture**.\n",
    "\n",
    "For reference, our solution is **30** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e247fe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def path_cost_problem_from_reward_problem(\n",
    "    reward_problem: RewardProblem) -> PathCostProblem:\n",
    "  \"\"\"Reduce a reward maximization problem into a path search problem.\n",
    "\n",
    "  You should take a close look that the class definition of `RewardProblem`, \n",
    "  since they will be handy. \n",
    "  Especially note that the horizon value is inclusive -- for a horizon value of $H$, \n",
    "  the agent should be allowed to step exactly $H$ number of steps.\n",
    "\n",
    "  Args:\n",
    "    problem: a RewardProblem.\n",
    "\n",
    "  Returns:\n",
    "    A PathCostProblem\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e38af86",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e946fee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_reward_reduction_test(path_cost_problem_from_reward_problem,\n",
    "                               loc,\n",
    "                               horizon: int = 4):\n",
    "  problem = GradientRewardFieldProblem(locs=[loc],\n",
    "                                       covs=0.3,\n",
    "                                       strengths=[1.],\n",
    "                                       horizon=horizon)\n",
    "  path_problem = path_cost_problem_from_reward_problem(problem)\n",
    "  state_sequence, action_sequence, cost_sequence, num_steps = run_uniform_cost_search(\n",
    "      path_problem, step_budget=100000)\n",
    "  assert ((len(state_sequence) - 1) == len(action_sequence) ==\n",
    "          len(cost_sequence) == horizon)\n",
    "  assert [s[1] for s in state_sequence] == list(range(horizon, -1, -1))\n",
    "  assert np.all(np.sign(np.array(action_sequence)) == np.sign(\n",
    "      loc)), f\"Actions {action_sequence} does not move towards {loc}\"\n",
    "  cost_sequence = np.array(cost_sequence)\n",
    "  assert np.all(cost_sequence <= problem.rmax), \"Costs should not exceed Rmax\"\n",
    "  assert np.all(cost_sequence >= 0), \"Costs should be positive\"\n",
    "  assert np.all(np.diff(cost_sequence) <= 0), \"Costs should be decreasing\"\n",
    "\n",
    "path_reward_reduction_test(path_cost_problem_from_reward_problem, (1, 1), 2)\n",
    "\n",
    "def path_reward_reduction_test(path_cost_problem_from_reward_problem,\n",
    "                               loc,\n",
    "                               horizon: int = 4):\n",
    "  problem = GradientRewardFieldProblem(locs=[loc],\n",
    "                                       covs=0.3,\n",
    "                                       strengths=[1.],\n",
    "                                       horizon=horizon)\n",
    "  path_problem = path_cost_problem_from_reward_problem(problem)\n",
    "  state_sequence, action_sequence, cost_sequence, num_steps = run_uniform_cost_search(\n",
    "      path_problem, step_budget=100000)\n",
    "  assert ((len(state_sequence) - 1) == len(action_sequence) ==\n",
    "          len(cost_sequence) == horizon)\n",
    "  assert [s[1] for s in state_sequence] == list(range(horizon, -1, -1))\n",
    "  assert np.all(np.sign(np.array(action_sequence)) == np.sign(\n",
    "      loc)), f\"Actions {action_sequence} does not move towards {loc}\"\n",
    "  cost_sequence = np.array(cost_sequence)\n",
    "  assert np.all(cost_sequence <= problem.rmax), \"Costs should not exceed Rmax\"\n",
    "  assert np.all(cost_sequence >= 0), \"Costs should be positive\"\n",
    "  assert np.all(np.diff(cost_sequence) <= 0), \"Costs should be decreasing\"\n",
    "\n",
    "path_reward_reduction_test(path_cost_problem_from_reward_problem, (1, 1), 3)\n",
    "\n",
    "def path_reward_reduction_test(path_cost_problem_from_reward_problem,\n",
    "                               loc,\n",
    "                               horizon: int = 4):\n",
    "  problem = GradientRewardFieldProblem(locs=[loc],\n",
    "                                       covs=0.3,\n",
    "                                       strengths=[1.],\n",
    "                                       horizon=horizon)\n",
    "  path_problem = path_cost_problem_from_reward_problem(problem)\n",
    "  state_sequence, action_sequence, cost_sequence, num_steps = run_uniform_cost_search(\n",
    "      path_problem, step_budget=100000)\n",
    "  assert ((len(state_sequence) - 1) == len(action_sequence) ==\n",
    "          len(cost_sequence) == horizon)\n",
    "  assert [s[1] for s in state_sequence] == list(range(horizon, -1, -1))\n",
    "  assert np.all(np.sign(np.array(action_sequence)) == np.sign(\n",
    "      loc)), f\"Actions {action_sequence} does not move towards {loc}\"\n",
    "  cost_sequence = np.array(cost_sequence)\n",
    "  assert np.all(cost_sequence <= problem.rmax), \"Costs should not exceed Rmax\"\n",
    "  assert np.all(cost_sequence >= 0), \"Costs should be positive\"\n",
    "  assert np.all(np.diff(cost_sequence) <= 0), \"Costs should be decreasing\"\n",
    "\n",
    "path_reward_reduction_test(path_cost_problem_from_reward_problem, (1, 1), 5)\n",
    "\n",
    "def path_reward_reduction_test(path_cost_problem_from_reward_problem,\n",
    "                               loc,\n",
    "                               horizon: int = 4):\n",
    "  problem = GradientRewardFieldProblem(locs=[loc],\n",
    "                                       covs=0.3,\n",
    "                                       strengths=[1.],\n",
    "                                       horizon=horizon)\n",
    "  path_problem = path_cost_problem_from_reward_problem(problem)\n",
    "  state_sequence, action_sequence, cost_sequence, num_steps = run_uniform_cost_search(\n",
    "      path_problem, step_budget=100000)\n",
    "  assert ((len(state_sequence) - 1) == len(action_sequence) ==\n",
    "          len(cost_sequence) == horizon)\n",
    "  assert [s[1] for s in state_sequence] == list(range(horizon, -1, -1))\n",
    "  assert np.all(np.sign(np.array(action_sequence)) == np.sign(\n",
    "      loc)), f\"Actions {action_sequence} does not move towards {loc}\"\n",
    "  cost_sequence = np.array(cost_sequence)\n",
    "  assert np.all(cost_sequence <= problem.rmax), \"Costs should not exceed Rmax\"\n",
    "  assert np.all(cost_sequence >= 0), \"Costs should be positive\"\n",
    "  assert np.all(np.diff(cost_sequence) <= 0), \"Costs should be decreasing\"\n",
    "\n",
    "path_reward_reduction_test(path_cost_problem_from_reward_problem, (-1, -1), 2)\n",
    "\n",
    "def path_reward_reduction_test(path_cost_problem_from_reward_problem,\n",
    "                               loc,\n",
    "                               horizon: int = 4):\n",
    "  problem = GradientRewardFieldProblem(locs=[loc],\n",
    "                                       covs=0.3,\n",
    "                                       strengths=[1.],\n",
    "                                       horizon=horizon)\n",
    "  path_problem = path_cost_problem_from_reward_problem(problem)\n",
    "  state_sequence, action_sequence, cost_sequence, num_steps = run_uniform_cost_search(\n",
    "      path_problem, step_budget=100000)\n",
    "  assert ((len(state_sequence) - 1) == len(action_sequence) ==\n",
    "          len(cost_sequence) == horizon)\n",
    "  assert [s[1] for s in state_sequence] == list(range(horizon, -1, -1))\n",
    "  assert np.all(np.sign(np.array(action_sequence)) == np.sign(\n",
    "      loc)), f\"Actions {action_sequence} does not move towards {loc}\"\n",
    "  cost_sequence = np.array(cost_sequence)\n",
    "  assert np.all(cost_sequence <= problem.rmax), \"Costs should not exceed Rmax\"\n",
    "  assert np.all(cost_sequence >= 0), \"Costs should be positive\"\n",
    "  assert np.all(np.diff(cost_sequence) <= 0), \"Costs should be decreasing\"\n",
    "\n",
    "path_reward_reduction_test(path_cost_problem_from_reward_problem, (-1, -1), 3)\n",
    "\n",
    "def path_reward_reduction_test(path_cost_problem_from_reward_problem,\n",
    "                               loc,\n",
    "                               horizon: int = 4):\n",
    "  problem = GradientRewardFieldProblem(locs=[loc],\n",
    "                                       covs=0.3,\n",
    "                                       strengths=[1.],\n",
    "                                       horizon=horizon)\n",
    "  path_problem = path_cost_problem_from_reward_problem(problem)\n",
    "  state_sequence, action_sequence, cost_sequence, num_steps = run_uniform_cost_search(\n",
    "      path_problem, step_budget=100000)\n",
    "  assert ((len(state_sequence) - 1) == len(action_sequence) ==\n",
    "          len(cost_sequence) == horizon)\n",
    "  assert [s[1] for s in state_sequence] == list(range(horizon, -1, -1))\n",
    "  assert np.all(np.sign(np.array(action_sequence)) == np.sign(\n",
    "      loc)), f\"Actions {action_sequence} does not move towards {loc}\"\n",
    "  cost_sequence = np.array(cost_sequence)\n",
    "  assert np.all(cost_sequence <= problem.rmax), \"Costs should not exceed Rmax\"\n",
    "  assert np.all(cost_sequence >= 0), \"Costs should be positive\"\n",
    "  assert np.all(np.diff(cost_sequence) <= 0), \"Costs should be decreasing\"\n",
    "\n",
    "path_reward_reduction_test(path_cost_problem_from_reward_problem, (-1, -1), 5)\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf737329",
   "metadata": {},
   "source": [
    "## MCTS vs. UCS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99b4c0b",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4365df",
   "metadata": {},
   "source": [
    "Our implementation of MCTS.\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c898cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dataclasses\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=False, eq=False)\n",
    "class MCTNode:\n",
    "  \"\"\"Node in the Monte Carlo search tree, keeps track of the children states.\"\"\"\n",
    "  state: State\n",
    "  U: float\n",
    "  N: int\n",
    "  horizon: int\n",
    "  parent: Optional['MCTNode']\n",
    "  children: Dict['MCTNode', Action] = dataclasses.field(default_factory=dict)\n",
    "\n",
    "\n",
    "def ucb(n: MCTNode, C: float = 1.4) -> float:\n",
    "  \"\"\"UCB for a node, note the C argument\"\"\"\n",
    "  return np.inf if n.N == 0 else (n.U / n.N +\n",
    "                                  C * np.sqrt(np.log(n.parent.N) / n.N))\n",
    "\n",
    "\n",
    "def run_mcts_search(problem: RewardProblem,\n",
    "                    C: float = 1.4,\n",
    "                    iteration_budget: int = 1000,\n",
    "                    step_budget: int = np.inf):\n",
    "  \"\"\"A generic MCTS search implementation.\n",
    "\n",
    "  Args:\n",
    "    problem: a reward problem.\n",
    "    C: the UCB parameter.\n",
    "    iteration_budget: maximum iterations to run the search.\n",
    "    step_budget: maximum number of allowed `problem.step`s.\n",
    "\n",
    "  Returns:\n",
    "    state_sequence: A list of states.\n",
    "    action_sequence: A list of actions.\n",
    "    reward_sequence: A list of rewards\n",
    "  \"\"\"\n",
    "  if min(iteration_budget, step_budget) == np.inf:\n",
    "    raise ValueError(\"Must provide at least one budget\")\n",
    "\n",
    "  problem_step_count = 0\n",
    "\n",
    "  class BudgetExceeded(Exception):\n",
    "    pass\n",
    "\n",
    "  def step_helper(state, action):\n",
    "    \"\"\"helper to track the problem's step count.\"\"\"\n",
    "    nonlocal problem_step_count\n",
    "    problem_step_count += 1\n",
    "    if problem_step_count > step_budget:\n",
    "      raise BudgetExceeded(\"step budget exceeded\")\n",
    "    return problem.step(state, action)\n",
    "\n",
    "  ucb_fixed_C = functools.partial(ucb, C=C)\n",
    "\n",
    "  def select(n: MCTNode) -> MCTNode:\n",
    "    \"\"\"select a leaf node in the tree\"\"\"\n",
    "    if n.children:\n",
    "      ucb_pick = max(n.children.keys(), key=ucb_fixed_C)\n",
    "      return select(ucb_pick)\n",
    "    return n\n",
    "\n",
    "  def expand(n: MCTNode) -> MCTNode:\n",
    "    \"\"\"expand the leaf node by adding all its children states\"\"\"\n",
    "    assert not n.children\n",
    "    if n.horizon == 0:\n",
    "      return n\n",
    "    for action in problem.actions(n.state):\n",
    "      child_state = step_helper(n.state, action)\n",
    "      new_node = MCTNode(state=child_state,\n",
    "                         horizon=n.horizon - 1,\n",
    "                         parent=n,\n",
    "                         U=0,\n",
    "                         N=0)\n",
    "      n.children[new_node] = action\n",
    "    child = random.choice(list(n.children.keys()))\n",
    "    return child\n",
    "\n",
    "  def simulate(node: MCTNode) -> float:\n",
    "    \"\"\"simulate the utility of current state by randomly picking a step\"\"\"\n",
    "    state = node.state\n",
    "    total_reward = 0\n",
    "    for h in range(node.horizon, 0, -1):\n",
    "      action = random.choice(problem.actions(state))\n",
    "      child_state = step_helper(state, action)\n",
    "      reward = problem.reward(state, action, child_state)\n",
    "      total_reward += reward\n",
    "      state = child_state\n",
    "    return total_reward\n",
    "\n",
    "  def backup(n: MCTNode, value: float) -> None:\n",
    "    \"\"\"passing the utility back to all parent nodes\"\"\"\n",
    "    if n.parent:\n",
    "      # Need to include the reward on the action *into* n\n",
    "      a = n.parent.children[n]\n",
    "      r = problem.reward(n.parent.state, a, n.state)\n",
    "      n.U += value + r\n",
    "      n.N += 1\n",
    "      backup(n.parent, value + r)\n",
    "    else:\n",
    "      n.N += 1\n",
    "\n",
    "  root = MCTNode(state=problem.initial,\n",
    "                 horizon=problem.horizon,\n",
    "                 parent=None,\n",
    "                 U=0,\n",
    "                 N=0)\n",
    "\n",
    "  try:\n",
    "    i = 0\n",
    "    while i < iteration_budget:\n",
    "      leaf = select(root)\n",
    "      child = expand(leaf)\n",
    "      value = simulate(child)\n",
    "      backup(child, value)\n",
    "      i += 1\n",
    "  except BudgetExceeded:\n",
    "    pass\n",
    "\n",
    "  return finish_mcts_plan(problem, root)\n",
    "\n",
    "\n",
    "def finish_mcts_plan(problem: RewardProblem, node: MCTNode):\n",
    "  \"\"\"Helper for run_mcts_search. Recover the plan. \"\"\"\n",
    "  state_sequence = [node.state]\n",
    "  action_sequence = []\n",
    "  reward_sequence = []\n",
    "\n",
    "  while node.children:\n",
    "    max_node = max(node.children,\n",
    "                   key=lambda p: p.U / p.N if p.N > 0 else -np.inf)\n",
    "    max_action = node.children.get(max_node)\n",
    "    action_sequence.append(max_action)\n",
    "    state_sequence.append(max_node.state)\n",
    "    reward_sequence.append(\n",
    "        problem.reward(node.state, max_action, max_node.state))\n",
    "    node = max_node\n",
    "\n",
    "  return state_sequence, action_sequence, reward_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a0ab9",
   "metadata": {},
   "source": [
    "### Question\n",
    "We have provided you an implementation of MCTS for reward problems in the Colab notebook.\n",
    "          You should make sure that you can run our implementation on the reward problems we have defined for you.\n",
    "          You are also encouraged to look into our code to see under how our MCTS is implemented.\n",
    "          **You do not need to submit any code for this question.**\n",
    "          Instead, we will ask you some questions to confirm your understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771a18d1",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b1391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mcts():\n",
    "  \"\"\"Solve one of the fractal problems and visualize the plan.\n",
    "\n",
    "  This function may take a few moments to run. \n",
    "  \"\"\"\n",
    "  # You may change this to visualize other reward fields.\n",
    "  problem = get_fractal_problems()[\"reward-field-1\"]\n",
    "\n",
    "  plan = run_mcts_search(problem, iteration_budget=10000)\n",
    "  problem.visualize_reward_field().visualize_plan(plan[0], show=True)\n",
    "\n",
    "visualize_mcts()\n",
    "\n",
    "print('Tests passed.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw01.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
