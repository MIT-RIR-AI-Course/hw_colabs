{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14110351",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4534613f",
   "metadata": {},
   "source": [
    "## Imports and Utilities\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33a22c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import (Callable, Iterable, List, Sequence, Tuple, Dict, Optional,\n",
    "                    Any, Union)\n",
    "from abc import abstractmethod\n",
    "\n",
    "from collections import namedtuple\n",
    "import signal\n",
    "import itertools\n",
    "import functools\n",
    "import random\n",
    "import contextlib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "########## Graph-Search-Related Utilities and Class Definitions ##########\n",
    "\n",
    "State = Any\n",
    "Action = Any\n",
    "\n",
    "StateSeq = List[State]\n",
    "ActionSeq = List[State]\n",
    "CostSeq = RewardSeq = List[float]\n",
    "\n",
    "\n",
    "class Problem(object):\n",
    "  \"\"\"The abstract base class for either a path cost problem or a reward problem.\"\"\"\n",
    "\n",
    "  def __init__(self, initial: State):\n",
    "    self.initial = initial\n",
    "\n",
    "  @abstractmethod\n",
    "  def actions(self, state: State) -> Iterable[Action]:\n",
    "    \"\"\"Returns the allowed actions in a given state. \n",
    "\n",
    "    The result would typically be a list. But if there are many actions, \n",
    "    consider yielding them one at a time in an iterator, \n",
    "    rather than building them all at once.\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "  @abstractmethod\n",
    "  def step(self, state: State, action: Action) -> State:\n",
    "    \"\"\"Returns the next state when executing a given action in a given state. \n",
    "\n",
    "    The action must be one of self.actions(state).\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "\n",
    "class PathCostProblem(Problem):\n",
    "  \"\"\"An abstract class for a path cost problem, based on AIMA.\n",
    "\n",
    "  To formalize a path cost problem, you should subclass from this and implement \n",
    "  the abstract methods. \n",
    "  Then you will create instances of your subclass and solve them with the \n",
    "  various search functions.\n",
    "  \"\"\"\n",
    "\n",
    "  @abstractmethod\n",
    "  def goal_test(self, state: State) -> bool:\n",
    "    \"\"\"Checks if the state is a goal.\"\"\"\n",
    "    ...\n",
    "\n",
    "  @abstractmethod\n",
    "  def step_cost(self, state1: State, action: Action, state2: State) -> float:\n",
    "    \"\"\"Returns the cost incurred at state2 from state1 via action.\"\"\"\n",
    "    ...\n",
    "\n",
    "  def h(self, state: State) -> float:\n",
    "    \"\"\"Returns the heuristic value, a lower bound on the distance to goal.\"\"\"\n",
    "    return -np.inf\n",
    "\n",
    "\n",
    "class RewardProblem(Problem):\n",
    "  \"\"\"An abstract class for a finite-horizon reward problem, based on AIMA.\n",
    "\n",
    "  To formalize a reward problem, you should subclass from this and implement \n",
    "  the abstract methods. \n",
    "  Then you will create instances of your subclass and solve them with the \n",
    "  various search functions.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, initial: State, horizon: int):\n",
    "    self.initial = initial\n",
    "    self.horizon = horizon\n",
    "\n",
    "  @abstractmethod\n",
    "  def reward(self, state1: State, action: Action, state2: State) -> float:\n",
    "    \"\"\"Returns the reward given at state2 from state1 via action.\n",
    "\n",
    "    A reward at each step must be no greater than `self.rmax`.\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "  @property\n",
    "  @abstractmethod\n",
    "  def rmax(self) -> float:\n",
    "    \"\"\"Returns the maximum reward per step.\"\"\"\n",
    "    ...\n",
    "\n",
    "\n",
    "class GridProblem(PathCostProblem):\n",
    "  \"\"\"A grid problem.\"\"\"\n",
    "\n",
    "  def __init__(self, initial, goal=(4, 4)):\n",
    "    super().__init__(initial)\n",
    "    self.goal = goal\n",
    "    self.all_grid_actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "    self.grid_act_to_delta = {\n",
    "        \"up\": (-1, 0),\n",
    "        \"down\": (1, 0),\n",
    "        \"left\": (0, -1),\n",
    "        \"right\": (0, 1)\n",
    "    }\n",
    "    # Somewhat unusual cost structure, depends on s', which is determined by s,a\n",
    "    self.grid_arrival_costs = np.array(\n",
    "        [\n",
    "            [1, 1, 8, 1, 1],\n",
    "            [1, 8, 1, 1, 1],\n",
    "            [1, 8, 1, 1, 1],\n",
    "            [1, 1, 1, 8, 1],\n",
    "            [1, 1, 2, 1, 1],\n",
    "        ],\n",
    "        dtype=int,\n",
    "    )\n",
    "\n",
    "  def actions(self, state):\n",
    "    (r, c) = state\n",
    "    actions = []\n",
    "    for act in self.all_grid_actions:\n",
    "      dr, dc = self.grid_act_to_delta[act]\n",
    "      new_r, new_c = r + dr, c + dc\n",
    "      # Check if in bounds\n",
    "      if (0 <= new_r < self.grid_arrival_costs.shape[0] and\n",
    "          0 <= new_c < self.grid_arrival_costs.shape[1]):\n",
    "        actions.append(act)\n",
    "    return actions\n",
    "\n",
    "  def step(self, state, action):\n",
    "    (r, c) = state\n",
    "    dr, dc = self.grid_act_to_delta[action]\n",
    "    return (r + dr, c + dc)\n",
    "\n",
    "  def goal_test(self, state):\n",
    "    return state == self.goal\n",
    "\n",
    "  def step_cost(self, state1, action, state2):\n",
    "    return self.grid_arrival_costs[state2]\n",
    "\n",
    "  def h(self, state):\n",
    "    \"\"\"Manhattan distance.\"\"\"\n",
    "    return abs(state[0] - self.goal[0]) + abs(state[1] - self.goal[1])\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def count_step_calls(problem: Problem):\n",
    "  \"\"\"Enforce that `problem.step`.\n",
    "\n",
    "  Example:\n",
    "    >>> problem = GridProblem()\n",
    "    >>> with count_step_calls(problem) as counter:\n",
    "    ...   problem.step((0, 0), \"down\")\n",
    "    ...   problem.step((1, 1), \"up\")\n",
    "    ...   assert counter[((0, 0), \"down\")] == counter[((1, 1), \"up\")]  == 1\n",
    "    ...   assert counter[\"total\"] == 2\n",
    "  \"\"\"\n",
    "  counter = collections.Counter()\n",
    "\n",
    "  def step_helper(state, action):\n",
    "    counter[(state, action)] += 1\n",
    "    counter[\"total\"] += 1\n",
    "\n",
    "  orig_problem_step = problem.step\n",
    "  problem.step = step_helper\n",
    "  try:\n",
    "    yield problem\n",
    "  finally:\n",
    "    problem.step = orig_problem_step\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe558f34",
   "metadata": {},
   "source": [
    "## Best-first Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f6d022",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ad78f0",
   "metadata": {},
   "source": [
    "\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3658540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A useful data structure for heuristic search\n",
    "Node = namedtuple(\"Node\", [\"state\", \"parent\", \"action\", \"cost\", \"g\"])\n",
    "\n",
    "\n",
    "class SearchFailed(ValueError):\n",
    "  \"\"\"Raise this exception whenever a search must fail.\"\"\"\n",
    "  pass\n",
    "\n",
    "\n",
    "import heapq as hq\n",
    "\n",
    "\n",
    "def get_trace_from_actions_helper(\n",
    "    problem: PathCostProblem,\n",
    "    actions: List[Action]) -> Tuple[List[State], List[float]]:\n",
    "  \"\"\"Get the planned states and costs given a list of planned actions.\n",
    "\n",
    "\n",
    "  This is a helper function for debugging and running our tests.\n",
    "  It invokes `problem.step` to reconstruct a sequence of states from actions.\n",
    "  Do not invoke this function in a search algorithm! You should only\n",
    "  call `problem.step` at most once for each state-action pair.\n",
    "\n",
    "  Args:\n",
    "    problem: a path cost problem\n",
    "    actions: a list of actions.\n",
    "\n",
    "  Returns:\n",
    "    a list of states and a list of rewards/costs.\n",
    "  \"\"\"\n",
    "\n",
    "  states = [problem.initial]\n",
    "  costs = []\n",
    "  for action in actions:\n",
    "    states.append(problem.step(states[-1], action))\n",
    "    costs.append(problem.step_cost(states[-2], action, states[-1]))\n",
    "  return states, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59bf2e1",
   "metadata": {},
   "source": [
    "### Question\n",
    "Complete an implementation of the best-first search, encompassing A*, GBFS, or UCS. You can assume any heuristics are consistent.\n",
    "\n",
    "For reference, our solution is **56** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d3abc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_best_first_search(\n",
    "    problem: PathCostProblem,\n",
    "    get_priority: Callable[[Node], float],\n",
    "    max_steps: int = 1000) -> Tuple[StateSeq, ActionSeq, CostSeq]:\n",
    "  \"\"\"A generic heuristic search implementation.\n",
    "\n",
    "  Depending on `get_priority`, can implement A*, GBFS, or UCS.\n",
    "\n",
    "  The `get_priority` function here should determine the order\n",
    "  in which nodes are expanded. For example, if you want to\n",
    "  use path cost as part of this determination, then the\n",
    "  path cost (node.g) should appear inside of get_priority,\n",
    "  rather than in this implementation of `run_best_first_search`.\n",
    "\n",
    "  Important: for determinism (and to make sure our tests pass),\n",
    "  please break ties using the state itself. For example,\n",
    "  if you would've otherwise sorted by `get_priority(node)`, you\n",
    "  should now sort by `(get_priority(node), node.state)`.\n",
    "\n",
    "  Args:\n",
    "    problem: a path cost problem.\n",
    "    get_priority: a callable taking in a search Node and returns the priority\n",
    "    max_steps: maximum number of `problem.step` before giving up.\n",
    "\n",
    "  Returns:\n",
    "    state_sequence: A list of states.\n",
    "    action_sequence: A list of actions.\n",
    "    cost_sequence: A list of costs.\n",
    "    num_steps: number of taken `problem.step`s. Must be less than or equal to `max_steps`.\n",
    "\n",
    "  Raises:\n",
    "    error: SearchFailed, if no plan is found.\n",
    "  \"\"\"\n",
    "  raise SearchFailed(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd2d183",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced57618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will test this implementation more thoroughly with the\n",
    "# specific heuristic search algorithms that follow\n",
    "grid_problem = GridProblem((0, 0))\n",
    "get_priority_fn = lambda node: 0\n",
    "result = run_best_first_search(grid_problem, get_priority_fn)\n",
    "assert len(result) == 3\n",
    "\n",
    "def best_first_search_test2():\n",
    "  # We will test this implementation more thoroughly with the\n",
    "  # specific heuristic search algorithms that follow\n",
    "  grid_problem = GridProblem((0, 0))\n",
    "  get_priority_fn = lambda node: 0\n",
    "  state_sequence, action_sequence, cost_sequence, num_expansions = run_best_first_search(\n",
    "      grid_problem, get_priority_fn)\n",
    "  # Textbook implementation\n",
    "  try:\n",
    "    assert state_sequence == [(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (1, 4),\n",
    "                              (2, 4), (3, 4), (4, 4)]\n",
    "    assert action_sequence == [\n",
    "        'right', 'right', 'right', 'right', 'down', 'down', 'down', 'down'\n",
    "    ]\n",
    "    assert cost_sequence == [1.0, 8.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "  # Alternative implementation that tracks best-cost-to-nodes\n",
    "  except AssertionError:\n",
    "    assert state_sequence == [(0, 0), (1, 0), (2, 0), (3, 0), (3, 1), (3, 2),\n",
    "                              (4, 2), (4, 3), (4, 4)]\n",
    "    assert action_sequence == [\n",
    "        'down', 'down', 'down', 'right', 'right', 'down', 'right', 'right'\n",
    "    ]\n",
    "    assert cost_sequence == [1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0]\n",
    "  assert num_expansions <= 35\n",
    "\n",
    "best_first_search_test2()\n",
    "\n",
    "def best_first_search_test3():\n",
    "  \"\"\"If your results do not match the expected ones, make sure that you are tiebreaking\n",
    "  as described in the docstring for `run_best_first_search`.\"\"\"\n",
    "  grid_problem = GridProblem((0, 0))\n",
    "  get_priority_fn = lambda node: node.g\n",
    "  state_sequence, action_sequence, cost_sequence, num_expansions = run_best_first_search(\n",
    "      grid_problem, get_priority_fn)\n",
    "  assert state_sequence == [(0, 0), (1, 0), (2, 0), (3, 0), (3, 1), (3, 2),\n",
    "                            (4, 2), (4, 3), (4, 4)]\n",
    "  assert action_sequence == [\n",
    "      'down', 'down', 'down', 'right', 'right', 'down', 'right', 'right'\n",
    "  ]\n",
    "  assert cost_sequence == [1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0]\n",
    "  assert num_expansions <= 35\n",
    "\n",
    "best_first_search_test3()\n",
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d434bdd",
   "metadata": {},
   "source": [
    "## Uniform Cost Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5a0f0e",
   "metadata": {},
   "source": [
    "### Question\n",
    "Use your implementation of `run_best_first_search` to implement uniform cost search.\n",
    "\n",
    "For reference, our solution is **4** line(s) of code.\n",
    "\n",
    "In addition to all of the utilities defined at the top of the colab notebook, the following functions are available in this question environment: `run_best_first_search`. You may not need to use all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d353b2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_uniform_cost_search(problem: PathCostProblem,\n",
    "                            max_expansions: int = 1000):\n",
    "  \"\"\"Uniform-cost search.\n",
    "\n",
    "  Use your implementation of `run_best_first_search`.\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c0bef8",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f115ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ucs_test1():\n",
    "  # If your results do not match the expected ones, make sure that you are tiebreaking\n",
    "  # as described in the docstring for `run_best_first_search`.\n",
    "  grid_problem = GridProblem((0, 0))\n",
    "  state_sequence, action_sequence, cost_sequence, num_expansions = run_uniform_cost_search(\n",
    "      grid_problem)\n",
    "  assert state_sequence == [(0, 0), (1, 0), (2, 0), (3, 0), (3, 1), (3, 2),\n",
    "                            (4, 2), (4, 3), (4, 4)]\n",
    "  assert action_sequence == [\n",
    "      'down', 'down', 'down', 'right', 'right', 'down', 'right', 'right'\n",
    "  ]\n",
    "  assert cost_sequence == [1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0]\n",
    "  assert num_expansions <= 30\n",
    "\n",
    "ucs_test1()\n",
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da1ab43",
   "metadata": {},
   "source": [
    "## A* Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4019af98",
   "metadata": {},
   "source": [
    "### Question\n",
    "Use your implementation of `run_best_first_search` to implement A* search.\n",
    "\n",
    "For reference, our solution is **4** line(s) of code.\n",
    "\n",
    "In addition to all of the utilities defined at the top of the colab notebook, the following functions are available in this question environment: `run_best_first_search`. You may not need to use all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a9640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_astar_search(problem: PathCostProblem, max_expansions: int = 1000):\n",
    "  \"\"\"A* search.\n",
    "\n",
    "  Use your implementation of `run_best_first_search`.\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14356b2c",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461de69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def astar_test1():\n",
    "  \"\"\"If your results do not match the expected ones, make sure that you are tiebreaking \n",
    "  as described in the docstring for `run_best_first_search`.\"\"\"\n",
    "  grid_problem = GridProblem((0, 0))\n",
    "  state_sequence, action_sequence, cost_sequence, num_expansions = run_astar_search(\n",
    "      grid_problem)\n",
    "  assert state_sequence == [(0, 0), (1, 0), (2, 0), (3, 0), (3, 1), (3, 2),\n",
    "                            (4, 2), (4, 3), (4, 4)]\n",
    "  assert action_sequence == [\n",
    "      'down', 'down', 'down', 'right', 'right', 'down', 'right', 'right'\n",
    "  ]\n",
    "  assert cost_sequence == [1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0]\n",
    "  assert num_expansions <= 20\n",
    "\n",
    "astar_test1()\n",
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4022c9ba",
   "metadata": {},
   "source": [
    "## Greedy Best-First Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdc4abf",
   "metadata": {},
   "source": [
    "### Question\n",
    "Use your implementation of `run_best_first_search` to implement GBFS.\n",
    "\n",
    "For reference, our solution is **4** line(s) of code.\n",
    "\n",
    "In addition to all of the utilities defined at the top of the colab notebook, the following functions are available in this question environment: `run_best_first_search`. You may not need to use all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cf3aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_greedy_best_first_search(problem: PathCostProblem,\n",
    "                                 max_expansions: int = 1000):\n",
    "  \"\"\"GBFS.\n",
    "\n",
    "  Use your implementation of `run_best_first_search`.\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd7e39f",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983b4c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbfs_test1():\n",
    "  \"\"\"If your results do not match the expected ones, make sure that you are tiebreaking\n",
    "  as described in the docstring for `run_best_first_search`.\"\"\"\n",
    "  initial_state = (0, 0)\n",
    "  problem = GridProblem(initial_state)\n",
    "  state_sequence, action_sequence, cost_sequence, num_expansions = run_greedy_best_first_search(\n",
    "      problem)\n",
    "  assert state_sequence == [(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (1, 4),\n",
    "                            (2, 4), (3, 4), (4, 4)]\n",
    "  assert action_sequence == [\n",
    "      'right', 'right', 'right', 'right', 'down', 'down', 'down', 'down'\n",
    "  ]\n",
    "  assert abs(num_expansions - 8) <= 1\n",
    "\n",
    "gbfs_test1()\n",
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0193fec0",
   "metadata": {},
   "source": [
    "## Path-cost Problem from Reward Problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee787a69",
   "metadata": {},
   "source": [
    "### Question\n",
    "Implement the reduction from a reward problem to a path-cost problem **as in lecture**.\n",
    "\n",
    "For reference, our solution is **30** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e00dc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def path_cost_problem_from_reward_problem(\n",
    "    reward_problem: RewardProblem) -> PathCostProblem:\n",
    "  \"\"\"Reduce a reward maximization problem into a path search problem.\n",
    "\n",
    "  You should take a close look that the class definition of `RewardProblem`, \n",
    "  since they will be handy. \n",
    "  Especially note that the horizon value is inclusive -- for a horizon value of $H$, \n",
    "  the agent should be allowed to step exactly $H$ number of steps.\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2815b4c",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a85bded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_reward_reduction_test1(horizon=2, max_expansions=1000):\n",
    "  path_problem = path_cost_problem_from_reward_problem(ForageProblem())\n",
    "  state_sequence, action_sequence, cost_sequence = run_uniform_cost_search(\n",
    "      path_problem, max_expansions=max_expansions)\n",
    "  print('actions', action_sequence)\n",
    "  print('states', state_sequence)\n",
    "  print('number of expansions', num_expansions)\n",
    "  assert len(action_sequence) == horizon\n",
    "  if horizon == 2:\n",
    "    assert action_sequence == [1, 1]\n",
    "    assert [s for s, h in state_sequence] == [1, 3, 1]\n",
    "\n",
    "path_reward_reduction_test1()\n",
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a27ff7",
   "metadata": {},
   "source": [
    "## Visualize Reward Fields\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1aa34c",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a23ece",
   "metadata": {},
   "source": [
    "The Fractal Problem and different reward fields. \n",
    "          You shoud read the code below to get rough idea what each reward field looks like.\n",
    "\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3c362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "FractalProblemState = namedtuple(\"FractalProblemState\", [\"x\", \"y\", \"t\"])\n",
    "\n",
    "\n",
    "class FractalProblem(RewardProblem):\n",
    "  \"\"\"A base class for \"fractal problem\".\n",
    "\n",
    "  Briefly, a fractal problem is as follows:\n",
    "    - The state space is the entire 2D Euclidean space\n",
    "    - The initial state is (0, 0)\n",
    "    - At each step $t$ the agent is allowed to move alone one of the directions of size\n",
    "      $$step_scale^{-t}$$.\n",
    "    - At each step the agent receives a scalar reward based on a reward field.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               action_directions=\"8-neighbors\",\n",
    "               step_scale=0.5,\n",
    "               horizon=6):\n",
    "    \"\"\"A base constructor for a fractal problem. \n",
    "\n",
    "    Args:\n",
    "      action_directions: allowed directions to move at each step. \n",
    "      step_scale: each step length is scaled down by this amount.\n",
    "    \"\"\"\n",
    "    super().__init__(FractalProblemState(0, 0, 1), horizon)\n",
    "    if action_directions == \"4-neighbors\":\n",
    "      self.action_directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    elif action_directions == \"8-neighbors\":\n",
    "      self.action_directions = [\n",
    "          (x, y)\n",
    "          for x, y in itertools.product([-1, 0, 1], [-1, 0, 1])\n",
    "          if (x, y) != (0, 0)\n",
    "      ]\n",
    "    else:\n",
    "      self.action_directions = action_directions\n",
    "    self.step_scale = step_scale\n",
    "    self.field_of_view = self.compute_field_of_view()\n",
    "\n",
    "  def compute_field_of_view(self, precision: float = 1.) -> Tuple[Tuple[float]]:\n",
    "    \"\"\"Computes a minimum rectangle that encloses all reachable states within horizon.\n",
    "\n",
    "    Args:\n",
    "      precision: the returned rectangle will have vertices that are integer \n",
    "        multiples of `precision`. Defaults to 1., which means that the rectangle\n",
    "        has integral vertices.\n",
    "\n",
    "    Returns:\n",
    "      the lower-left and upper-right vertices that represents the rectangle.\n",
    "    \"\"\"\n",
    "    extent = sum([self.step_scale**(t + 1) for t in range(self.horizon)])\n",
    "    dir_extent = np.array(self.action_directions) * extent\n",
    "    return (\n",
    "        tuple(np.floor(np.min(dir_extent, axis=0) / precision) * precision),\n",
    "        tuple(np.ceil(np.max(dir_extent, axis=0) / precision) * precision),\n",
    "    )\n",
    "\n",
    "  @property\n",
    "  def field_of_view_size(self):\n",
    "    return (self.field_of_view[1][0] - self.field_of_view[0][0],\n",
    "            self.field_of_view[1][1] - self.field_of_view[0][1])\n",
    "\n",
    "  def actions(self, state):\n",
    "    scale = self.step_scale**state.t\n",
    "    return [(x * scale, y * scale) for x, y in self.action_directions]\n",
    "\n",
    "  def step(self, state, action):\n",
    "    ax, ay = action\n",
    "    return FractalProblemState(state.x + ax, state.y + ay, state.t + 1)\n",
    "\n",
    "  def reward(self, state, action, next_state):\n",
    "    coord = np.array([state.x, state.y])\n",
    "    return self.reward_field(coord).item()\n",
    "\n",
    "  def reward_field(self, coords: np.ndarray):\n",
    "    \"\"\"The reward(s) at `coords`. Handle a batch of coordinates for efficiency of plotting.\n",
    "\n",
    "    Subclass must override to provide a meaningful reward field.\n",
    "\n",
    "    Args:\n",
    "      coords: an array with shape (2,) or (...batch, 2) denoting the coordinate(s)\n",
    "        to compute the reward field.\n",
    "\n",
    "    Returns:\n",
    "      A scalar or an array of shape (...batch,)\n",
    "    \"\"\"\n",
    "    return np.zeros(coords.shape[:-1])\n",
    "\n",
    "  @property\n",
    "  def rmax(self):\n",
    "    return 1.\n",
    "\n",
    "  def visualize_reward_field(self, resolution=(1000, 1000), show=False):\n",
    "    \"\"\"Visualize a reward field.\n",
    "\n",
    "    Args:\n",
    "      resolution: number of pixels (w, h) to discretize the field of view. \n",
    "    \"\"\"\n",
    "    fov = self.field_of_view\n",
    "    coords = np.dstack(\n",
    "        np.meshgrid(\n",
    "            np.linspace(fov[0][0], fov[1][0], resolution[0]),\n",
    "            np.linspace(fov[0][1], fov[1][1], resolution[1]),\n",
    "        )).reshape(-1, 2)\n",
    "    vals = self.reward_field(coords).reshape(resolution)\n",
    "    plt.imshow(vals,\n",
    "               origin=\"lower\",\n",
    "               extent=(fov[0][0], fov[1][0], fov[0][1], fov[1][1]))\n",
    "    plt.colorbar()\n",
    "    if show:\n",
    "      plt.show()\n",
    "    return self\n",
    "\n",
    "  def visualize_plan(self,\n",
    "                     state_sequence: StateSeq,\n",
    "                     show=False,\n",
    "                     **arrow_kwargs):\n",
    "    \"\"\"Visualize a plan on the reward field.\n",
    "\n",
    "    Args:\n",
    "      state_sequence: A sequence state moving in the fractal problem.\n",
    "      arrow_kwargs: passed to `plt.arrow`.\n",
    "    \"\"\"\n",
    "    for s1, s2 in zip(state_sequence[:-1], state_sequence[1:]):\n",
    "      x1, y1, d = s1\n",
    "      x2, y2, _ = s2\n",
    "      dx, dy = x2 - x1, y2 - y1\n",
    "      plt.arrow(x1,\n",
    "                y1,\n",
    "                dx,\n",
    "                dy,\n",
    "                length_includes_head=True,\n",
    "                width=0.01 / d**0.5,\n",
    "                fill=True,\n",
    "                **arrow_kwargs)\n",
    "    if show:\n",
    "      plt.show()\n",
    "    return plt\n",
    "\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "\n",
    "class GradientRewardFieldProblem(FractalProblem):\n",
    "  \"\"\"A fractal problem with a gradient reward field by adding multiple (scaled) \n",
    "  gaussian distributions.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               locs: Sequence[Tuple[int, int]] = ((0, 0),),\n",
    "               covs: Union[float, Sequence[Union[float, np.ndarray]]] = 0.1,\n",
    "               strengths: Union[float, Sequence[float]] = 1.,\n",
    "               **kwargs):\n",
    "    \"\"\"A reward field with a mixture of guassian gradients.\n",
    "\n",
    "    Args:\n",
    "      locs: the centers of the gaussians\n",
    "      covs: a scalar, a sequence of scalars, or a sequence of matrices \n",
    "        for the covariances of the gaussians.\n",
    "      strenghts: a scalar or a sequence of scalars for the scalaring factors \n",
    "        for each guassian. \n",
    "    \"\"\"\n",
    "    super().__init__(**kwargs)\n",
    "    self.locs = locs\n",
    "    if np.isscalar(covs):\n",
    "      covs = [covs] * len(self.locs)\n",
    "    covs = [np.eye(2) * cov if np.isscalar(cov) else cov for cov in covs]\n",
    "    self.covs = covs\n",
    "    if np.isscalar(strengths):\n",
    "      strengths = [strengths] * len(self.locs)\n",
    "    self.strengths = strengths\n",
    "\n",
    "  def reward_field(self, coords):\n",
    "    return sum([\n",
    "        multivariate_normal.pdf(coords, mean=loc, cov=cov) * strength\n",
    "        for loc, cov, strength in zip(self.locs, self.covs, self.strengths)\n",
    "    ])\n",
    "\n",
    "\n",
    "class NoisyRewardFieldProblem(FractalProblem):\n",
    "  \"\"\"A fractal problem with a reward field sampled from iid Beta distributions \n",
    "  in a discretized grid.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, seed=0, bin_size=5e-2, beta_params=(1., 2.), **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    # Compute number of bins required for the field of view\n",
    "    self.nbins = tuple(int(s / bin_size) for s in self.field_of_view_size)\n",
    "    # Initialize the random rewards within the field of view\n",
    "    rng_state = np.random.RandomState(seed)\n",
    "    self.random_locs = rng_state.beta(*beta_params, size=self.nbins)\n",
    "    self.random_locs = np.pad(self.random_locs, [(1, 1), (1, 1)],\n",
    "                              constant_values=0.)\n",
    "\n",
    "  def reward_field(self, coords):\n",
    "    binX, binY = tuple(\n",
    "        np.digitize(\n",
    "            coords[..., i],\n",
    "            np.linspace(self.field_of_view[0][i], self.field_of_view[1][i],\n",
    "                        self.nbins[i] + 1)) for i in range(2))\n",
    "    return self.random_locs[binX, binY]\n",
    "\n",
    "\n",
    "def get_fractal_problems() -> Dict[str, FractalProblem]:\n",
    "  \"\"\"We have defined here three fractal problems with different reward fields.\n",
    "\n",
    "  You are encouraged to play with the class and function definitions above.\n",
    "  But, DO NOT CHANGE THIS FUNCTION --- it exists to protect you from \n",
    "  accidentally changing the problems.\n",
    "  \"\"\"\n",
    "  return {\n",
    "      \"reward-field-1\":\n",
    "          GradientRewardFieldProblem(locs=[(-1.3, -1.3), (-1.3, 1.3),\n",
    "                                           (1.3, -1.3), (1.3, 1.3)],\n",
    "                                     covs=[.3, .3, .3, .3],\n",
    "                                     strengths=[2., 2., 2., 2.]),\n",
    "      \"reward-field-2\":\n",
    "          GradientRewardFieldProblem(locs=[(-1.3, -1.3), (-1.3, 1.3),\n",
    "                                           (1.3, -1.3), (1.3, 1.3)],\n",
    "                                     covs=[.25, .25, .25, .2],\n",
    "                                     strengths=[1, 1, 1, 1.5]),\n",
    "      \"reward-field-3\":\n",
    "          NoisyRewardFieldProblem(seed=42, bin_size=5e-2,\n",
    "                                  beta_params=(0.2, 2.)),\n",
    "  }\n",
    "\n",
    "\n",
    "# Feel free to play with these problem instances however you'd like\n",
    "fractal_problems = get_fractal_problems()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ac8de8",
   "metadata": {},
   "source": [
    "### Question\n",
    "We have defined for you three fractal problems in `get_fractal_problems()`.\n",
    "          We have provided you below some code (also in the Colab notebook) to visualize the reward fields of these three problems. \n",
    "          You should run this visualization code in the Colab notebook and get an idea of what each reward field looks like.\n",
    "          **You do not need to submit any code for this question --- it is not graded.**\n",
    "          Instead, we will ask you questions in the following problems to confirm your understanding.\n",
    "\n",
    "\n",
    "For reference, our solution is **3** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13533b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_fractal_problems():\n",
    "  \"\"\"Visualize the fractal problems with different rewards fields.\n",
    "  \"\"\"\n",
    "  for name, problem in get_fractal_problems().items():\n",
    "    plt.title(name)\n",
    "    problem.visualize_reward_field(show=True)\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca408e99",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41c8924",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a650356d",
   "metadata": {},
   "source": [
    "## Play with MCTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6f312c",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a986d30",
   "metadata": {},
   "source": [
    "Our implementation of MCTS.\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1886ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#\n",
    "# MCTS\n",
    "#\n",
    "##############################################################################\n",
    "\n",
    "import dataclasses\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=False, eq=False)\n",
    "class MCTNode:\n",
    "  \"\"\"Node in the Monte Carlo search tree, keeps track of the children states.\"\"\"\n",
    "  state: State\n",
    "  U: float\n",
    "  N: int\n",
    "  horizon: int\n",
    "  parent: Optional['MCTNode']\n",
    "  children: Dict['MCTNode', Action] = dataclasses.field(default_factory=dict)\n",
    "\n",
    "\n",
    "def ucb(n: MCTNode, C: float = 1.4) -> float:\n",
    "  \"\"\"UCB for a node, note the C argument\"\"\"\n",
    "  return np.inf if n.N == 0 else (n.U / n.N +\n",
    "                                  C * np.sqrt(np.log(n.parent.N) / n.N))\n",
    "\n",
    "\n",
    "def run_mcts_search(problem: RewardProblem,\n",
    "                    C: float = 1.4,\n",
    "                    iteration_budget: int = 1000,\n",
    "                    step_budget: int = np.inf,\n",
    "                    time_budget: float = np.inf):\n",
    "  \"\"\"A generic MCTS search implementation.\n",
    "\n",
    "  Args:\n",
    "    problem: a reward problem.\n",
    "    C: the UCB parameter.\n",
    "    iteration_budget: maximum iterations to run the search.\n",
    "    step_budget: maximum number of allowed problems steps.\n",
    "    time_budget: maximum allowed CPU time.\n",
    "\n",
    "  Returns:\n",
    "    state_sequence: A list of states.\n",
    "    action_sequence: A list of actions.\n",
    "    avg_reward_sequence: A list of rewards\n",
    "    num_steps: An int.\n",
    "  \"\"\"\n",
    "  if min(iteration_budget, step_budget, time_budget) == np.inf:\n",
    "    raise ValueError(\"Must provide at least one budget\")\n",
    "\n",
    "  problem_step_count = 0\n",
    "\n",
    "  class BudgetExceeded(Exception):\n",
    "    pass\n",
    "\n",
    "  def step_helper(state, action):\n",
    "    \"\"\"helper to track the problem's step count.\"\"\"\n",
    "    nonlocal problem_step_count\n",
    "    problem_step_count += 1\n",
    "    if problem_step_count > step_budget:\n",
    "      raise BudgetExceeded(\"step budget exceeded\")\n",
    "    return problem.step(state, action)\n",
    "\n",
    "  if time_budget != np.inf:\n",
    "\n",
    "    def signal_handler(signum, frame):\n",
    "      raise BudgetExceeded(\"time budget exceeded\")\n",
    "\n",
    "    prev_handler = signal.signal(signal.SIGPROF, signal_handler)\n",
    "    signal.setitimer(signal.ITIMER_PROF, time_budget)\n",
    "\n",
    "  ucb_fixed_C = functools.partial(ucb, C=C)\n",
    "\n",
    "  def select(n: MCTNode) -> MCTNode:\n",
    "    \"\"\"select a leaf node in the tree\"\"\"\n",
    "    if n.children:\n",
    "      # print([(c, ucb(c)) for c in n.children])\n",
    "      ucb_pick = max(n.children.keys(), key=ucb_fixed_C)\n",
    "      # print('ucb_pick', ucb_pick, 'act', n.children[ucb_pick])\n",
    "      return select(ucb_pick)\n",
    "    else:\n",
    "      return n\n",
    "\n",
    "  def expand(n: MCTNode) -> MCTNode:\n",
    "    \"\"\"expand the leaf node by adding all its children states\"\"\"\n",
    "    assert not n.children\n",
    "    if not n.horizon == 0:\n",
    "      for action in problem.actions(n.state):\n",
    "        child_state = step_helper(n.state, action)\n",
    "        new_node = MCTNode(state=child_state,\n",
    "                           horizon=n.horizon - 1,\n",
    "                           parent=n,\n",
    "                           U=0,\n",
    "                           N=0)\n",
    "        n.children[new_node] = action\n",
    "      child = random.choice(list(n.children.keys()))\n",
    "      # print('expand', 'state', n.state, 'action', n.children[child])\n",
    "      return child\n",
    "    else:\n",
    "      return n\n",
    "\n",
    "  def simulate(node: MCTNode) -> float:\n",
    "    \"\"\"simulate the utility of current state by randomly picking a step\"\"\"\n",
    "    state = node.state\n",
    "    total_reward = 0\n",
    "    for h in range(node.horizon, 0, -1):\n",
    "      action = random.choice(problem.actions(state))\n",
    "      child_state = step_helper(state, action)\n",
    "      reward = problem.reward(state, action, child_state)\n",
    "      # print('   rollout: h', h, 'action', action, 'reward', reward)\n",
    "      total_reward += reward\n",
    "      state = child_state\n",
    "    # print('simulate', node.state, '->', total_reward)\n",
    "    return total_reward\n",
    "\n",
    "  def backup(n: MCTNode, value: float) -> None:\n",
    "    \"\"\"passing the utility back to all parent nodes\"\"\"\n",
    "    if n.parent:\n",
    "      # Need to include the reward on the action *into* n\n",
    "      a = n.parent.children[n]\n",
    "      r = problem.reward(n.parent.state, a, n.state)\n",
    "      n.U += value + r\n",
    "      n.N += 1\n",
    "      backup(n.parent, value + r)\n",
    "    else:\n",
    "      n.N += 1\n",
    "\n",
    "  root = MCTNode(state=problem.initial,\n",
    "                 horizon=problem.horizon,\n",
    "                 parent=None,\n",
    "                 U=0,\n",
    "                 N=0)\n",
    "\n",
    "  try:\n",
    "    i = 0\n",
    "    while i < iteration_budget:\n",
    "      leaf = select(root)\n",
    "      child = expand(leaf)\n",
    "      value = simulate(child)\n",
    "      # print(i, leaf, child, value)\n",
    "      backup(child, value)\n",
    "      i += 1\n",
    "  except BudgetExceeded:\n",
    "    pass\n",
    "\n",
    "  if time_budget != np.inf:\n",
    "    signal.setitimer(signal.ITIMER_PROF, 0)\n",
    "    signal.signal(signal.SIGPROF, prev_handler)\n",
    "\n",
    "  # print('children of root', root.children)\n",
    "\n",
    "  return finish_mcts_plan(problem, root)\n",
    "\n",
    "\n",
    "def finish_mcts_plan(problem: RewardProblem, node: MCTNode):\n",
    "  \"\"\"Helper for run_mcts_search. Recover the plan. \"\"\"\n",
    "  state_sequence = [node.state]\n",
    "  action_sequence = []\n",
    "  reward_sequence = []\n",
    "\n",
    "  while node.children:\n",
    "    max_node = max(node.children,\n",
    "                   key=lambda p: p.U / p.N if p.N > 0 else -np.inf)\n",
    "    max_action = node.children.get(max_node)\n",
    "    action_sequence.append(max_action)\n",
    "    state_sequence.append(max_node.state)\n",
    "    reward_sequence.append(\n",
    "        problem.reward(node.state, max_action, max_node.state))\n",
    "    node = max_node\n",
    "\n",
    "  return state_sequence, action_sequence, reward_sequence\n",
    "\n",
    "\n",
    "class ForageProblem(RewardProblem):\n",
    "\n",
    "  def __init__(self, horizon=8):\n",
    "    super().__init__(\"s1\", horizon)\n",
    "\n",
    "  def actions(self, state):\n",
    "    return [\"a1\", \"a2\"]\n",
    "\n",
    "  def step(self, state, action):\n",
    "    \"\"\"\n",
    "    T(s, a) -> s'\n",
    "    s1  a1  -> s3\n",
    "    s1  a2  -> s2\n",
    "    s2  a1  -> s4\n",
    "    s2  a2  -> s4\n",
    "    s3  a1  -> s1\n",
    "    s3  a2  -> s4\n",
    "    s4  a1  -> s1\n",
    "    s4  a2  -> s1\n",
    "    \"\"\"\n",
    "    transition = np.array([[3, 2], [4, 4], [1, 4], [1, 1]], dtype=int)\n",
    "\n",
    "    state_idx, action_idx = int(state[1:]) - 1, int(action[1:]) - 1\n",
    "    return f\"s{transition[state_idx, action_idx]}\"\n",
    "\n",
    "  def reward(self, state, action, next_state):\n",
    "    \"\"\"\n",
    "    R(s, a, s') = Food_At(s) - Length(s, a)\n",
    "    s     Food_At\n",
    "    s1      2\n",
    "    s2      3\n",
    "    s3      2\n",
    "    s4      10\n",
    "    (s, a) -> length\n",
    "    s1  a1  -> 1\n",
    "    s1  a2  -> 1\n",
    "    s2  a1  -> 7\n",
    "    s2  a2  -> 7\n",
    "    s3  a1  -> 1\n",
    "    s3  a2  -> 5\n",
    "    s4  a1  -> 2\n",
    "    s4  a2  -> 2\n",
    "    \"\"\"\n",
    "    food_at = np.array([2, 3, 2, 10])\n",
    "    length = np.array([[1, 1], [7, 7], [1, 5], [2, 2]], dtype=int)\n",
    "    state_idx, action_idx = int(state[1:]) - 1, int(action[1:]) - 1\n",
    "    return food_at[state_idx] - length[state_idx, action_idx]\n",
    "\n",
    "  @property\n",
    "  def rmax(self):\n",
    "    return 8\n",
    "\n",
    "\n",
    "def mcts_test1(horizon=2, iteration_budget=100):\n",
    "  problem = ForageProblem()\n",
    "  state_sequence, action_sequence, reward_sequence = run_mcts_search(\n",
    "      problem, horizon, iteration_budget=iteration_budget)\n",
    "\n",
    "  print('actions', action_sequence)\n",
    "  print('states', state_sequence)\n",
    "\n",
    "  if horizon == 2:\n",
    "    assert state_sequence == [3, 1]\n",
    "    assert action_sequence == [1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a68b48",
   "metadata": {},
   "source": [
    "### Question\n",
    "We have provided you an implementation of MCTS for reward problems. \n",
    "          You should make sure that you can run our implementation on the reward problems we have defined for you.\n",
    "          You are also encouraged to look into our code to see under how our MCTS is implemented.\n",
    "          **You do not need to submit any code for this question --- it is not graded.**\n",
    "          Instead, we will ask you questions in the following problems to confirm your understanding.\n",
    "\n",
    "\n",
    "For reference, our solution is **2** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44faecf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_mcts():\n",
    "  \"\"\"Solve one of the fractal problems and visualize the plan.\n",
    "  \"\"\"\n",
    "  problem = get_fractal_problems()[\"reward-field-1\"]\n",
    "  plan = run_mcts_search(problem, iteration_budget=10000)\n",
    "  problem.visualize_reward_field().visualize_plan(plan[0], show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b068809",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f039ed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Tests passed.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw01.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
