{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18de371c",
   "metadata": {},
   "source": [
    "# Miniproject 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f00a756",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install backports.cached_property\n",
    "\n",
    "# Setup matplotlib animation\n",
    "import matplotlib\n",
    "matplotlib.rc('animation', html='jshtml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6776d08f",
   "metadata": {},
   "source": [
    "## Imports and Utilities\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0590a65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "def implementation_for(cls, method_name: Optional[str] = None):\n",
    "    \"\"\"Helper to provide a concrete implementation for a class method.\n",
    "\n",
    "    This utility allows one to define methods outside of a class defintion.\n",
    "    Thus allowing one to provide the implementations of a class \"incrementally\". \n",
    "    This utility is useful for gradually submitting class method implementations \n",
    "    to catsoop. \n",
    "\n",
    "    It is intended to be used as a decorator.\n",
    "\n",
    "    Args:\n",
    "        cls: a Python class on which we are implementing the method.\n",
    "        method_name: an optional method name to indicate which method we are \n",
    "            implementing. If not provided, infer the method name by using the \n",
    "            decorated function's name.\n",
    "\n",
    "    Example:\n",
    "        >>> class A: \n",
    "        ...   def foo(self):\n",
    "        ...     raise NotImplementedError()\n",
    "\n",
    "        >>> @implementation_for(A)\n",
    "        ... def foo(self):\n",
    "        ...   print(\"implemented foo!\")\n",
    "\n",
    "        >>> A().foo()\n",
    "        implemented foo!\n",
    "    \"\"\"\n",
    "\n",
    "    def decorator(meth: Callable) -> Callable:\n",
    "        mname = method_name or meth.__name__\n",
    "        setattr(cls, mname, meth)\n",
    "        return meth\n",
    "\n",
    "    return decorator\n",
    "\n",
    "\n",
    "\n",
    "from typing import (Callable, Iterable, List, Sequence, Optional, Dict)\n",
    "\n",
    "from abc import abstractmethod, ABCMeta\n",
    "import itertools\n",
    "import math\n",
    "import random\n",
    "import dataclasses\n",
    "from collections import namedtuple\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import scipy.stats\n",
    "import scipy.special\n",
    "\n",
    "# Basic data structures.\n",
    "\n",
    "# x, y coordinate and the rotation of the robot.\n",
    "Pose = namedtuple(\"Pose\", [\"x\", \"y\", \"theta\"])\n",
    "# unique id (str) and x, y coordinate of the landmark.\n",
    "Landmark = namedtuple(\"Landmark\", [\"id\", \"x\", \"y\"])\n",
    "# r --- distance to the landmark; b --- angle to the landmark, computed by arctan2.\n",
    "Measurement = namedtuple(\"Measurement\", [\"landmark_id\", \"r\", \"b\"])\n",
    "# delta_p --- distance to move; delta_theta --- rotation of the robot\n",
    "Command = namedtuple(\"Command\", [\"delta_p\", \"delta_theta\"])\n",
    "\n",
    "\n",
    "def grid_of_landmarks(\n",
    "        x_range: Iterable[float] = range(-4, 20, 4),\n",
    "        y_range: Iterable[float] = range(-4, 20, 4),\n",
    ") -> Sequence[Landmark]:\n",
    "    \"\"\"Constructs a grid of landmarks from a catesion product of x and y coordinates.\"\"\"\n",
    "    return tuple(\n",
    "        Landmark(f\"landmark-{i}\", *loc)\n",
    "        for i, loc in enumerate(itertools.product(x_range, y_range)))\n",
    "\n",
    "\n",
    "class Inference(metaclass=ABCMeta):\n",
    "    \"\"\"Interface for an inference algorithm (localization or SLAM variants).\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def estimated_pose(self) -> Pose:\n",
    "        \"\"\"Returns the current estimation of the robot's Pose.\"\"\"\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def estimated_landmarks(self) -> Sequence[Landmark]:\n",
    "        \"\"\"Returns the current estimation of the landmarks.\"\"\"\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def init(self, init_state: Pose) -> None:\n",
    "        \"\"\"Initialize the inference with an initial pose.\"\"\"\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self, measurements: Sequence[Measurement]) -> None:\n",
    "        \"\"\"An update step of the inference, called after every simulation step.\"\"\"\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def plot_state(self, ax) -> None:\n",
    "        \"\"\"Helper to visualize the internal state of the inference algorithm.\"\"\"\n",
    "        ...\n",
    "\n",
    "\n",
    "def normalize_angles(angles: np.array) -> np.ndarray:\n",
    "    \"\"\"Given an array of angles in radians, element-wise normalize the angles to the range [-pi, pi].\n",
    "\n",
    "    Args:\n",
    "        angles: array of any shape.\n",
    "\n",
    "    Returns:\n",
    "        normalized angles, array of same shape as the input.\n",
    "    \"\"\"\n",
    "    return np.arctan2(np.sin(angles), np.cos(angles))\n",
    "\n",
    "\n",
    "def circular_mean(angles: np.array, axis=None) -> float:\n",
    "    \"\"\"Given an array of angles in radians, find the [circular mean](https://en.wikipedia.org/wiki/Circular_mean).\n",
    "\n",
    "    Args:\n",
    "        angles: array of any shape.\n",
    "        axis: axis or axes along which the means are computed. The default is to compute the mean of the flattened array.\n",
    "\n",
    "    Returns:\n",
    "        mean of the angles, normalized to [-pi, pi].\n",
    "    \"\"\"\n",
    "    return np.arctan2(np.sum(np.sin(angles), axis=axis),\n",
    "                      np.sum(np.cos(angles), axis=axis))\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class SimulationResult:\n",
    "    \"\"\"A helper class to hold the results of running a simulation.\"\"\"\n",
    "    sim: 'Simulator'  # Simulator that generated this result\n",
    "    infer: Optional[Inference]  # Inference procedure that generated this result\n",
    "    true_poses: Sequence[Pose]\n",
    "    estimated_poses: Optional[Sequence[Pose]] = None\n",
    "    # History snapshots for the Inference, for animation purpose\n",
    "    snapshots: Optional[Sequence[Inference]] = None\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=False)\n",
    "class Simulator:\n",
    "    \"\"\"A simulator for a point robot in R^2.\n",
    "\n",
    "    The simulator simulates the motion and sensor noises of the robot by \n",
    "    sampling from 2D Gaussians.\n",
    "    The simulation has a list of landmarks, each with a unique identifier.\n",
    "    The sensor of the robot can sense, for each landmark in the robot's sensing\n",
    "    range, a noise-corrupted distance and bearing to that landmarks (together\n",
    "    with the identifier that identifies the landmark).\n",
    "    \"\"\"\n",
    "\n",
    "    # Initial simulator state\n",
    "    init_state: Pose = Pose(0, 0, 0)\n",
    "\n",
    "    # 2x2 covariance matrix of the motion noise.\n",
    "    motion_noise_covariance: np.ndarray = np.diag([1e-3, np.deg2rad(3)**2])\n",
    "\n",
    "    # 2x2 covariance matrix of the sensor noise.\n",
    "    sensor_noise_covariance: np.ndarray = np.diag([1, np.deg2rad(5)**2])\n",
    "\n",
    "    # A set of landmarks\n",
    "    landmarks: Sequence[Landmark] = grid_of_landmarks()\n",
    "\n",
    "    # Robot can only sense landmarks within this range\n",
    "    max_sensing_range: float = np.inf\n",
    "\n",
    "    # Current state of the robot\n",
    "    state: Pose = dataclasses.field(init=False, default=None)\n",
    "\n",
    "    # Random seed control\n",
    "    rng: np.random.Generator = dataclasses.field(init=False, default=None)\n",
    "    # Using the same seed will induce the same trajectory\n",
    "    seed: dataclasses.InitVar[int] = 0\n",
    "\n",
    "    def __post_init__(self, seed: int):\n",
    "        assert (len({l.id for l in self.landmarks\n",
    "                    }) == len(self.landmarks)), \"Landmark must have unique IDs\"\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def init(self) -> None:\n",
    "        self.state = self.init_state\n",
    "\n",
    "    def simulate_motion(self, command: Command) -> None:\n",
    "        \"\"\"Simulate robot motion. This function updates the value of\n",
    "        self.state.\n",
    "\n",
    "        Args:\n",
    "          command: a Command tuple containing fields delta_p(float), the \n",
    "          distance of the movement, and delta_theta(float), the rotation of \n",
    "          the movement.\n",
    "        \"\"\"\n",
    "        noise = self.rng.multivariate_normal(np.zeros(2),\n",
    "                                             self.motion_noise_covariance)\n",
    "        x = (self.state.x + math.cos(self.state.theta) *\n",
    "             (command.delta_p + noise[0]))\n",
    "        y = (self.state.y + math.sin(self.state.theta) *\n",
    "             (command.delta_p + noise[0]))\n",
    "        theta = self.state.theta + command.delta_theta + noise[1]\n",
    "        self.state = Pose(x, y, theta)\n",
    "\n",
    "    def simulate_sensing(self) -> Sequence[Measurement]:\n",
    "        \"\"\"Simulate the robot sensing process. This function returns a list of\n",
    "        measurements, in the same order as the self.landmarks list.\n",
    "        Specifically, it measures the distance and the angle w.r.t. each\n",
    "        landmark, and add a Gaussian noise on the measurement.\n",
    "\n",
    "        Returns:\n",
    "          measurements: a list of measurements.\n",
    "        \"\"\"\n",
    "        measurements = []\n",
    "        # Simulate that the measurements' order is random\n",
    "        for landmark in sorted(self.landmarks, key=lambda _: random.random()):\n",
    "            r = math.hypot(landmark.x - self.state.x, landmark.y - self.state.y)\n",
    "\n",
    "            if r >= self.max_sensing_range:\n",
    "                # Can't sense this landmark, skip\n",
    "                continue\n",
    "\n",
    "            b = math.atan2(landmark.y - self.state.y,\n",
    "                           landmark.x - self.state.x) - self.state.theta\n",
    "            # Normalize angle to -pi to pi\n",
    "            b = math.atan2(math.sin(b), math.cos(b))\n",
    "\n",
    "            noise = self.rng.multivariate_normal(np.zeros(2),\n",
    "                                                 self.sensor_noise_covariance)\n",
    "\n",
    "            measurements.append(\n",
    "                Measurement(landmark.id, r + noise[0], b + noise[1]))\n",
    "\n",
    "        return measurements\n",
    "\n",
    "    def run(self,\n",
    "            commands: Iterable[Command],\n",
    "            infer: Optional[Inference] = None,\n",
    "            snapshot_every_n: Optional[int] = 0) -> SimulationResult:\n",
    "        \"\"\"Run the simulation with a sequence of commands. Optionally, execute\n",
    "        an inference algorithm along way with the simulation.\n",
    "\n",
    "        Args:\n",
    "            commands: an interable of commands.\n",
    "            infer: the inference algorithm to run the simulation with.\n",
    "            snapshot_every_n: snapshot the inference algorithm every n steps.\n",
    "                if <= 0, don't take any snapshot. \n",
    "\n",
    "        Returns:\n",
    "            SimulationResult\n",
    "        \"\"\"\n",
    "        true_poses, est_poses, snapshots = [], None, None\n",
    "\n",
    "        self.init()\n",
    "\n",
    "        i = 0\n",
    "        true_poses.append(self.state)\n",
    "        if infer:\n",
    "            est_poses, snapshots = [], []\n",
    "            infer.init(self.init_state)\n",
    "            est_poses.append(infer.estimated_pose())\n",
    "            if snapshot_every_n > 0 and i % snapshot_every_n == 0:\n",
    "                snapshots.append(copy.deepcopy(infer))\n",
    "\n",
    "        for command in commands:\n",
    "            i += 1\n",
    "            # Simulate motion\n",
    "            self.simulate_motion(command)\n",
    "            true_poses.append(self.state)\n",
    "            if infer:\n",
    "                # Simulate measurement\n",
    "                measurements = self.simulate_sensing()\n",
    "                infer.update(command, measurements)\n",
    "                est_poses.append(infer.estimated_pose())\n",
    "                if snapshot_every_n > 0 and i % snapshot_every_n == 0:\n",
    "                    snapshots.append(copy.deepcopy(infer))\n",
    "\n",
    "        return SimulationResult(self, copy.deepcopy(infer), true_poses,\n",
    "                                est_poses, snapshots)\n",
    "\n",
    "\n",
    "def drive_in_a_square_commands() -> Iterable[Command]:\n",
    "    \"\"\"Commands to simulate the robot's movement in a square.\"\"\"\n",
    "    for _ in range(4):\n",
    "        for i in range(100):\n",
    "            yield Command(.1, 0)\n",
    "        yield Command(0, np.pi / 2)\n",
    "\n",
    "\n",
    "def patrol_commands() -> Iterable[Command]:\n",
    "    \"\"\"Commands that walks back and forth horizontally.\"\"\"\n",
    "    for i in range(10):\n",
    "        yield Command(0.5, 0)\n",
    "    yield Command(0, np.pi)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_inference(sim: Simulator,\n",
    "                  commands: Sequence[Command] = tuple(\n",
    "                      drive_in_a_square_commands()),\n",
    "                  mode: str = \"localization\",\n",
    "                  num_particles: int = 100,\n",
    "                  snapshot_every_n=0) -> SimulationResult:\n",
    "    \"\"\"Utility to run inference on a simulator with a particular inference mode.\n",
    "\n",
    "    Args:\n",
    "        sim: the simulator instance to run the simulation.\n",
    "        commands: see `Simulator.run`.\n",
    "        mode: specified the mode of the inference; one of \"localization\", \"SLAM\" or \"RBSLAM\".\n",
    "            You must implement the corresponding inference algorithm in order \n",
    "            to perform inference in any mode.\n",
    "        num_particles: number of particles of the particle filter.\n",
    "        snapshot_every_n: see `Simulator.run`.\n",
    "\n",
    "    Returns:\n",
    "        A simulation result returned by `Simulator.run`.\n",
    "    \"\"\"\n",
    "\n",
    "    mode = mode.lower()\n",
    "    try:\n",
    "        if mode == \"localization\":\n",
    "            infer = Localization(\n",
    "                motion_noise_covariance=sim.motion_noise_covariance,\n",
    "                sensor_noise_covariance=sim.sensor_noise_covariance,\n",
    "                landmarks=sim.landmarks,\n",
    "                num_particles=num_particles,\n",
    "            )\n",
    "        elif mode == \"slam\":\n",
    "            infer = SLAM(\n",
    "                motion_noise_covariance=sim.motion_noise_covariance,\n",
    "                sensor_noise_covariance=sim.sensor_noise_covariance,\n",
    "                num_particles=num_particles,\n",
    "            )\n",
    "        elif mode == \"rbslam\":\n",
    "            infer = RBSLAM(\n",
    "                motion_noise_covariance=sim.motion_noise_covariance,\n",
    "                sensor_noise_covariance=sim.sensor_noise_covariance,\n",
    "                num_particles=num_particles,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unrecoginized inference: {mode}\")\n",
    "    except NotImplementedError:\n",
    "        raise NotImplementedError(\n",
    "            f\"You must completely implement the {infer.__class__} class to \"\n",
    "            f\"run simulation under {mode} mode\")\n",
    "\n",
    "    return sim.run(commands, infer=infer, snapshot_every_n=snapshot_every_n)\n",
    "\n",
    "\n",
    "def plot_samples(x, y, fov=((-6, -6), (20, 20)), show=True):\n",
    "    \"\"\"Plots a list of samples of x, y coordinates as input and scatter plot the samples.\n",
    "\n",
    "    Args:\n",
    "        x: array of shape (N, ), the x-coordinates of the pose component of the particles.\n",
    "        y: array of shape (N, ), the y-coordinates of the pose component of the particles.\n",
    "        fov: a rectangular field of view. The function plots within this \n",
    "            rectanglular fov.\n",
    "        show: whether to display the plot immediately.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "    ax.axis([fov[0][0], fov[1][0], fov[0][1], fov[1][1]])\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_aspect('equal', 'box')\n",
    "\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.plot(x, y, 'g+')\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_trajectories(true_poses: Sequence[Pose],\n",
    "                      estimated_poses: Sequence[Pose],\n",
    "                      landmarks: Optional[Sequence[Landmark]] = None,\n",
    "                      ax: Optional['matplotlib.axes.Axes'] = None,\n",
    "                      fov=((-6, -6), (20, 20)),\n",
    "                      show=True):\n",
    "    \"\"\"Plot true and estimated poses on the same plot by overlaying them.\n",
    "    It ignores the pose.theta and only plots the location.\n",
    "\n",
    "    Args:\n",
    "        true_poses: a sequence of true poses.\n",
    "        estimated_poses: a sequence of estimated poses.\n",
    "        landmarks: optinally plot the location of true landmarks.\n",
    "        ax: an optional matplotlib Axes on which to plot the trajectories.\n",
    "        fov: a rectangular field of view. The function plots within this \n",
    "            rectanglular fov.\n",
    "        show: whether to display the plot immediately.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "    ax.axis([fov[0][0], fov[1][0], fov[0][1], fov[1][1]])\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_aspect('equal', 'box')\n",
    "\n",
    "    if landmarks:\n",
    "        ax.scatter([l.x for l in landmarks], [l.y for l in landmarks],\n",
    "                   marker=\"o\",\n",
    "                   color=\"black\")\n",
    "\n",
    "    ax.plot([p.x for p in true_poses], [p.y for p in true_poses],\n",
    "            color=(0, 0, 1, 0.5))\n",
    "    ax.plot([p.x for p in estimated_poses], [p.y for p in estimated_poses],\n",
    "            color=(1, 0, 0, 0.5))\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_simulation_result(result: SimulationResult,\n",
    "                           ax: Optional['matplotlib.axes.Axes'] = None,\n",
    "                           fov=((-6, -6), (20, 20)),\n",
    "                           title=None,\n",
    "                           show=True):\n",
    "    \"\"\"Plot the inference result (the state at last step of the simulation result).\n",
    "\n",
    "    Args:\n",
    "        result: result of a simulation.\n",
    "        ax: an optional matplotlin Axes to draw the result.\n",
    "        fov: see `plot_trajectories`.\n",
    "        show: whether to display the plot immediately.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "\n",
    "    plot_trajectories(result.true_poses,\n",
    "                      result.estimated_poses,\n",
    "                      landmarks=result.sim.landmarks,\n",
    "                      ax=ax,\n",
    "                      fov=fov,\n",
    "                      show=False)\n",
    "\n",
    "    result.infer.plot_state(ax)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def render_animation(\n",
    "    result: SimulationResult,\n",
    "    fov=((-6, -6), (20, 20)),\n",
    "    save_file: Optional[str] = None,\n",
    ") -> 'matplotlib.animation.FuncAnimation':\n",
    "    \"\"\"Render simulation as a matplotlib animation.\n",
    "\n",
    "    Args:\n",
    "        result: result of a simulation.\n",
    "        fov: see `plot_trajectories`.\n",
    "        save_file: an optional string file name to save the animation. \n",
    "\n",
    "    Returns:\n",
    "        an instance of `matplotlib.animation.FuncAnimation`. You can do further \n",
    "        processing on the returned animation, display it on Colab, or save it\n",
    "        as an mp4 file.  \n",
    "    \"\"\"\n",
    "    if result.snapshots is None:\n",
    "        raise ValueError(\"SimulationResult does not have any snapshots. \"\n",
    "                         \"Please re-run simulation with snapshot_every_n > 0.\")\n",
    "\n",
    "    import matplotlib.animation as animation\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    snapshot_every_n = len(result.true_poses) // len(result.snapshots)\n",
    "    true_poses = result.true_poses[::snapshot_every_n]\n",
    "    estimated_poses = result.estimated_poses[::snapshot_every_n]\n",
    "\n",
    "    def render_frame(i):\n",
    "        ax.clear()\n",
    "        ax.set_title(f\"Step {i}\")\n",
    "        plot_trajectories(true_poses[:i + 1],\n",
    "                          estimated_poses[:i + 1],\n",
    "                          landmarks=result.sim.landmarks,\n",
    "                          ax=ax,\n",
    "                          fov=fov,\n",
    "                          show=False)\n",
    "        result.snapshots[i].plot_state(ax)\n",
    "        return ax\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, render_frame, len(result.snapshots))\n",
    "    if save_file:\n",
    "        anim.save(f\"{save_file}.mp4\", fps=60)\n",
    "    return anim\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0844bf4",
   "metadata": {},
   "source": [
    "## Warmup 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669356b6",
   "metadata": {},
   "source": [
    "### Question\n",
    "Let's make sure we know how to sample from a set \n",
    "of values based on their weights.\n",
    "\n",
    "Please write a function that:\n",
    "- Takes in a list of samples and associated weights.\n",
    "- Re-samples from the list of samples with replacement according the\n",
    "likelihood of each sample given by the list of weights.\n",
    "\n",
    "_Hint:_ You should use the function `numpy.random.choice`.\n",
    "\n",
    "\n",
    "For reference, our solution is **2** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649cf8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_1(samples: np.ndarray, weights: np.ndarray,\n",
    "             nr_samples: int) -> np.ndarray:\n",
    "    \"\"\"Draw N samples from the input list based on their weights.\n",
    "\n",
    "    Args:\n",
    "      samples: a numpy array of shape (n_samples, ), a list of samples.\n",
    "      weights: a numpy float array of shape (n_samples, ), indicating \n",
    "        the weights for individual samples.\n",
    "      nr_samples: an integer, indicating the number of samples to draw.\n",
    "\n",
    "    Returns:\n",
    "      resampled_samples: the return numpy array, containing nr_samples\n",
    "        samples drawn from the input list, based on the weights with replacement.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a21f5c",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8913db22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random; import numpy.random; random.seed(0); numpy.random.seed(0);\n",
    "assert np.allclose(warmup_1(np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=np.int64), np.array([0.03191091, 0.10365558, 0.07293406, 0.13085621, 0.12995933, 0.0454193 , 0.04606439, 0.1336077 , 0.15964489, 0.14594763], dtype=np.float64), 10), np.array([6, 8, 7, 6, 4, 7, 4, 9, 9, 4], dtype=np.int64), atol=1e-6)\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d059e6bb",
   "metadata": {},
   "source": [
    "## Warmup 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c471927f",
   "metadata": {},
   "source": [
    "### Question\n",
    "We are going to be sampling from Gaussian distributions in this miniproject. \n",
    "First, let us check that we know how to sample from a multivariate Gaussian.\n",
    "Please write a function that takes in a mean vector, a covariance matrix and a integer number \n",
    "of samples and returns a list of the requested number of samples drawn from a Gaussian with the given mean and covariance.\n",
    "\n",
    "_Hint:_ You should use the function `numpy.random.multivariate_normal`.\n",
    "\n",
    "\n",
    "For reference, our solution is **2** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea19b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_2(mean: np.ndarray, covariance: np.ndarray,\n",
    "             num_particles: int) -> np.ndarray:\n",
    "    \"\"\"Sample from a multi-variate Gaussian distribution.\n",
    "\n",
    "    Args:\n",
    "      mean: a numpy array of shape (N, ).\n",
    "      covariance: a numpy array of shape (N, N).\n",
    "      num_particles: an integer.\n",
    "\n",
    "    Returns:\n",
    "      sample: a numpy vector with length num_particles.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9422b8",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f1832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random; import numpy.random; random.seed(0); numpy.random.seed(0);\n",
    "assert np.allclose(warmup_2(np.array([0.58345008, 0.05778984, 0.13608912], dtype=np.float64), np.array([[1.61405906, 0.93399482, 0.79165733],  [0.93399482, 1.65599579, 1.43616931],  [0.79165733, 1.43616931, 1.25177385]], dtype=np.float64), 5), np.array([[-0.8347847 , -2.30256418, -1.83805061],  [-0.13115218, -3.33917428, -2.91354857],  [-0.47476854, -1.05885092, -0.83435982],  [ 0.29510331, -0.55660856, -0.28675913],  [-0.06919861, -0.94320128, -0.69794801]], dtype=np.float64), atol=1e-6)\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e06c444",
   "metadata": {},
   "source": [
    "## Localization: Update by Motion Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b957824",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d456faa6",
   "metadata": {},
   "source": [
    "Particle Filter for Localization\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6591049",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    from functools import cached_property\n",
    "except ImportError:\n",
    "    # Import for Colab (Python==3.7)\n",
    "    from backports.cached_property import cached_property\n",
    "\n",
    "\n",
    "def multivariate_normal_logpdf(xs: np.ndarray, means: np.ndarray,\n",
    "                               covs: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Batch version of `scipy.stats.multivariate_normal.logpdf`.\n",
    "\n",
    "    Compute multivariate normal's log probability density function, with batch \n",
    "    inputs and distributions.\n",
    "    Modified from https://gregorygundersen.com/blog/2020/12/12/group-multivariate-normal-pdf/. \n",
    "\n",
    "    This function uses the following broadcasting logic:\n",
    "    - First, `means` and `covs` are broadcasted together, which determines `dist_batch`, \n",
    "        the number of individual multivariate normal distributions. \n",
    "    - Then, broadcast the rightmost dimensions of `xs` to `means`. The rest of the dimensions \n",
    "    of `xs` becomes the `input_batch` dimension.\n",
    "\n",
    "    Args:\n",
    "        xs: array of shape (...input_batch,  ...dist_batch, dist_dim)\n",
    "        means: array of shape (...dist_batch, dist_dim)\n",
    "        covs: array of shape (...dist_batch, dist_dim, dist_dim)\n",
    "\n",
    "    Returns:\n",
    "        array of shape (...input_batch, ...dist_batch, dist_dim)\n",
    "\n",
    "\n",
    "    Below we describe some uses cases, with different input shapes and the result shape.\n",
    "\n",
    "    1.  xs: (100, 2), means: (2,),  covs: (2, 2), result: (100, 2)\n",
    "        Explanation: \n",
    "            - dist_dim = 2 i.e., Gaussian is 2D\n",
    "            - `input_batch=(100,)` and `dist_batch=()`\n",
    "            - returns logpdf of 100 inputs, evaluated by the same mean and covariance.\n",
    "\n",
    "    2.  xs: (100, 2), means: (2,),  covs: (100, 2, 2), result: (100, 2)\n",
    "        Explanation: \n",
    "            - dist_dim = 2 i.e., Gaussian is 2D\n",
    "            - `input_batch=()` and `dist_batch=(100,)`\n",
    "            - returns logpdf of 100 inputs, evaluated by the same mean but with 100 different covariances.\n",
    "    \"\"\"\n",
    "    dist_dim = xs.shape[-1]\n",
    "    dist_batch_shape = np.broadcast_shapes(means.shape[:-1], covs.shape[:-2])\n",
    "    means = np.broadcast_to(means, dist_batch_shape + (dist_dim,))\n",
    "    covs = np.broadcast_to(covs, dist_batch_shape + (dist_dim, dist_dim))\n",
    "    xs = np.broadcast_to(xs, xs.shape[:-len(means.shape)] + means.shape)\n",
    "\n",
    "    vals, vecs = np.linalg.eigh(covs)\n",
    "    logdets = np.sum(np.log(vals), axis=-1)\n",
    "    valsinvs = 1. / vals\n",
    "    Us = vecs * np.sqrt(valsinvs)[..., np.newaxis, :]\n",
    "    devs = xs - means\n",
    "    devUs = np.einsum(\"...i,...ij->...j\", devs, Us)\n",
    "    mahas = np.sum(np.square(devUs), axis=-1)\n",
    "    log2pi = np.log(2 * np.pi)\n",
    "    out = -0.5 * (dist_dim * log2pi + mahas + logdets)\n",
    "    return out\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class LocalizationParticles:\n",
    "    \"\"\"A batch of particles.\"\"\"\n",
    "\n",
    "    x: np.ndarray  # shape (N, )\n",
    "    y: np.ndarray  # shape (N, )\n",
    "    theta: np.ndarray  # shape (N, )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert self.x.shape == self.y.shape == self.theta.shape\n",
    "        assert len(self.x.shape) == 1\n",
    "        # Always normalize the angles\n",
    "        object.__setattr__(self, \"theta\", normalize_angles(self.theta))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"The number of particles.\"\"\"\n",
    "        return self.x.shape[0]\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Localization(Inference):\n",
    "    \"\"\"Localization Inference by particle filtering.\"\"\"\n",
    "\n",
    "    # A 2x2 array for the motion covariance.\n",
    "    motion_noise_covariance: np.ndarray\n",
    "\n",
    "    # A 2x2 numpy array for the sensor noise for the\n",
    "    # measurements of range and bearing to the landmarks\n",
    "    sensor_noise_covariance: np.ndarray\n",
    "\n",
    "    # In localization, we assume that we have access to the set of landmarks.\n",
    "    landmarks: List[Landmark] = dataclasses.field(default_factory=list)\n",
    "\n",
    "    num_particles: int = 10\n",
    "\n",
    "    particles: LocalizationParticles = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.particles:\n",
    "            assert len(self.particles) == self.num_particles\n",
    "\n",
    "    @cached_property\n",
    "    def landmarks_id_map(self) -> Dict[str, Landmark]:\n",
    "        \"\"\"Map from unique landmark id to the Landmark instance.\"\"\"\n",
    "        return {l.id: l for l in self.landmarks}\n",
    "\n",
    "    def estimated_pose(self) -> Pose:\n",
    "        return Pose(np.mean(self.particles.x), np.mean(self.particles.y),\n",
    "                    circular_mean(self.particles.theta))\n",
    "\n",
    "    def estimated_landmarks(self) -> Sequence[Landmark]:\n",
    "        # we know the landmarks so no estimation required!\n",
    "        return self.landmarks\n",
    "\n",
    "    def init(self, init_state: Pose) -> None:\n",
    "        self.particles = LocalizationParticles(\n",
    "            np.full(self.num_particles, init_state.x, dtype=np.float64),\n",
    "            np.full(self.num_particles, init_state.y, dtype=np.float64),\n",
    "            np.full(self.num_particles, init_state.theta, dtype=np.float64),\n",
    "        )\n",
    "\n",
    "    def plot_state(self, ax: 'matplotlib.axes.Axes') -> None:\n",
    "        ax.quiver(\n",
    "            self.particles.x,\n",
    "            self.particles.y,\n",
    "            np.cos(self.particles.theta),\n",
    "            np.sin(self.particles.theta),\n",
    "            angles=\"xy\",\n",
    "            scale=100,\n",
    "            color=(1.0, 0, 0, 0.4),\n",
    "        )\n",
    "\n",
    "    # We incrementally implement methods below using the `implementation_for` helper\n",
    "    def motion_model(self, command: Command):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def compute_weights(self,\n",
    "                        measurements: Sequence[Measurement]) -> np.ndarray:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def update(self, command: Command, measurements: Sequence[Measurement]):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "def plot_particles(samples: LocalizationParticles, fov=((-6, -6), (20, 20))):\n",
    "    \"\"\"matplotlib helper function. It takes a list of samples as input and\n",
    "    scatter plot the samples.\n",
    "\n",
    "    Args:\n",
    "        particles: a set of particles for localization. This function only plots \n",
    "            the first two components: x and y.\n",
    "        fov: a rectangular field of view. The function plots within this \n",
    "            rectanglular fov.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.axis([fov[0][0], fov[1][0], fov[0][1], fov[1][1]])\n",
    "\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.plot(samples.x, samples.y, 'g+')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57f0bea",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Let us now start by building a motion model for a\n",
    "simple ground robot that moves in straight lines, and performs point turns.\n",
    "\n",
    "A common motion model for such a robot assumes that the robot has a pose given\n",
    "by $[x_t, y_t, \\theta_t]$ at time $t$. The robot takes a command $[\\Delta p_t,\n",
    "\\Delta \\theta_t]$ which modifies the robot's pose according to the following:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} x_{t+1} \\\\ y_{t+1} \\\\ \\theta_{t+1} \\end{bmatrix} =\n",
    "\\begin{bmatrix} x_t + \\cos(\\theta_t) (\\Delta p_t + \\omega_t) \\\\\n",
    "y_t + \\sin(\\theta_t)(\\Delta p_t + \\omega_t) \\\\\n",
    "\\theta_t + \\Delta \\theta_t + \\nu_t\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $[\\omega_t, \\nu_t]$ is Gaussian error injected at each time\n",
    "step, distributed as $\\omega_t \\sim \\mathcal{N}(0, \\sigma^2_p)$, and\n",
    "$\\nu_t \\sim \\mathcal{N}(0, \\sigma^2_\\theta)$. The units of $\\Delta  p_t$ is in\n",
    "metres and $\\Delta \\theta_t$ in radians.\n",
    "\n",
    "Let us assume that the robot's pose at each time step is a random variable,\n",
    "represented as a set of samples.\n",
    "\n",
    "Please implement a function that takes in a set of sampled poses\n",
    "drawn from prior pose random variable $\\mathbf{X}_t$\n",
    "and a control $[\\Delta p_t, \\Delta \\theta_t]$ and returns a set\n",
    "of samples drawn from the posterior $X_{t+1}$.\n",
    "\n",
    "**Note**\n",
    "In this project, we will use a helper decorator `implementation_for` to implement\n",
    "class methods outside of the class. \n",
    "The decorator `@implementation_for(Localization)` simply inserts the declared\n",
    "function as a method of the Localization class. \n",
    "Using this decorator, you copy only the implemented method into \n",
    "catsoop's solution box and avoid repeating other parts of the class definition.\n",
    "\n",
    "\n",
    "**Hint**: Use `np.random.multivariate_normal` to generate the noise.\n",
    "\n",
    "\n",
    "For reference, our solution is **10** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@implementation_for(Localization)\n",
    "def motion_model(self: Localization, command: Command) -> None:\n",
    "    \"\"\"A motion model that simulates a one-step movement of the robot, and\n",
    "    updates to the new samples.\n",
    "\n",
    "    Args:\n",
    "        command: a Command tuple containing fields delta_p(float), the distance of the\n",
    "            movement, and delta_theta(float), the rotation of the movement.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d006d75d",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085d384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def localization_motion_model_problem_test():\n",
    "    # A simulator with no noise\n",
    "    sim = Simulator(motion_noise_covariance=np.zeros((2, 2)),\n",
    "                    sensor_noise_covariance=np.zeros((2, 2)))\n",
    "    sim.init()\n",
    "    # Particle filter with implemented motion_model\n",
    "    num_particles = 10\n",
    "    pf = Localization(\n",
    "        motion_noise_covariance=[[.01, 0], [0, np.deg2rad(5)**2]],\n",
    "        sensor_noise_covariance=sim.sensor_noise_covariance,\n",
    "        landmarks=sim.landmarks,\n",
    "        num_particles=num_particles,\n",
    "    )\n",
    "    pf.init(sim.state)\n",
    "    command = Command(1, 0)\n",
    "    assert pf.motion_model(command) is None, \"update should be in-place\"\n",
    "    assert len(pf.particles) == num_particles\n",
    "    sim.simulate_motion(command)\n",
    "    pose = sim.state\n",
    "    est_pose = pf.estimated_pose()\n",
    "    assert np.allclose(\n",
    "        [pose.x, pose.y, pose.theta],\n",
    "        [est_pose.x, est_pose.y, est_pose.theta],\n",
    "        atol=0.1\n",
    "    ), \"Estimated pose should be quite close to simulation without noise\"\n",
    "\n",
    "localization_motion_model_problem_test()\n",
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed20013f",
   "metadata": {},
   "source": [
    "## Localization: Computing the Importance Weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c49953",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Let us now add in a measurement model. Let us assume that\n",
    "there are four landmarks located at $(x, y)$ locations (-1, -1), (11, -1), (11, 11) and\n",
    "(-1, 11). The robot is equipped with a sensor that can measure the range and\n",
    "relative bearing $[r, b]$ from the robot's location to each of these landmarks. If the\n",
    "robot's pose is $(x, y, \\theta)$ and the landmark position is $(l_x, l_y)$,\n",
    "then range is just the Euclidean distance $$r = \\sqrt{(l_x - x)^2 + (l_y\n",
    "-y)^2}$$, \n",
    "and the bearing is the angle to the landmark in the robot's body frame (hence\n",
    "relative bearing), $$b = \\texttt{atan2}(l_y - y, l_x - x) - \\theta.$$  Each of\n",
    "these measurements is corrupted with noise $[q_r, q_b]$ distributed according\n",
    "to $q_r \\sim \\mathcal{N}(0, \\sigma^2_r)$ and $q_b \\sim \\mathcal{N}(0,\n",
    "\\sigma^2_b)$.\n",
    "\n",
    "Please take a look at our implementation in `simulator.simulate_sensing` and\n",
    "understand how the values are simulated.\n",
    "\n",
    "Now the problem is that for a real robot, even if you know this is the model\n",
    "that generates the measurements, you can't know the noise that nature applied. (That would be omniscience!) \n",
    "What we can do is work out how likely each measurement is. More precisely, given a set of measurements, we can use\n",
    "this model to compute the importance weights of a set of samples of the robot\n",
    "pose. See [Lecture 13 Page 13](https://sicp-s4.mit.edu/_static/fall22/lectures/lec13/L13-Sampling.pdf#page=13)\n",
    "for details.\n",
    "\n",
    "In particular, to compute the importance weight of a particle:\n",
    "- Compute the _predicted measurements_ from the current state of the robot's pose and the landmark locations.\n",
    "- Compute the errors $\\mathit{err}$ between the predicted and true measurements.\n",
    "- Compute the likelihood of the $\\mathit{err}$ as the unnormalized importance weights. \n",
    "The likelihood of the error is the evaluation of the probability density function (PDF) of the two-dimensional Gaussian distribution with covariance `sensor_noise_covariance` at $\\mathit{err}$.\n",
    "- Finally, normalize the weights so that they sum to one. \n",
    "\n",
    "Please implement a function that takes a set of samples and measurements and\n",
    "returns the importance weights of the samples.\n",
    "\n",
    "There are a few technicalities during this process:\n",
    "- We highly recommend you normalize **all the rotational values** (in radians) to the interval $[-\\pi, \\pi]$. \n",
    "Notably, the rotational component of the $\\mathit{err}$, which we denote as $\\mathit{err}_\\theta$, is a rotational \n",
    "value in radians. Therefore, theoretically, the likelihood of $\\mathit{err}_\\theta$ should have been\n",
    "the evaluation of the PDF of a [wrapped Gaussian distribution](https://en.wikipedia.org/wiki/Wrapped_normal_distribution). \n",
    "In other words, to evaluate the likelihood of \n",
    "$\\mathit{err}_\\theta$, we should have taken the sum of the Gaussian PDFs at the points \n",
    "$\\mathit{err}_\\theta + 2k\\pi$ for all $k \\in \\mathbb{Z}$. \n",
    "Nonetheless, since our sensor noise is relatively small, we expect only the single point at $k=0$\n",
    "$\\mathit{err}_\\theta \\in [-\\pi, \\pi]$ to have a relatively large value, and all other\n",
    "points to have close-to-zero values. Therefore, we may use the Gaussian distribution as a \n",
    "good approximation to the true wrapped Gaussian distribution.\n",
    "- The likelihood for the particles can become really small. For numerical stability, \n",
    "you will want to compute the _log-likelihood_ instead of the likelihood. \n",
    "To that end, you may:\n",
    "    - Use the utility `multivariate_normal_logpdf` that we provided you to evaluate the log PDF. \n",
    "    - Take the sum (instead of multiplications) of the log-likelihoods to compute the unnormalized log-weight of each particle.\n",
    "    - Normalize the log-weights. You should use [`scipy.special.logsumexp`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.logsumexp.html) to evaluate \n",
    "the log-sum of the weights. In short, The `logsumexp` function calculates the following expression in a more \n",
    "numerically stable way:\n",
    "$$\n",
    "\\texttt{logsumexp}(x_1, ... x_n) = \\log \\left( \\sum_{i=1}^{n} \\exp(x_i) \\right).\n",
    "$$ See <a href=\"https://gregorygundersen.com/blog/2020/02/09/log-sum-exp/\" target=\"_blank\">this blog</a> for a more detailed description of this function.\n",
    "    - Use `numpy.exp` to undo the logarithm from the log-weights and return the weights.\n",
    "\n",
    "\n",
    "For reference, our solution is **29** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732c77df",
   "metadata": {},
   "outputs": [],
   "source": [
    "@implementation_for(Localization)\n",
    "def compute_weights(self: Localization,\n",
    "                    measurements: Sequence[Measurement]) -> np.ndarray:\n",
    "    \"\"\"Compute the importance weights of the samples, based on the new\n",
    "    measurement.\n",
    "\n",
    "    Args:\n",
    "        measurements: a sequence of measurements made by the robot.\n",
    "\n",
    "    Returns:\n",
    "        weights: a numpy array of importance weights, normalized to 1.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d89952",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15d79d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_problem2_test(compute_weights, infer,\n",
    "                           measurements: Sequence[Measurement],\n",
    "                           result_weights: np.ndarray):\n",
    "    weights = compute_weights(infer, measurements)\n",
    "    assert np.allclose(weights, result_weights, atol=1e-4)\n",
    "\n",
    "sampling_problem2_test(compute_weights, Localization(motion_noise_covariance=np.array([[0.01      , 0.        ],  [0.        , 0.00761544]], dtype=np.float64), sensor_noise_covariance=np.array([[1.        , 0.        ],  [0.        , 0.00761544]], dtype=np.float64), landmarks=(Landmark(id='landmark-0', x=-4, y=-4), Landmark(id='landmark-1', x=-4, y=0), Landmark(id='landmark-2', x=-4, y=4), Landmark(id='landmark-3', x=-4, y=8), Landmark(id='landmark-4', x=-4, y=12), Landmark(id='landmark-5', x=-4, y=16), Landmark(id='landmark-6', x=0, y=-4), Landmark(id='landmark-7', x=0, y=0), Landmark(id='landmark-8', x=0, y=4), Landmark(id='landmark-9', x=0, y=8), Landmark(id='landmark-10', x=0, y=12), Landmark(id='landmark-11', x=0, y=16), Landmark(id='landmark-12', x=4, y=-4), Landmark(id='landmark-13', x=4, y=0), Landmark(id='landmark-14', x=4, y=4), Landmark(id='landmark-15', x=4, y=8), Landmark(id='landmark-16', x=4, y=12), Landmark(id='landmark-17', x=4, y=16), Landmark(id='landmark-18', x=8, y=-4), Landmark(id='landmark-19', x=8, y=0), Landmark(id='landmark-20', x=8, y=4), Landmark(id='landmark-21', x=8, y=8), Landmark(id='landmark-22', x=8, y=12), Landmark(id='landmark-23', x=8, y=16), Landmark(id='landmark-24', x=12, y=-4), Landmark(id='landmark-25', x=12, y=0), Landmark(id='landmark-26', x=12, y=4), Landmark(id='landmark-27', x=12, y=8), Landmark(id='landmark-28', x=12, y=12), Landmark(id='landmark-29', x=12, y=16), Landmark(id='landmark-30', x=16, y=-4), Landmark(id='landmark-31', x=16, y=0), Landmark(id='landmark-32', x=16, y=4), Landmark(id='landmark-33', x=16, y=8), Landmark(id='landmark-34', x=16, y=12), Landmark(id='landmark-35', x=16, y=16)), num_particles=20, particles=LocalizationParticles(x=np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=np.float64), y=np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=np.float64), theta=np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=np.float64))), [Measurement(landmark_id='landmark-35', r=22.753147219062917, b=0.7738698392857971), Measurement(landmark_id='landmark-25', r=12.640422650443282, b=0.009154262150241616), Measurement(landmark_id='landmark-15', r=8.408602536838048, b=1.1387038391295914), Measurement(landmark_id='landmark-3', r=10.248271955129297, b=2.117092341243524), Measurement(landmark_id='landmark-32', r=15.78868726666365, b=0.1345498076509456), Measurement(landmark_id='landmark-12', r=5.033579786955029, b=-0.7817917913663452), Measurement(landmark_id='landmark-7', r=-2.3250307746388343, b=-0.01909317455769924), Measurement(landmark_id='landmark-20', r=7.698360962746094, b=0.399745227278584), Measurement(landmark_id='landmark-5', r=15.948163519613333, b=1.7881725941556874), Measurement(landmark_id='landmark-2', r=6.068484785866513, b=2.4471709441565186), Measurement(landmark_id='landmark-26', r=12.520575977729484, b=0.44099698774324514), Measurement(landmark_id='landmark-24', r=11.983915967186904, b=-0.291075514011446), Measurement(landmark_id='landmark-8', r=4.903470181651809, b=1.5790004474613593), Measurement(landmark_id='landmark-30', r=15.748923253116834, b=-0.32541437620132907), Measurement(landmark_id='landmark-11', r=15.542274174332661, b=1.5900119763018357), Measurement(landmark_id='landmark-4', r=11.639492457134782, b=1.8742928687102223), Measurement(landmark_id='landmark-34', r=19.84077499008552, b=0.6906987897814902), Measurement(landmark_id='landmark-9', r=8.21465912250634, b=1.6018084460172888), Measurement(landmark_id='landmark-27', r=13.76837649243762, b=0.5766916802305041), Measurement(landmark_id='landmark-14', r=6.44082971955371, b=0.9157246165771075), Measurement(landmark_id='landmark-23', r=16.629478287894198, b=1.2392634902611783), Measurement(landmark_id='landmark-21', r=12.659583922767066, b=0.8535804455254041), Measurement(landmark_id='landmark-13', r=4.264455630329303, b=-0.027394933553940907), Measurement(landmark_id='landmark-1', r=5.458020683536959, b=3.312657462648057), Measurement(landmark_id='landmark-6', r=5.801634869866125, b=-1.4560318732949584), Measurement(landmark_id='landmark-33', r=18.245924230657273, b=0.3582019162682134), Measurement(landmark_id='landmark-18', r=8.939817776879076, b=-0.40635936363131575), Measurement(landmark_id='landmark-0', r=4.368492785742826, b=-2.3217135857051487), Measurement(landmark_id='landmark-31', r=16.42986369482223, b=0.06074118633851044), Measurement(landmark_id='landmark-22', r=13.23808713509877, b=0.9250492804923485), Measurement(landmark_id='landmark-19', r=7.563564752856779, b=-0.10208447443373762), Measurement(landmark_id='landmark-10', r=13.739367877130134, b=1.5275199517585014), Measurement(landmark_id='landmark-16', r=12.97808027013372, b=1.2264810610293408), Measurement(landmark_id='landmark-28', r=18.554035627279266, b=0.9006213960947502), Measurement(landmark_id='landmark-29', r=20.633352622824916, b=0.7350027054140555), Measurement(landmark_id='landmark-17', r=16.54445147673053, b=1.3854805390653757)], np.array([0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05], dtype=np.float64))\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc383f9",
   "metadata": {},
   "source": [
    "## Localization: Particle Filter Update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288ba205",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "What we want you to do is to write a method for step 4: `update` that takes in a\n",
    "set of samples, the most recent command, and the most recent measurements, and then:\n",
    "  * Use the motion_model method to update the sample based on the command\n",
    "  * Use compute_weights to compute the sample weights from the\n",
    "measurement,\n",
    "  * Resample a new set of samples,\n",
    "  * Blur the samples by applying noise drawn from a zero-mean Gaussian with\n",
    "covariance equal to `np.diag([1e-3, 1e-3, np.deg2rad(1)**2])`. (You'll need to\n",
    "recall from lecture why we do this blurring --- if you have machine learning\n",
    "background, it's a form of regularization, but you don't need to know about\n",
    "regularization to understand why this is important.)\n",
    "\n",
    "\n",
    "For reference, our solution is **16** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cc05ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@implementation_for(Localization)\n",
    "def update(self: Localization, command: Command,\n",
    "           measurements: Sequence[Measurement]) -> None:\n",
    "    \"\"\"Update the samples, based on the command and the new measurement.\n",
    "\n",
    "    Args:\n",
    "        command: a Command tuple containing fields delta_p(float), the distance of the\n",
    "        movement, and delta_theta(float), the rotation of the movement.\n",
    "        measurement: a measurement vector. The measurement is computed by\n",
    "            `simulator.simulate_sensing` function.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42402f55",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fde058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_problem3_test(_, infer, command: Command,\n",
    "                           measurements: Sequence[Measurement], pose: Pose):\n",
    "    infer.update(command, measurements)\n",
    "    est_pose = infer.estimated_pose()\n",
    "    print(est_pose)\n",
    "    assert np.allclose(\n",
    "        [pose.x, pose.y, pose.theta],\n",
    "        [est_pose.x, est_pose.y, est_pose.theta],\n",
    "        atol=0.3), \"Estimated pose should be quite close to our estimation\"\n",
    "\n",
    "sampling_problem3_test(update, Localization(motion_noise_covariance=np.array([[0.0001, 0.    ],  [0.    , 0.0001]], dtype=np.float64), sensor_noise_covariance=np.array([[1.00000000e-06, 0.00000000e+00],  [0.00000000e+00, 9.27917724e-08]], dtype=np.float64), landmarks=(Landmark(id='landmark-0', x=-4, y=-4), Landmark(id='landmark-1', x=-4, y=0), Landmark(id='landmark-2', x=-4, y=4), Landmark(id='landmark-3', x=-4, y=8), Landmark(id='landmark-4', x=-4, y=12), Landmark(id='landmark-5', x=-4, y=16), Landmark(id='landmark-6', x=0, y=-4), Landmark(id='landmark-7', x=0, y=0), Landmark(id='landmark-8', x=0, y=4), Landmark(id='landmark-9', x=0, y=8), Landmark(id='landmark-10', x=0, y=12), Landmark(id='landmark-11', x=0, y=16), Landmark(id='landmark-12', x=4, y=-4), Landmark(id='landmark-13', x=4, y=0), Landmark(id='landmark-14', x=4, y=4), Landmark(id='landmark-15', x=4, y=8), Landmark(id='landmark-16', x=4, y=12), Landmark(id='landmark-17', x=4, y=16), Landmark(id='landmark-18', x=8, y=-4), Landmark(id='landmark-19', x=8, y=0), Landmark(id='landmark-20', x=8, y=4), Landmark(id='landmark-21', x=8, y=8), Landmark(id='landmark-22', x=8, y=12), Landmark(id='landmark-23', x=8, y=16), Landmark(id='landmark-24', x=12, y=-4), Landmark(id='landmark-25', x=12, y=0), Landmark(id='landmark-26', x=12, y=4), Landmark(id='landmark-27', x=12, y=8), Landmark(id='landmark-28', x=12, y=12), Landmark(id='landmark-29', x=12, y=16), Landmark(id='landmark-30', x=16, y=-4), Landmark(id='landmark-31', x=16, y=0), Landmark(id='landmark-32', x=16, y=4), Landmark(id='landmark-33', x=16, y=8), Landmark(id='landmark-34', x=16, y=12), Landmark(id='landmark-35', x=16, y=16)), num_particles=100, particles=LocalizationParticles(x=np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=np.float64), y=np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=np.float64), theta=np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=np.float64))), Command(delta_p=1, delta_theta=0.7853981633974483), [Measurement(landmark_id='landmark-4', r=12.649236370894611, b=1.8925066397489418), Measurement(landmark_id='landmark-16', r=12.649751063323961, b=1.249077726801277), Measurement(landmark_id='landmark-20', r=8.943736240625999, b=0.46375775715344025), Measurement(landmark_id='landmark-35', r=22.628720998014654, b=0.7856866607567657), Measurement(landmark_id='landmark-34', r=19.999296264764194, b=0.6431156393698316), Measurement(landmark_id='landmark-10', r=11.999376725537463, b=1.5708089154080955), Measurement(landmark_id='landmark-12', r=5.654529218717742, b=-0.7854648111495861), Measurement(landmark_id='landmark-7', r=-0.0012459109472530653, b=-0.00022306139218380394), Measurement(landmark_id='landmark-33', r=17.887999561015462, b=0.4635512584632948), Measurement(landmark_id='landmark-17', r=16.492834133007015, b=1.3261352314007253), Measurement(landmark_id='landmark-8', r=3.999871465337056, b=1.5712125753715283), Measurement(landmark_id='landmark-31', r=15.999334805326514, b=0.00010707609058078074), Measurement(landmark_id='landmark-1', r=4.000903470181652, b=3.141621291373365), Measurement(landmark_id='landmark-29', r=19.999256500750647, b=0.9270144443957444), Measurement(landmark_id='landmark-15', r=8.943814184173492, b=1.1072157930644515), Measurement(landmark_id='landmark-5', r=16.491412884287104, b=1.815711271397861), Measurement(landmark_id='landmark-18', r=8.944112684989244, b=-0.46348285801429606), Measurement(landmark_id='landmark-25', r=12.000214659122506, b=0.00010825271769035358), Measurement(landmark_id='landmark-23', r=17.8878899913889, b=1.1071092352234257), Measurement(landmark_id='landmark-21', r=11.314492474454822, b=0.7858530885395344), Measurement(landmark_id='landmark-11', r=15.998740934467897, b=1.5712574943489113), Measurement(landmark_id='landmark-32', r=16.493768377894426, b=0.24521666418979546), Measurement(landmark_id='landmark-28', r=16.970827204107472, b=0.7853025370396719), Measurement(landmark_id='landmark-30', r=16.49388052315418, b=-0.24438153429639173), Measurement(landmark_id='landmark-27', r=14.424006736725824, b=0.5884032070631326), Measurement(landmark_id='landmark-3', r=8.944629290409818, b=2.0340758608916567), Measurement(landmark_id='landmark-22', r=14.422200647722839, b=0.9829936969482067), Measurement(landmark_id='landmark-0', r=5.655565888028631, b=-2.3560741291298712), Measurement(landmark_id='landmark-14', r=5.6572841131872025, b=0.785610190136083), Measurement(landmark_id='landmark-24', r=12.647926522706761, b=-0.32195212052680294), Measurement(landmark_id='landmark-2', r=5.656417814245238, b=2.3558381481535373), Measurement(landmark_id='landmark-6', r=4.00173936787713, b=-1.5709473898414392), Measurement(landmark_id='landmark-9', r=8.00032896962946, b=1.5707175610932667), Measurement(landmark_id='landmark-19', r=8.001583472878803, b=0.00040220495707190147), Measurement(landmark_id='landmark-26', r=12.649743993296344, b=0.3210793269023251), Measurement(landmark_id='landmark-13', r=4.00005202897426, b=0.00020826272337815132)], Pose(x=0.9858299248258179, y=-0.00020686081759629056, theta=0.7682645354557489))\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceb0e8f",
   "metadata": {},
   "source": [
    "## SLAM: Update by Motion Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3e6143",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3c2973",
   "metadata": {},
   "source": [
    "SLAM Particle Filter\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eb734f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class SLAMParticles:\n",
    "    \"\"\"A batch of samples for SLAM.\"\"\"\n",
    "    x: np.ndarray  # shape: (num_particles, )\n",
    "    y: np.ndarray  # shape: (num_particles, )\n",
    "    theta: np.ndarray  # shape: (num_particles, )\n",
    "\n",
    "    # Mapping from landmark_id to the index of that landmark in `landmarks_loc`\n",
    "    landmarks_id_to_idx: Dict[str, int]\n",
    "\n",
    "    landmarks_loc: np.ndarray  # shape (num_particles, num_landmarks, 2)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert self.x.shape == self.y.shape == self.theta.shape\n",
    "        assert self.x.shape[0] == self.landmarks_loc.shape[0]\n",
    "        assert self.landmarks_loc.shape[1] == len(self.landmarks_id_to_idx)\n",
    "        object.__setattr__(self, \"theta\", normalize_angles(self.theta))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"The number of samples.\"\"\"\n",
    "        return self.x.shape[0]\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class SLAM(Inference):\n",
    "\n",
    "    # A 2x2 array for the motion covariance.\n",
    "    motion_noise_covariance: np.ndarray\n",
    "\n",
    "    # A 2x2 numpy array for the sensor noise for the\n",
    "    # measurements of range and bearing to the landmarks\n",
    "    sensor_noise_covariance: np.ndarray\n",
    "\n",
    "    # A 2x2 array for the covariance when initializing new landmarks\n",
    "    new_landmarks_init_covariance: np.ndarray = np.eye(2) * 10\n",
    "\n",
    "    num_particles: int = 10\n",
    "\n",
    "    particles: SLAMParticles = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.particles:\n",
    "            assert len(self.particles) == self.num_particles\n",
    "\n",
    "    def estimated_pose(self) -> Pose:\n",
    "        return Pose(np.mean(self.particles.x), np.mean(self.particles.y),\n",
    "                    circular_mean(self.particles.theta))\n",
    "\n",
    "    def estimated_landmarks(self) -> Sequence[Landmark]:\n",
    "        landmarks_loc = np.mean(self.particles.landmarks_loc, axis=0)\n",
    "        idx_to_id = {\n",
    "            idx: id for id, idx in self.particles.landmarks_id_to_idx.items()\n",
    "        }\n",
    "        return [\n",
    "            Landmark(idx_to_id[i], *loc) for i, loc in enumerate(landmarks_loc)\n",
    "        ]\n",
    "\n",
    "    def init(self, init_state: Pose) -> None:\n",
    "        self.particles = SLAMParticles(\n",
    "            np.full(self.num_particles, init_state.x, dtype=np.float64),\n",
    "            np.full(self.num_particles, init_state.y, dtype=np.float64),\n",
    "            np.full(self.num_particles, init_state.theta, dtype=np.float64),\n",
    "            {},\n",
    "            np.empty((self.num_particles, 0, 2), dtype=np.float64),\n",
    "        )\n",
    "\n",
    "    def get_measured_landmarks_idx(\n",
    "            self, measurements: Sequence[Measurement]) -> np.ndarray:\n",
    "        \"\"\"Given a sequence of measurements, find the indices of each of the l\n",
    "        andmark in `self.particles`.\n",
    "\n",
    "        Args:\n",
    "            measurements: a sequence of measures. This function assumes that all\n",
    "            landmarks in this input sequence of measurements are already tracked \n",
    "            (i.e., using `self.add_new_landmarks`).\n",
    "        \"\"\"\n",
    "        return np.array(\n",
    "            [\n",
    "                self.particles.landmarks_id_to_idx[m.landmark_id]\n",
    "                for m in measurements\n",
    "            ],\n",
    "            dtype=int,\n",
    "        )\n",
    "\n",
    "    def plot_state(self, ax: 'matplotlib.axes.Axes') -> None:\n",
    "        import matplotlib.pyplot as plt\n",
    "        cmap = plt.get_cmap('viridis')\n",
    "\n",
    "        ax.quiver(\n",
    "            self.particles.x,\n",
    "            self.particles.y,\n",
    "            np.cos(self.particles.theta),\n",
    "            np.sin(self.particles.theta),\n",
    "            angles=\"xy\",\n",
    "            scale=100,\n",
    "            color=(1.0, 0, 0, 0.1),\n",
    "        )\n",
    "\n",
    "        idx_to_id = {\n",
    "            idx: id for id, idx in self.particles.landmarks_id_to_idx.items()\n",
    "        }\n",
    "        landmark_idx_to_color_code = np.array([\n",
    "            float(hash(idx_to_id[idx]) % 256) / 256\n",
    "            for idx in range(self.particles.landmarks_loc.shape[1])\n",
    "        ])\n",
    "\n",
    "        ax.scatter(self.particles.landmarks_loc[..., 0].flatten(),\n",
    "                   self.particles.landmarks_loc[..., 1].flatten(),\n",
    "                   marker=\"x\",\n",
    "                   c=np.broadcast_to(\n",
    "                       landmark_idx_to_color_code,\n",
    "                       self.particles.landmarks_loc.shape[:-1]).flatten(),\n",
    "                   cmap=cmap)\n",
    "\n",
    "    # We will incrementally implement methods below using the `implementation_for`\n",
    "    # helper\n",
    "    def motion_model(self, command: Command) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def add_new_landmarks(self, measurements: Sequence[Measurement]) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def compute_weights(self,\n",
    "                        measurements: Sequence[Measurement]) -> np.ndarray:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def update(self, command: Command,\n",
    "               measurements: Sequence[Measurement]) -> None:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce40a56",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Please implement a function that takes in a control command $[\\Delta p_t, \\Delta \\theta_t]$ \n",
    "and updates the particles by drawing drawn from the posterior $X_{t+1}$ using the motion model. \n",
    "\n",
    "This function behaves the same as `Localization.motion_model` for the pose \n",
    "components of the particles, and simply keeps the landmarks components \n",
    "(`SLAMParticles.landmarks_id_to_idx` and `SLAMParticles.landmarks_loc`) \n",
    "of the particles the same.\n",
    "\n",
    "**Hint**\n",
    "\n",
    "Please take a look at the fields of the `SLAMParticles` class.\n",
    "Note that `landmarks_id_to_idx` stores all the landmarks the robot has encountered so far.\n",
    "It is a mapping from a landmark identifier (a string) to an index (an integer). \n",
    "You can use the integer index to index into the `landmarks_loc` array. \n",
    "In particular, `landmarks_loc` is an array of shape `(num_particles, len(landmarks_id_to_idx), 2)`.\n",
    "Therefore, to access the coordinate of the landmark with the identifier `\"Stata\"` contained\n",
    "in the $i$-th particle:\n",
    "```python\n",
    "# assert isinstance(particles, SLAMParticles)\n",
    "loc = particles.landmarks_loc[i, particles.landmarks_id_to_idx[\"Stata\"]]\n",
    "```\n",
    "\n",
    "\n",
    "For reference, our solution is **12** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f583db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@implementation_for(SLAM)\n",
    "def motion_model(self: SLAM, command: Command) -> None:\n",
    "    \"\"\"A motion model that simulates a one-step movement of the robot and updates the particles.\n",
    "\n",
    "    Args:\n",
    "        command: a Command tuple containing fields delta_p(float), the distance of the\n",
    "            movement, and delta_theta(float), the rotation of the movement.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aec68a9",
   "metadata": {},
   "source": [
    "## SLAM: Tracking New Landmarks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be109651",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Since our robot might have limited sensing range (see the parameter `Simulator.max_sensing_range`),\n",
    "the robot is not aware of all the possible landmarks out there in the world.\n",
    "Therefore, we have to match newly observed landmarks to ones we have seen before, \n",
    "and add landmarks that we are seeing for the first time to our map.  \n",
    "In some situations, the matching is difficult, but for our purposes, \n",
    "well assume that the landmarks are uniquely identifiable, so we know exactly \n",
    "which ones we are observing on each step.\n",
    "\n",
    "One way to handle missing data due to sensing range is to explicitly model the \n",
    "sensing range in our probabilistic model of the sensors. \n",
    "Howewer, this approach would induce a difficult inference problem.  \n",
    "Instead, we will take the approach of treating the missingness as uninformative.\n",
    "In particular, we will consider sensor data within-range as observed variable, \n",
    "and sensor data out-of-range as unobserved variables.\n",
    "When we see a new landmark for the first time, we will assume that our belief \n",
    "about the location of that landmark has a mean equal to that first observed location \n",
    "and a standard variance (new_landmarks_init_covariance).\n",
    "Note, though, that in practice it might be more robust to wait until we have gotten \n",
    "a few observations and use their mean as the mean of the distribution for initializing our particles.\n",
    "\n",
    "Please implement a function that takes a set of measurements and updates the \n",
    "particles to include potentially new landmarks that are not yet tracked by the \n",
    "particles. \n",
    "\n",
    "Below we give a description for this procedure:\n",
    "\n",
    "- Filter the input measurements to contain only measurements to the new (untracked) \n",
    "landmarks. \n",
    "- For each new landmark measurement and each particle:\n",
    "    - Find the predicted location using the robot's pose of that particle and the landmark's measurement.\n",
    "    - Add a Gaussian noise of covariance `self.new_landmarks_init_covariance` to this predicted location.\n",
    "    - This noisy location becomes the landmark's location in the current particle.\n",
    "- Update `self.particles.landmarks_id_to_idx` to maintain the invariance that it \n",
    "holds a mapping from each tracked unique landmark indentifier to the index of \n",
    "that landmark in `self.particles.landmarks_loc`. \n",
    "\n",
    "\n",
    "For reference, our solution is **31** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a35f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@implementation_for(SLAM)\n",
    "def add_new_landmarks(self: SLAM, measurements: Sequence[Measurement]) -> None:\n",
    "    \"\"\"Add new landmarks into all particles to track them.\n",
    "\n",
    "    Args:\n",
    "        measurements: the measurements. This function will add only the new\n",
    "        landmarks in these measurements (i.e., those that had not been added \n",
    "        before).\n",
    "    \"\"\"\n",
    "    new_measurements = [\n",
    "        m for m in measurements\n",
    "        if m.landmark_id not in self.particles.landmarks_id_to_idx\n",
    "    ]\n",
    "\n",
    "    # Insert new landmarks into the mapping `landmarks_id_to_idx`\n",
    "    new_landmarks_id_to_idx = dict(self.particles.landmarks_id_to_idx)\n",
    "    for m in new_measurements:\n",
    "        new_landmarks_id_to_idx[m.landmark_id] = len(\n",
    "            new_landmarks_id_to_idx)\n",
    "\n",
    "    # Find estimated landmark location\n",
    "    # based on the new measurements and the current pose in the particles\n",
    "    est_landmarks_loc = ...  # shape: (num_particles, |new_measurements|, 2)\n",
    "\n",
    "    # Sample new landmark locations around est_landmarks_loc with\n",
    "    # zero-mean Gaussian noise of `new_landmarks_init_covariance``\n",
    "    sampled_landmark_locs = ...  # shape: (num_particles, |new_measurements|, 2)\n",
    "\n",
    "    # Update particles\n",
    "    new_particles = dataclasses.replace(\n",
    "        self.particles,\n",
    "        landmarks_id_to_idx=new_landmarks_id_to_idx,\n",
    "        landmarks_loc=np.concatenate(\n",
    "            [self.particles.landmarks_loc, sampled_landmark_locs], axis=1),\n",
    "    )\n",
    "\n",
    "    raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    self.particles = new_particles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a27a0ef",
   "metadata": {},
   "source": [
    "## SLAM: Computing the Importance Weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fe1caa",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Please implement the `SLAM.compute_weights` function that takes a \n",
    "sequence of measurements and returns the importance weights of the particles.\n",
    "\n",
    "This function is similar to `Localization.compute_weights`, except that it takes \n",
    "into account the landmarks location tracked in the particles.\n",
    "\n",
    "\n",
    "For reference, our solution is **30** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92762616",
   "metadata": {},
   "outputs": [],
   "source": [
    "@implementation_for(SLAM)\n",
    "def compute_weights(self: SLAM,\n",
    "                    measurements: Sequence[Measurement]) -> np.ndarray:\n",
    "    \"\"\"Compute the importance weights of the particles, based on the measurement.\n",
    "\n",
    "    Args:\n",
    "        measurements: a sequence of measurements made by the robot.\n",
    "\n",
    "    Returns:\n",
    "        weights: a numpy array of importance weights, normalized to 1.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d0dd38",
   "metadata": {},
   "source": [
    "## SLAM: Particle Filter Update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfa1c7c",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Please implement the update step of SLAM.\n",
    "\n",
    "The procedure is similar to `Localization.update`, except that it does an additional `add_new_landmarks` \n",
    "step after forward updating with the `motion_model` and before `compute_weights`. \n",
    "\n",
    "\n",
    "For reference, our solution is **20** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a99f430",
   "metadata": {},
   "outputs": [],
   "source": [
    "@implementation_for(SLAM)\n",
    "def update(self: SLAM, command: Command,\n",
    "           measurements: Sequence[Measurement]) -> None:\n",
    "    \"\"\"Update the particles, based on the command and measurements.\n",
    "\n",
    "    Args:\n",
    "        command: a Command tuple containing fields delta_p(float), the distance of the\n",
    "        movement, and delta_theta(float), the rotation of the movement.\n",
    "        measurement: a measurement vector. The measurement is computed by\n",
    "            `simulator.simulate_sensing` function.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173429b6",
   "metadata": {},
   "source": [
    "## Einsum Warmup (Optional)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7afd6d",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "This question intends to get you familiar with using `numpy.einsum`.\n",
    "**This question is optional and you do not have to use `einsum` in your implementation.** \n",
    "However, using `einsum` can greatly simplify your implementation.\n",
    "\n",
    "In particular, the Rao-blackwellized particle filter performs quite a few \n",
    "matrix multiplications. \n",
    "The function `numpy.einsum` allows one to express batched multiplication of \n",
    "multiple matrices succinctly.\n",
    "\n",
    "If you are not familar with `np.einsum`, here are some guides online:\n",
    "- <a href=\"https://stackoverflow.com/questions/26089893/understanding-numpys-einsum\" target=\"_blank\">StackOverflow Post</a>\n",
    "- <a href=\"https://rockt.github.io/2018/04/30/einsum\" target=\"_blank\">Blog: einsum is all you need</a>\n",
    "\n",
    "For this warmup question, please fill in the indexing expression string for \n",
    "`einsum` to implement a 'sandwich matrix product'. \n",
    "A sandwich matrix product of two matrices $U$ and $S$ is\n",
    "$M = U \\cdot S \\cdot U^\\top$.\n",
    "\n",
    "\n",
    "For reference, our solution is **2** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf3e00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sandwich_matrix_product(U: np.ndarray, S: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Perform a batched 'sandwitch matrix product': $$M = U \\cdot S \\cdot U^\\top$$.\n",
    "    Uses `numpy.einsum`.\n",
    "\n",
    "    Args:\n",
    "        U, S: input matrices to perform the product, both of shape (batch, N, N)\n",
    "\n",
    "    Returns:\n",
    "        the matrices M, of shape (batch, N, N)\n",
    "    \"\"\"\n",
    "    return np.einsum(\"FILL IN THE STRING HERE\", U, S, U)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6de285",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a83118",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert np.allclose(sandwich_matrix_product(np.array([[[-0.91509102,  0.02896068,  0.09373636],   [-0.31979851, -0.86280633, -0.5421848 ],   [-0.28403213, -0.12971603,  0.18185345]],   [[ 0.44478304, -0.36473625, -0.34209248],   [-0.96061671, -0.91825028, -0.48435661],   [ 0.48049   ,  0.25662766,  0.53957804]],   [[ 0.53783887,  0.71313494,  0.44063853],   [ 0.95802184,  0.79765044,  0.17343433],   [ 0.17631534, -0.93146592,  0.99705316]],   [[-0.73684801,  0.48069439,  0.64203039],   [-0.25389094, -0.60629589, -0.80248023],   [ 0.49721201, -0.09469294,  0.42743552]],   [[ 0.8308153 , -0.70683253,  0.838342  ],   [-0.17674708, -0.38946598,  0.88612452],   [ 0.98130339, -0.60221556,  0.31367669]]], dtype=np.float64), np.array([[[-0.78700937,  0.30182801,  0.65462646],   [ 0.36899709, -0.16533372, -0.23386728],   [-0.21375517,  0.17942364,  0.76313454]],   [[ 0.85813231, -0.89294076, -0.63675521],   [-0.77555137, -0.61333072, -0.30678438],   [ 0.01306337,  0.25892245,  0.46428444]],   [[ 0.78022308,  0.97817687,  0.32571296],   [ 0.69072904,  0.55607769, -0.38493592],   [ 0.75138454, -0.91447372, -0.99926531]],   [[-0.45253474, -0.07580494,  0.27672579],   [-0.79645947,  0.34602027,  0.60363173],   [-0.62937416, -0.16974949,  0.03996998]],   [[-0.09638596,  0.59965986,  0.9210448 ],   [ 0.59790633, -0.84401436,  0.60987114],   [-0.86680734, -0.52805923, -0.69380621]]], dtype=np.float64)), np.array([[[-0.70821093,  0.29028311, -0.264794  ],   [-0.0860032 ,  0.25683661, -0.0839603 ],   [-0.12767688, -0.00731765, -0.0378081 ]],   [[ 0.5021111 , -0.23356351,  0.07177661],   [-0.75393822, -1.39959106,  0.63624027],   [ 0.30684435,  0.26353458, -0.08116496]],   [[ 0.80153175,  1.51206662, -0.80293068],   [ 1.56577159,  2.31436395, -1.05422099],   [-1.02144813, -0.87038954,  0.63539018]],   [[ 0.46041849, -0.04331244, -0.18835761],   [-1.13941324,  0.12874572,  0.35331832],   [ 0.30230967,  0.11473338, -0.15291035]],   [[-1.68979947, -0.25599802, -1.6767293 ],   [-1.49516034, -0.63011489, -1.13628625],   [-0.99438737,  0.04839135, -1.17364301]]], dtype=np.float64), atol=1e-6)\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d0e747",
   "metadata": {},
   "source": [
    "## RBSLAM: Update by Motion Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b3799f",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09283289",
   "metadata": {},
   "source": [
    "Rao-blackwellized Particle Filter for SLAM\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9683f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class RBSLAMParticles:\n",
    "    \"\"\"A batch of particles for Rao-blackwellized SLAM.\"\"\"\n",
    "    x: np.ndarray  # shape: (num_particles, )\n",
    "    y: np.ndarray  # shape: (num_particles, )\n",
    "    theta: np.ndarray  # shape: (num_particles, )\n",
    "\n",
    "    # Mapping from landmark_id to the index of that landmark in `landmarks_loc`\n",
    "    landmarks_id_to_idx: Dict[str, int]\n",
    "\n",
    "    landmarks_loc: np.ndarray  # shape: (num_particles, num_landmarks, 2)\n",
    "    landmarks_cov: np.ndarray  # shape: (num_particles, num_landmarks, 2, 2)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert self.x.shape == self.y.shape == self.theta.shape\n",
    "        assert (self.landmarks_loc.shape + (2,) == self.landmarks_cov.shape)\n",
    "        assert self.landmarks_loc.shape[1] == len(self.landmarks_id_to_idx)\n",
    "        assert self.landmarks_loc.shape[0] == self.x.shape[0]\n",
    "        object.__setattr__(self, \"theta\", normalize_angles(self.theta))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"The number of particles.\"\"\"\n",
    "        return self.x.shape[0]\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class RBSLAM(Inference):\n",
    "    \"\"\"Rao-blackwellized particle filter for SLAM.\"\"\"\n",
    "\n",
    "    # A 2x2 array for the motion covariance.\n",
    "    motion_noise_covariance: np.ndarray\n",
    "\n",
    "    # A 2x2 numpy array for the sensor noise for the\n",
    "    # measurements of range and bearing to the landmarks\n",
    "    sensor_noise_covariance: np.ndarray\n",
    "\n",
    "    # A 2x2 array for the covariance when initializing new landmarks\n",
    "    # By default, we simply use a large enough symmetric covariance\n",
    "    new_landmarks_init_covariance: np.ndarray = np.eye(2) * 10\n",
    "\n",
    "    num_particles: int = 10\n",
    "\n",
    "    particles: RBSLAMParticles = None\n",
    "\n",
    "    # To account for the approximation error of EKF when a landmark gets really\n",
    "    # close, we will ignore any measurement that is within min_sensing_range!\n",
    "    min_sensing_range: float = 1.\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.particles:\n",
    "            assert len(self.particles) == self.num_particles\n",
    "\n",
    "    def estimated_pose(self) -> Pose:\n",
    "        return Pose(np.mean(self.particles.x), np.mean(self.particles.y),\n",
    "                    circular_mean(self.particles.theta))\n",
    "\n",
    "    def estimated_landmarks(self) -> Sequence[Landmark]:\n",
    "        landmarks_loc = np.mean(self.particles.landmarks_loc, axis=0)\n",
    "        idx_to_id = {\n",
    "            idx: id for id, idx in self.particles.landmarks_id_to_idx.items()\n",
    "        }\n",
    "        return [\n",
    "            Landmark(idx_to_id[i], *loc) for i, loc in enumerate(landmarks_loc)\n",
    "        ]\n",
    "\n",
    "    def init(self, init_state: Pose) -> None:\n",
    "        self.particles = RBSLAMParticles(\n",
    "            np.full(self.num_particles, init_state.x, dtype=np.float64),\n",
    "            np.full(self.num_particles, init_state.y, dtype=np.float64),\n",
    "            np.full(self.num_particles, init_state.theta, dtype=np.float64),\n",
    "            {},\n",
    "            np.empty((self.num_particles, 0, 2), dtype=np.float64),\n",
    "            np.empty((self.num_particles, 0, 2, 2), dtype=np.float64),\n",
    "        )\n",
    "\n",
    "    def get_measured_landmarks_idx(\n",
    "            self, measurements: Sequence[Measurement]) -> np.ndarray:\n",
    "        \"\"\"Given a sequence of measurements, find the indices of each of the \n",
    "        landmark in `self.particles`.\n",
    "\n",
    "        Args:\n",
    "            measurements: a sequence of measures. This function assumes that all\n",
    "            landmarks in this input sequence of measurements are already tracked \n",
    "            (i.e., using `self.add_new_landmarks`).\n",
    "        \"\"\"\n",
    "        return np.array(\n",
    "            [\n",
    "                self.particles.landmarks_id_to_idx[m.landmark_id]\n",
    "                for m in measurements\n",
    "            ],\n",
    "            dtype=int,\n",
    "        )\n",
    "\n",
    "    def plot_state(self, ax: 'matplotlib.axes.Axes') -> None:\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        cmap = plt.get_cmap('viridis')\n",
    "\n",
    "        ax.quiver(\n",
    "            self.particles.x,\n",
    "            self.particles.y,\n",
    "            np.cos(self.particles.theta),\n",
    "            np.sin(self.particles.theta),\n",
    "            angles=\"xy\",\n",
    "            scale=100,\n",
    "            color=(1.0, 0, 0, 0.1),\n",
    "        )\n",
    "\n",
    "        idx_to_id = {\n",
    "            idx: id for id, idx in self.particles.landmarks_id_to_idx.items()\n",
    "        }\n",
    "        landmark_idx_to_color_code = np.array([\n",
    "            float(hash(idx_to_id[idx]) % 256) / 256\n",
    "            for idx in range(self.particles.landmarks_loc.shape[1])\n",
    "        ])\n",
    "\n",
    "        ax.scatter(self.particles.landmarks_loc[..., 0].flatten(),\n",
    "                   self.particles.landmarks_loc[..., 1].flatten(),\n",
    "                   marker=\"x\",\n",
    "                   c=np.broadcast_to(\n",
    "                       landmark_idx_to_color_code,\n",
    "                       self.particles.landmarks_loc.shape[:-1]).flatten(),\n",
    "                   cmap=cmap)\n",
    "\n",
    "        avg_landmarks_loc = np.mean(self.particles.landmarks_loc, axis=0)\n",
    "        avg_landmarks_cov = np.mean(self.particles.landmarks_cov, axis=0)\n",
    "        for i, (loc, cov) in enumerate(zip(avg_landmarks_loc,\n",
    "                                           avg_landmarks_cov)):\n",
    "            edgecolor = cmap(landmark_idx_to_color_code[i])\n",
    "            confidence_ellipse(loc, cov, ax, n_std=5, edgecolor=edgecolor)\n",
    "\n",
    "    # We will incrementally implement methods below using the `implementation_for`\n",
    "    # helper\n",
    "    def motion_model(self, command: Command) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def add_new_landmarks(self, measurements: Sequence[Measurement]) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def update_EKF(self, measurements: Sequence[Measurement]) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def compute_weights(self, measurements: Sequence[Measurement],\n",
    "                        sensor_noise_covariance: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def update(self, command: Command,\n",
    "               measurements: Sequence[Measurement]) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "def confidence_ellipse(loc, cov, ax, n_std=3.0, facecolor=\"none\", **kwargs):\n",
    "    \"\"\"Draws a confidence ellipse for a 2D Gaussian.\"\"\"\n",
    "\n",
    "    from matplotlib.patches import Ellipse\n",
    "    import matplotlib.transforms as transforms\n",
    "\n",
    "    pearson = cov[0, 1] / np.sqrt(cov[0, 0] * cov[1, 1])\n",
    "    # Using a special case to obtain the eigenvalues of this\n",
    "    # two-dimensionl dataset.\n",
    "    ell_radius_x = np.sqrt(1 + pearson)\n",
    "    ell_radius_y = np.sqrt(1 - pearson)\n",
    "    ellipse = Ellipse((0, 0),\n",
    "                      width=ell_radius_x * 2,\n",
    "                      height=ell_radius_y * 2,\n",
    "                      facecolor=facecolor,\n",
    "                      **kwargs)\n",
    "\n",
    "    # Calculating the stdandard deviation of x from\n",
    "    # the squareroot of the variance and multiplying\n",
    "    # with the given number of standard deviations.\n",
    "    scale_x = np.sqrt(cov[0, 0]) * n_std\n",
    "\n",
    "    # calculating the stdandard deviation of y\n",
    "    scale_y = np.sqrt(cov[1, 1]) * n_std\n",
    "\n",
    "    transf = transforms.Affine2D()\\\n",
    "        .rotate_deg(45)\\\n",
    "        .scale(scale_x,scale_y)\\\n",
    "        .translate(*loc)\n",
    "\n",
    "    ellipse.set_transform(transf + ax.transData)\n",
    "    return ax.add_patch(ellipse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bce153",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Please implement a function that takes in a control command $[\\Delta p_t, \\Delta \\theta_t]$ \n",
    "and updates the particles by drawing drawn from the posterior $X_{t+1}$ using the \n",
    "motion model. \n",
    "\n",
    "This function behaves the same as `Localization.motion_model` for the pose \n",
    "components of the particles, and simply keeps the landmarks components \n",
    "(`landmarks_id_to_idx`, `landmarks_loc` and `landmarks_cov`) of the \n",
    "particles the same.\n",
    "\n",
    "\n",
    "For reference, our solution is **9** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5c58dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@implementation_for(RBSLAM)\n",
    "def motion_model(self: RBSLAM, command: Command) -> None:\n",
    "    \"\"\"A motion model that simulates a one-step movement of the robot and updates the particles.\n",
    "\n",
    "    Args:\n",
    "        command: a Command tuple containing fields delta_p(float), the distance of the\n",
    "            movement, and delta_theta(float), the rotation of the movement.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e78dc06",
   "metadata": {},
   "source": [
    "## RBSLAM: Tracking New Landmarks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2d18bc",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Since our robot might have limited sensing range (see the parameter `Simulator.max_sensing_range`),\n",
    "the robot is not aware of all the possible landmarks out there in the world.\n",
    "Therefore, we must track new landmarks that appears in the measurements in our \n",
    "particles.\n",
    "\n",
    "Please implement `add_new_landmarks` that takes a sequence of measurements and updates \n",
    "the particles to include potentially new landmarks that are not yet tracked by the \n",
    "particles. \n",
    "\n",
    "When you encounter a new landmark, instead of generating a set of particles to represent a distribution (as in particle filter for SLAM section), \n",
    "you should store that landmark into each particle.\n",
    "For each particle, the new landmark location's distribution \n",
    "- is centered the observed location of the landmark (relative to the particle's robot pose).\n",
    "- has a fixed initial covariance of `self.new_landmarks_init_covariance`.\n",
    "\n",
    "\n",
    "For reference, our solution is **31** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b48e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@implementation_for(RBSLAM)\n",
    "def add_new_landmarks(self, measurements: Sequence[Measurement]) -> None:\n",
    "    \"\"\"Add new landmarks into all particles to track them.\n",
    "\n",
    "    Args:\n",
    "        measurements: the measurements. This function will add only the new\n",
    "        landmarks in these measurements (i.e., those that had not been added \n",
    "        before).\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700b5875",
   "metadata": {},
   "source": [
    "## RBSLAM: Extended Kalman Filter Update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f16412",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Please implement `update_EKF` that takes a sequence of measurements and performs\n",
    "an extended Kalman filter update on the landmarks of the particles.\n",
    "\n",
    "Suppose our robot's pose is $\\texttt{Pose}(x, y, \\theta)$.\n",
    "The state that we would like to track by our filter is the landmark location $[\\hat{l_x}, \\hat{l_y}]$.\n",
    "Recall that our observation model (without noise) is \n",
    "$$\n",
    "\\mathit{Obs}\\left(\\begin{bmatrix} l_x \\\\ l_y \\end{bmatrix}\\right) = \\begin{bmatrix} \n",
    "\\sqrt{(l_x - x)^2 + (l_y -y)^2} \\\\   \n",
    "\\texttt{atan2}(l_y - y, l_x - x) - \\theta\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "This model is non-linear, and the standard Kalman filter does not apply.\n",
    "This is where an extended Kalman filter could be useful.\n",
    "\n",
    "In our case, an EKF update is the same as a Kalman filter update.\n",
    "The idea of EKF is to approximate $\\mathit{Obs}$ by its Jacobian around the current belief of location of the landmark, $[\\hat{l_x}, \\hat{l_y}]$\n",
    ", so that we get a linear approximation:\n",
    "$$\n",
    "\\mathit{Obs}\\left(\\begin{bmatrix} l_x \\\\ l_y \\end{bmatrix}\\right) \\approx \n",
    "\\nabla\\mathit{Obs}\\left(\\begin{bmatrix} \\hat{l_x} \\\\ \\hat{l_y} \\end{bmatrix}\\right) \\cdot \\begin{bmatrix} l_x \\\\ l_y \\end{bmatrix}\n",
    "= \\begin{bmatrix} \n",
    "\\frac{\\hat{l_x} - x}{r} & \\frac{\\hat{l_y} - y}{r} \\\\   \n",
    "-\\frac{\\hat{l_y} - y}{r^2} & \\frac{\\hat{l_x} - x}{r^2}\n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix} l_x \\\\ l_y \\end{bmatrix} \\triangleq H \\cdot \\begin{bmatrix} l_x \\\\ l_y \\end{bmatrix} \n",
    "$$, where $r = \\sqrt{(\\hat{l_x} - x)^2 + (\\hat{l_y} - y)^2}$.\n",
    "Therefore, the implementation of an EKF for our application is almost the same as a KF, \n",
    "where we use the matrix $H$ above for the observation model. \n",
    "\n",
    "\n",
    "For reference, our solution is **36** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a36b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@implementation_for(RBSLAM)\n",
    "def update_EKF(self, measurements: Sequence[Measurement]) -> None:\n",
    "    \"\"\"Performs an extended Kalman filter update on the landmarks.\n",
    "\n",
    "    Args:\n",
    "        measurements: the sequence of landmark measurements.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcca9961",
   "metadata": {},
   "source": [
    "## RBSLAM: Computing the Importance Weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b534340a",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Please implement a function that takes a set of particles and measurements and\n",
    "returns the importance weights of the particles. \n",
    "\n",
    "\n",
    "For reference, our solution is **44** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5782d396",
   "metadata": {},
   "outputs": [],
   "source": [
    "@implementation_for(RBSLAM)\n",
    "def compute_weights(self: RBSLAM,\n",
    "                    measurements: Sequence[Measurement]) -> np.ndarray:\n",
    "    \"\"\"Compute the importance weights of the particles, based on the new\n",
    "    measurement.\n",
    "\n",
    "    Args:\n",
    "        measurements: a sequence of measurements made by the robot.\n",
    "\n",
    "    Returns:\n",
    "        weights: a numpy array of importance weights, normalized to 1.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28ce71d",
   "metadata": {},
   "source": [
    "## RBSLAM: Particle Filter Update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11b20d5",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Please implement the update step of RB SLAM.\n",
    "\n",
    "The procedure is similar to `SLAM.update`, except it does an additional `update_EKF` step after `add_new_landmarks` and before `compute_weights`. \n",
    "\n",
    "Further, since by using EKF we approximated the non-linear observation model by a linear model, the approximation can be quite bad\n",
    "when the robot is very close to a landmark. \n",
    "To fix, we will simply ignore any landmark measurement that is within `min_sensing_range`!\n",
    "Don't panic! If everything is implemented correctly, you will see that even when we ignore information of nearby landmarks, \n",
    "we still get a much better result than using a plain particle filter for SLAM. \n",
    "\n",
    "\n",
    "For reference, our solution is **27** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b77811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@implementation_for(RBSLAM)\n",
    "def update(self: RBSLAM, command: Command,\n",
    "           measurements: Sequence[Measurement]) -> None:\n",
    "    \"\"\"Update the samples, based on the command and the new measurement.\n",
    "\n",
    "    Args:\n",
    "        command: a Command tuple containing fields delta_p(float), the distance of the\n",
    "        movement, and delta_theta(float), the rotation of the movement.\n",
    "        measurement: a measurement vector. The measurement is computed by\n",
    "            `simulator.simulate_sensing` function.\n",
    "    \"\"\"\n",
    "    # Filter out any measurement that gets too close,\n",
    "    # since they mess up with our EKF updates\n",
    "    measurements = [m for m in measurements if m.r > self.min_sensing_range]\n",
    "\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mp02.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
