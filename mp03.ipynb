{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2e4ed16",
   "metadata": {},
   "source": [
    "# Miniproject 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21887d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install backports.cached_property\n",
    "\n",
    "# Setup matplotlib animation\n",
    "import matplotlib\n",
    "matplotlib.rc('animation', html='jshtml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f1a4b4",
   "metadata": {},
   "source": [
    "## Imports and Utilities\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3813db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import (Callable, Iterable, List, Sequence, Tuple, Dict, Optional,\n",
    "                    Any, Union, Set, ClassVar, Type, TypeVar)\n",
    "\n",
    "from abc import abstractmethod, ABC\n",
    "import collections\n",
    "import textwrap\n",
    "import math\n",
    "import functools\n",
    "import itertools\n",
    "import random\n",
    "import dataclasses\n",
    "import numpy as np\n",
    "import heapq as hq\n",
    "\n",
    "from functools import cached_property\n",
    "\n",
    "import scipy.signal\n",
    "\n",
    "\n",
    "def heatmap(data,\n",
    "            row_labels=None,\n",
    "            col_labels=None,\n",
    "            ax=None,\n",
    "            cbar_kw=None,\n",
    "            cbarlabel=\"\",\n",
    "            **kwargs):\n",
    "    \"\"\"\n",
    "    Create a heatmap from a numpy array and two lists of labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data\n",
    "        A 2D numpy array of shape (M, N).\n",
    "    row_labels\n",
    "        A list or array of length M with the labels for the rows.\n",
    "    col_labels\n",
    "        A list or array of length N with the labels for the columns.\n",
    "    ax\n",
    "        A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If\n",
    "        not provided, use current axes or create a new one.  Optional.\n",
    "    cbar_kw\n",
    "        A dictionary with arguments to `matplotlib.Figure.colorbar`.  Optional.\n",
    "    cbarlabel\n",
    "        The label for the colorbar.  Optional.\n",
    "    **kwargs\n",
    "        All other arguments are forwarded to `imshow`.\n",
    "    \"\"\"\n",
    "    import matplotlib.pylab as plt\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    # Plot the heatmap\n",
    "    im = ax.imshow(data, **kwargs)\n",
    "\n",
    "    # Create colorbar\n",
    "    if cbar_kw is not None:\n",
    "        cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n",
    "        cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n",
    "\n",
    "    # Show all ticks and label them with the respective list entries.\n",
    "    ax.set_xticks(np.arange(data.shape[1]), labels=col_labels)\n",
    "    ax.set_yticks(np.arange(data.shape[0]), labels=row_labels)\n",
    "\n",
    "    # Let the horizontal axes labeling appear on top.\n",
    "    ax.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(),\n",
    "             rotation=-30,\n",
    "             ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Turn spines off and create white grid.\n",
    "    ax.spines[:].set_visible(False)\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1] + 1) - .5, minor=True)\n",
    "    ax.set_yticks(np.arange(data.shape[0] + 1) - .5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"black\", linestyle='-', linewidth=1.5)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "    return im\n",
    "\n",
    "\n",
    "def annotate_heatmap(im,\n",
    "                     data=None,\n",
    "                     valfmt=\"{x:.2f}\",\n",
    "                     textcolors=(\"black\", \"white\"),\n",
    "                     threshold=None,\n",
    "                     **textkw):\n",
    "    \"\"\"\n",
    "    A function to annotate a heatmap.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    im\n",
    "        The AxesImage to be labeled.\n",
    "    data\n",
    "        Data used to annotate.  If None, the image's data is used.  Optional.\n",
    "    valfmt\n",
    "        The format of the annotations inside the heatmap.  This should either\n",
    "        use the string format method, e.g. \"$ {x:.2f}\", or be a\n",
    "        `matplotlib.ticker.Formatter`.  Optional.\n",
    "    textcolors\n",
    "        A pair of colors.  The first is used for values below a threshold,\n",
    "        the second for those above.  Optional.\n",
    "    threshold\n",
    "        Value in data units according to which the colors from textcolors are\n",
    "        applied.  If None (the default) uses the middle of the colormap as\n",
    "        separation.  Optional.\n",
    "    **kwargs\n",
    "        All other arguments are forwarded to each call to `text` used to create\n",
    "        the text labels.\n",
    "    \"\"\"\n",
    "\n",
    "    import matplotlib.ticker\n",
    "\n",
    "    if not isinstance(data, (list, np.ndarray)):\n",
    "        data = im.get_array()\n",
    "\n",
    "    # Normalize the threshold to the images color range.\n",
    "    if threshold is not None:\n",
    "        threshold = im.norm(threshold)\n",
    "    else:\n",
    "        threshold = im.norm(data.max()) / 2.\n",
    "\n",
    "    # Set default alignment to center, but allow it to be\n",
    "    # overwritten by textkw.\n",
    "    kw = dict(horizontalalignment=\"center\", verticalalignment=\"center\")\n",
    "    kw.update(textkw)\n",
    "\n",
    "    # Get the formatter in case a string is supplied\n",
    "    if isinstance(valfmt, str):\n",
    "        valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)\n",
    "\n",
    "    # Loop over the data and create a `Text` for each \"pixel\".\n",
    "    # Change the text's color depending on the data.\n",
    "    texts = []\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
    "            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)\n",
    "            texts.append(text)\n",
    "\n",
    "    return texts\n",
    "\n",
    "\n",
    "State = Any\n",
    "Observation = Any\n",
    "Action = Any\n",
    "\n",
    "\n",
    "class Problem(ABC):\n",
    "    \"\"\"The abstract base class for either a path cost problem or a reward problem.\"\"\"\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def initial(self) -> State:\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def actions(self, state: State) -> Iterable[Action]:\n",
    "        \"\"\"Returns the allowed actions in a given state.\n",
    "\n",
    "        The result would typically be a list. But if there are many\n",
    "        actions, consider yielding them one at a time in an iterator,\n",
    "        rather than building them all at once.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self, state: State, action: Action) -> State:\n",
    "        \"\"\"Returns the next state when executing a given action in a given\n",
    "        state.\n",
    "\n",
    "        The action must be one of self.actions(state).\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "\n",
    "class PathCostProblem(Problem):\n",
    "    \"\"\"An abstract class for a path cost problem, based on AIMA.\n",
    "\n",
    "    To formalize a path cost problem, you should subclass from this and\n",
    "    implement the abstract methods. Then you will create instances of\n",
    "    your subclass and solve them with the various search functions.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def goal_test(self, state: State) -> bool:\n",
    "        \"\"\"Checks if the state is a goal.\"\"\"\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def step_cost(self, state1: State, action: Action, state2: State) -> float:\n",
    "        \"\"\"Returns the cost incurred at state2 from state1 via action.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def h(self, state: State) -> float:\n",
    "        \"\"\"Returns the heuristic value, a lower bound on the distance to goal.\"\"\"\n",
    "        return 0\n",
    "\n",
    "\n",
    "class POMDP(Problem):\n",
    "    \"\"\"A generative-model-based POMDP.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def discount(self) -> float:\n",
    "        \"\"\"The discount factor.\"\"\"\n",
    "        return 1.\n",
    "\n",
    "    @property\n",
    "    def horizon(self) -> int:\n",
    "        \"\"\"The planning horizon.\"\"\"\n",
    "        return np.inf\n",
    "\n",
    "    @abstractmethod\n",
    "    def terminal(self, state: State) -> bool:\n",
    "        \"\"\"If this state is terminating (absorbing state).\"\"\"\n",
    "        return False\n",
    "\n",
    "    @abstractmethod\n",
    "    def reward(self, state1: State, action: Action, state2: State) -> float:\n",
    "        \"\"\"Returns the reward given at state2 from state1 via action.\"\"\"\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_observation(self, state: State) -> State:\n",
    "        \"\"\"Sample an observation from current state.\"\"\"\n",
    "        ...\n",
    "\n",
    "\n",
    "class MDP(POMDP):\n",
    "    \"\"\"An generative-model-based MDP.\"\"\"\n",
    "\n",
    "    def get_observation(self, state: State) -> State:\n",
    "        \"\"\"An MDP is fully observable.\"\"\"\n",
    "        return state\n",
    "\n",
    "\n",
    "class SearchFailed(ValueError):\n",
    "    \"\"\"Raise this exception whenever a search must fail.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# A useful data structure for best-first search\n",
    "BFSNode = collections.namedtuple(\"BFSNode\",\n",
    "                                 [\"state\", \"parent\", \"action\", \"cost\", \"g\"])\n",
    "\n",
    "\n",
    "def run_best_first_search(\n",
    "    problem: PathCostProblem,\n",
    "    get_priority: Callable[[BFSNode], float],\n",
    "    step_budget: int = 10000\n",
    ") -> Tuple[List[State], List[Action], List[float], int]:\n",
    "    \"\"\"A generic heuristic search implementation.\n",
    "\n",
    "    Depending on `get_priority`, can implement A*, GBFS, or UCS.\n",
    "\n",
    "    The `get_priority` function here should determine the order\n",
    "    in which nodes are expanded. For example, if you want to\n",
    "    use path cost as part of this determination, then the\n",
    "    path cost (node.g) should appear inside of get_priority,\n",
    "    rather than in this implementation of `run_best_first_search`.\n",
    "\n",
    "    Important: for determinism (and to make sure our tests pass),\n",
    "    please break ties using the state itself. For example,\n",
    "    if you would've otherwise sorted by `get_priority(node)`, you\n",
    "    should now sort by `(get_priority(node), node.state)`.\n",
    "\n",
    "    Args:\n",
    "      problem: a path cost problem.\n",
    "      get_priority: a callable taking in a search Node and returns the priority\n",
    "      step_budget: maximum number of `problem.step` before giving up.\n",
    "\n",
    "    Returns:\n",
    "      state_sequence: A list of states.\n",
    "      action_sequence: A list of actions.\n",
    "      cost_sequence: A list of costs.\n",
    "      num_steps: number of taken `problem.step`s. Must be less than or equal to `step_budget`.\n",
    "\n",
    "    Raises:\n",
    "      error: SearchFailed, if no plan is found.\n",
    "    \"\"\"\n",
    "    num_steps = 0\n",
    "    frontier = []\n",
    "    reached = {}\n",
    "\n",
    "    root_node = BFSNode(state=problem.initial,\n",
    "                        parent=None,\n",
    "                        action=None,\n",
    "                        cost=None,\n",
    "                        g=0)\n",
    "    hq.heappush(frontier, (get_priority(root_node), problem.initial, root_node))\n",
    "    reached[problem.initial] = root_node\n",
    "    num_expansions = 0\n",
    "\n",
    "    while frontier:\n",
    "        pri, s, node = hq.heappop(frontier)\n",
    "        # If reached the goal, return\n",
    "        if problem.goal_test(node.state):\n",
    "            return (*finish_plan(node), num_steps)\n",
    "\n",
    "        num_expansions += 1\n",
    "        # Generate successors\n",
    "        for action in problem.actions(node.state):\n",
    "            if num_steps >= step_budget:\n",
    "                raise SearchFailed(\n",
    "                    f\"Failed to find a plan in {step_budget} steps\")\n",
    "            child_state = problem.step(node.state, action)\n",
    "            num_steps += 1\n",
    "            cost = problem.step_cost(node.state, action, child_state)\n",
    "            path_cost = node.g + cost\n",
    "            # If the state is already in explored or reached, don't bother\n",
    "            if not child_state in reached or path_cost < reached[child_state].g:\n",
    "                # Add new node\n",
    "                child_node = BFSNode(state=child_state,\n",
    "                                     parent=node,\n",
    "                                     action=action,\n",
    "                                     cost=cost,\n",
    "                                     g=path_cost)\n",
    "                priority = get_priority(child_node)\n",
    "                hq.heappush(frontier, (priority, child_state, child_node))\n",
    "                reached[child_state] = child_node\n",
    "    raise SearchFailed(f\"Frontier exhausted after {num_steps} steps\")\n",
    "\n",
    "\n",
    "def finish_plan(node: BFSNode):\n",
    "    \"\"\"Helper for run_best_first_search.\"\"\"\n",
    "    state_sequence = []\n",
    "    action_sequence = []\n",
    "    cost_sequence = []\n",
    "\n",
    "    while node.parent is not None:\n",
    "        action_sequence.insert(0, node.action)\n",
    "        state_sequence.insert(0, node.state)\n",
    "        cost_sequence.insert(0, node.cost)\n",
    "        node = node.parent\n",
    "    state_sequence.insert(0, node.state)\n",
    "\n",
    "    return state_sequence, action_sequence, cost_sequence\n",
    "\n",
    "\n",
    "def run_astar_search(problem: PathCostProblem, step_budget: int = 10000):\n",
    "    \"\"\"A* search.\n",
    "\n",
    "    Use your implementation of `run_best_first_search`.\n",
    "    \"\"\"\n",
    "    get_priority = lambda node: node.g + problem.h(node.state)\n",
    "    return run_best_first_search(problem, get_priority, step_budget=step_budget)\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=False, eq=False)\n",
    "class MCTStateNode:\n",
    "    \"\"\"Node in the Monte Carlo search tree, keeps track of the children states.\"\"\"\n",
    "    # For MDP, this is a state; for POMDP, this is an observation\n",
    "    obs: Union[State, Observation]\n",
    "    N: int\n",
    "    horizon: int\n",
    "    parent: Optional['MCTChanceNode']\n",
    "    children: Dict['MCTChanceNode',\n",
    "                   Action] = dataclasses.field(default_factory=dict)\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=False, eq=False)\n",
    "class MCTChanceNode:\n",
    "    U: float\n",
    "    N: int\n",
    "    parent: MCTStateNode\n",
    "    children: Dict[State,\n",
    "                   MCTStateNode] = dataclasses.field(default_factory=dict)\n",
    "\n",
    "\n",
    "def ucb(n: MCTStateNode, C: float = 1.4) -> float:\n",
    "    \"\"\"UCB for a node, note the C argument\"\"\"\n",
    "    return (np.inf if n.N == 0 else\n",
    "            (n.U / n.N + C * np.sqrt(np.log(n.parent.N) / n.N)))\n",
    "\n",
    "\n",
    "RolloutPolicy = Callable[[State], Action]\n",
    "\n",
    "\n",
    "def random_rollout_policy(problem: MDP, state: State) -> Action:\n",
    "    return random.choice(list(problem.actions(state)))\n",
    "\n",
    "\n",
    "def run_mcts_search(problem: POMDP,\n",
    "                    state: Optional[State] = None,\n",
    "                    state_sampler: Iterable[State] = None,\n",
    "                    horizon: Optional[int] = None,\n",
    "                    C: float = np.sqrt(2),\n",
    "                    iteration_budget: int = 100,\n",
    "                    n_simulations: int = 10,\n",
    "                    max_backup: bool = True,\n",
    "                    rollout_policy: RolloutPolicy = None,\n",
    "                    verbose: bool = False) -> Action:\n",
    "    \"\"\"A generic MCTS search implementation for MDP and POMDPs.\n",
    "\n",
    "    For MDP, this is a standard MCTS implementation based on description in AIMA \n",
    "    (with some additional features). \n",
    "    For POMDP, this is an implementation of the POMCP algorithm.\n",
    "\n",
    "    Args:\n",
    "        problem: an MDP or POMDP.\n",
    "        state: the initial state. If None, then `state_sampler` must be provided.\n",
    "        state_sampler: an iterable of states. If None, then `state` must be provided. This is used to sample the initial state for POMCP.\n",
    "        horizon: the horizon of the search. If None, then the horizon is set to the problem's horizon.\n",
    "        C: the C parameter for UCB.\n",
    "        iteration_budget: the maximum number of iterations.\n",
    "        n_simulations: the number of simulations to run at each MCTS iteration. In AIMA, \n",
    "            this is set 1, but in general, we run multiple simulations to reduce variance.\n",
    "        max_backup: whether to use max backup or sum backup.\n",
    "        rollout_policy: the rollout policy. If None, then the random rollout policy is used.\n",
    "        verbose: whether to print debug information.\n",
    "\n",
    "    Returns:\n",
    "        action: the best action to take at the initial state according to the search.\n",
    "    \"\"\"\n",
    "    if state is None:\n",
    "        state = problem.initial\n",
    "\n",
    "    if state_sampler is None:\n",
    "        state_sampler = itertools.repeat(state)\n",
    "\n",
    "    if horizon is None:\n",
    "        horizon = problem.horizon\n",
    "\n",
    "    if rollout_policy is None:\n",
    "        rollout_policy = functools.partial(random_rollout_policy, problem)\n",
    "\n",
    "    ucb_fixed_C = functools.partial(ucb, C=C)\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    def select(n: MCTStateNode, state: State) -> Tuple[MCTStateNode, State]:\n",
    "        \"\"\"select a leaf node in the tree.\"\"\"\n",
    "        if n.children:\n",
    "            # Select the best child, break ties randomly\n",
    "            children = list(n.children.keys())\n",
    "            random.shuffle(children)\n",
    "            ucb_pick: MCTChanceNode = max(children, key=ucb_fixed_C)\n",
    "            act = n.children[ucb_pick]\n",
    "            next_state = problem.step(state, act)\n",
    "            rewards.append(problem.reward(state, act, next_state))\n",
    "            next_obs = problem.get_observation(\n",
    "                next_state)  # For MDP this is same as next_state\n",
    "            if next_obs not in ucb_pick.children:\n",
    "                new_leaf = MCTStateNode(next_obs,\n",
    "                                        horizon=n.horizon - 1,\n",
    "                                        parent=ucb_pick,\n",
    "                                        N=0)\n",
    "                ucb_pick.children[next_obs] = new_leaf\n",
    "                return new_leaf, next_state\n",
    "            return select(ucb_pick.children[next_obs], next_state)\n",
    "        return n, state\n",
    "\n",
    "    def expand(n: MCTStateNode, state: State) -> Tuple[MCTStateNode, State]:\n",
    "        \"\"\"expand the leaf node by adding all its children actions.\"\"\"\n",
    "        assert not n.children\n",
    "        if n.horizon == 0 or problem.terminal(state):\n",
    "            return n, state\n",
    "        for action in problem.actions(state):\n",
    "            new_chance_node = MCTChanceNode(parent=n, U=0, N=0)\n",
    "            n.children[new_chance_node] = action\n",
    "        chance_node, action = random.choice(list(n.children.items()))\n",
    "        next_state = problem.step(state, action)\n",
    "        rewards.append(problem.reward(state, action, next_state))\n",
    "        next_obs = problem.get_observation(\n",
    "            next_state)  # For MDP this is same as next_state\n",
    "        new_node = MCTStateNode(next_obs,\n",
    "                                N=0,\n",
    "                                horizon=n.horizon - 1,\n",
    "                                parent=chance_node)\n",
    "        chance_node.children[next_obs] = new_node\n",
    "        return new_node, next_state\n",
    "\n",
    "    def simulate(node: MCTStateNode, state: State) -> float:\n",
    "        \"\"\"simulate the utility of current state by taking a rollout policy.\"\"\"\n",
    "        total_reward = 0\n",
    "        disc = 1\n",
    "        h = node.horizon\n",
    "        while h > 0 and not problem.terminal(state):\n",
    "            action = rollout_policy(state)\n",
    "            next_state = problem.step(state, action)\n",
    "            reward = problem.reward(state, action, next_state)\n",
    "            total_reward += disc * reward\n",
    "            state = next_state\n",
    "            disc = disc * problem.discount\n",
    "            h -= 1\n",
    "        return total_reward\n",
    "\n",
    "    def backup(state_node: MCTStateNode, value: float) -> None:\n",
    "        \"\"\"passing the utility back to all parent nodes.\"\"\"\n",
    "        state_node.N += 1\n",
    "        if state_node.parent:\n",
    "            # Need to include the reward on the action *into* n\n",
    "            parent_chance_node = state_node.parent\n",
    "            parent_state_node = parent_chance_node.parent\n",
    "            r = rewards.pop()\n",
    "            future_val = r + problem.discount * value\n",
    "            parent_chance_node.U += future_val\n",
    "            parent_chance_node.N += 1\n",
    "            if max_backup:\n",
    "                bk_val = max(0 if n.N == 0 else n.U / n.N\n",
    "                             for n in parent_state_node.children)\n",
    "            else:\n",
    "                bk_val = future_val\n",
    "            backup(parent_state_node, bk_val)\n",
    "\n",
    "    state = next(state_sampler)\n",
    "    root = MCTStateNode(obs=problem.get_observation(state),\n",
    "                        horizon=horizon,\n",
    "                        parent=None,\n",
    "                        N=0)\n",
    "\n",
    "    i = 0\n",
    "    while i < iteration_budget:\n",
    "        state = next(state_sampler)\n",
    "        assert len(rewards) == 0\n",
    "        leaf, state = select(root, state)\n",
    "        child, state = expand(leaf, state)\n",
    "        value = np.mean([simulate(child, state) for _ in range(n_simulations)])\n",
    "        backup(child, value)\n",
    "        i += 1\n",
    "\n",
    "    children = list(root.children.keys())\n",
    "    random.shuffle(children)\n",
    "    act = root.children[max(children, key=lambda p: p.U / p.N)]\n",
    "    if verbose:\n",
    "        print(\n",
    "            {\n",
    "                act: (c.U / c.N if c.N > 0 else 0, c.N)\n",
    "                for c, act in root.children.items()\n",
    "            }, act)\n",
    "    return act\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True, eq=True, order=True)\n",
    "class PickupProblemState:\n",
    "    robot_loc: Tuple[int, int]\n",
    "    carried_patient: bool\n",
    "\n",
    "\n",
    "OneWayBlock = collections.namedtuple('OneWayBlock', ['loc', 'dest'])\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class PickupProblem(PathCostProblem):\n",
    "    \"\"\"A simple deterministic pickup problem in a grid.\n",
    "\n",
    "    The robot starts at some location and can move up, down, left, or right.\n",
    "    There is a patient at some location.\n",
    "    The goal is to pick up the patient and drop them off at the hospital.\n",
    "    The robot can only move to a location if there is a path from the robot's\n",
    "    current location to the destination that does not pass through any roadblocks.\n",
    "    \"\"\"\n",
    "    grid_shape: Tuple[int, int]\n",
    "\n",
    "    initial_robot_loc: Tuple[int, int] = (0, 0)\n",
    "\n",
    "    patient_loc: Tuple[int, int] = (4, 0)  # initial location of the patient\n",
    "    hospital_loc: Tuple[int, int] = (0, 3)\n",
    "\n",
    "    one_ways: List[OneWayBlock] = dataclasses.field(default_factory=list)\n",
    "\n",
    "    grid_act_to_delta: ClassVar = {\n",
    "        \"up\": (-1, 0),\n",
    "        \"down\": (1, 0),\n",
    "        \"left\": (0, -1),\n",
    "        \"right\": (0, 1)\n",
    "    }\n",
    "    all_grid_actions: ClassVar = tuple(grid_act_to_delta.keys())\n",
    "\n",
    "    @property\n",
    "    def initial(self) -> State:\n",
    "        return PickupProblemState(self.initial_robot_loc,\n",
    "                                  self.initial_robot_loc == self.patient_loc)\n",
    "\n",
    "    @cached_property\n",
    "    def _one_way_set(self) -> Set[OneWayBlock]:\n",
    "        \"\"\"Set of one-way blocks. Helps with fast lookup.\"\"\"\n",
    "        return set(self.one_ways)\n",
    "\n",
    "    def actions(self, state: PickupProblemState) -> Iterable[Action]:\n",
    "        \"\"\"Actions from the current state: move up, down, left, or right unless blocked.\"\"\"\n",
    "        (r, c) = state.robot_loc\n",
    "        actions = []\n",
    "        for act in self.all_grid_actions:\n",
    "            dr, dc = self.grid_act_to_delta[act]\n",
    "            new_r, new_c = r + dr, c + dc\n",
    "            if (new_r in range(self.grid_shape[0]) and\n",
    "                    new_c in range(self.grid_shape[1]) and\n",
    "                    OneWayBlock(state.robot_loc,\n",
    "                                (new_r, new_c)) not in self._one_way_set):\n",
    "                actions.append(act)\n",
    "        return actions\n",
    "\n",
    "    def step(self, state: PickupProblemState,\n",
    "             action: Action) -> PickupProblemState:\n",
    "        \"\"\"We automatically pick up patient if we're on that square.\"\"\"\n",
    "        (r, c) = state.robot_loc\n",
    "        dr, dc = self.grid_act_to_delta[action]\n",
    "        return PickupProblemState(\n",
    "            (r + dr, c + dc),\n",
    "            state.carried_patient or self.patient_loc == (r + dr, c + dc),\n",
    "        )\n",
    "\n",
    "    def step_cost(self, state1, action, state2) -> float:\n",
    "        \"\"\"Cost of taking an action in a state. \n",
    "\n",
    "        Actually not used in this project, but we keep it here for completeness.\n",
    "        \"\"\"\n",
    "        return 1.\n",
    "\n",
    "    def goal_test(self, state: PickupProblemState) -> bool:\n",
    "        \"\"\"True if at hospital and holding patient.\"\"\"\n",
    "        return state.robot_loc == self.hospital_loc and state.carried_patient\n",
    "\n",
    "    def render(self, state: PickupProblemState, ax=None):\n",
    "        \"\"\"Render the state as a grid.\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        if ax is None:\n",
    "            ax = plt.gca()\n",
    "\n",
    "        heatmap(np.zeros(self.grid_shape),\n",
    "                ax=ax,\n",
    "                cmap=\"YlOrRd\",\n",
    "                vmin=0,\n",
    "                vmax=1,\n",
    "                origin=\"upper\")\n",
    "\n",
    "        # Render the robot\n",
    "        robot = plt.Circle(state.robot_loc[::-1], 0.5, color='blue')\n",
    "        ax.add_patch(robot)\n",
    "\n",
    "        # Render the patient\n",
    "        patient_loc = (state.robot_loc\n",
    "                       if state.carried_patient else self.patient_loc)\n",
    "        patient = plt.Circle(patient_loc[::-1], 0.3, color='orange')\n",
    "        ax.add_patch(patient)\n",
    "\n",
    "        # Render the hospital\n",
    "        plt.plot([self.hospital_loc[1]], [self.hospital_loc[0]],\n",
    "                 marker='P',\n",
    "                 color='r',\n",
    "                 markersize=15)\n",
    "\n",
    "        # Render the walls and one-way doors\n",
    "        one_ways = set(self._one_way_set)\n",
    "        while one_ways:\n",
    "            one_way = one_ways.pop()\n",
    "            rev = OneWayBlock(one_way.dest, one_way.loc)\n",
    "            if rev in one_ways:\n",
    "                one_ways.remove(rev)\n",
    "                src, dst = one_way.loc, one_way.dest\n",
    "                if src < dst:\n",
    "                    dst, src = src, dst\n",
    "                src, dst = np.array(src), np.array(dst)\n",
    "                mid_pt = (src + dst) / 2\n",
    "                delta = dst - src\n",
    "                wall = plt.Rectangle(mid_pt[::-1] - delta / 2 +\n",
    "                                     delta[::-1] * 0.05,\n",
    "                                     delta[0] if delta[0] != 0 else 0.1,\n",
    "                                     delta[1] if delta[1] != 0 else 0.1,\n",
    "                                     color='black')\n",
    "                ax.add_patch(wall)\n",
    "            else:\n",
    "                src, dst = np.array(one_way.loc), np.array(one_way.dest)\n",
    "                mid_pt = (src + dst) / 2\n",
    "                delta = dst - src\n",
    "                base = mid_pt + delta * 0.07\n",
    "                length = -delta * 0.2\n",
    "                arrow = plt.arrow(base[1],\n",
    "                                  base[0],\n",
    "                                  length[1],\n",
    "                                  length[0],\n",
    "                                  width=0.2,\n",
    "                                  length_includes_head=True,\n",
    "                                  head_length=0.2,\n",
    "                                  color='black')\n",
    "                ax.add_patch(arrow)\n",
    "\n",
    "\n",
    "conv2D = functools.partial(scipy.signal.convolve2d,\n",
    "                           mode='same',\n",
    "                           boundary='fill',\n",
    "                           fillvalue=0)\n",
    "\n",
    "FireGridT = np.ndarray  # boolean array\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class FireProcess:\n",
    "    \"\"\"A probabilistic model for the evolution of fire in a grid.\n",
    "\n",
    "    At time step $t$, the probability of a cell being on fire is weighted probability \n",
    "    of the neighboring cell being on fire at time step $t-1$.\n",
    "    \"\"\"\n",
    "\n",
    "    initial_fire_grid: np.ndarray\n",
    "\n",
    "    fire_weights: np.ndarray = np.array([[0, 1, 0], [1, 4, 1], [0, 1, 0]])\n",
    "    attenuation: float = 1.0\n",
    "\n",
    "    rng: np.random.Generator = dataclasses.field(\n",
    "        default_factory=np.random.default_rng)\n",
    "\n",
    "    @cached_property\n",
    "    def normalized_fire_weights(self) -> np.ndarray:\n",
    "        return self.attenuation * self.fire_weights / np.sum(self.fire_weights)\n",
    "\n",
    "    def dist(self, fire_grid: FireGridT) -> np.ndarray:\n",
    "        \"\"\"Given the fire grid at time t, return a new grid with marginal distributions of fire for t + 1.\"\"\"\n",
    "        next_fire_dist = conv2D(fire_grid, self.normalized_fire_weights)\n",
    "        return np.clip(next_fire_dist, 0, 1)  # clip for numerical stability\n",
    "\n",
    "    def sample(self, fire_grid: FireGridT) -> FireGridT:\n",
    "        \"\"\"Given the fire grid at time t, return a new grid that's a sample from the distribution.\"\"\"\n",
    "        return self.rng.binomial(1, self.dist(fire_grid)).astype(bool)\n",
    "\n",
    "    def render(self, fire_grid: FireGridT, ax=None):\n",
    "        heatmap(fire_grid, ax=ax, cmap=\"YlOrRd\", vmin=0, vmax=1, origin=\"upper\")\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True, eq=True)\n",
    "class FireMDPState(PickupProblemState):\n",
    "    \"\"\"A state in the fire MDP extends the pickup problem state with a fire grid.\"\"\"\n",
    "\n",
    "    fire_grid: np.ndarray\n",
    "\n",
    "    # Below we implement __eq__ and __hash__ to make FireMDPState hashable.\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, FireMDPState):\n",
    "            return False\n",
    "        return (self.robot_loc == other.robot_loc and\n",
    "                self.carried_patient == other.carried_patient and\n",
    "                np.all(self.fire_grid == other.fire_grid))\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((super().__hash__(), self.fire_grid.tobytes()))\n",
    "\n",
    "\n",
    "T = TypeVar('T', bound='FireProblemCommon')\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class FireProblemCommon:\n",
    "    \"\"\"Common code for the fire MDP and POMDP problems.\"\"\"\n",
    "\n",
    "    pickup_problem: PickupProblem\n",
    "    fire_process: FireProcess\n",
    "\n",
    "    _horizon: int = np.inf\n",
    "    _discount: float = 0.999\n",
    "\n",
    "    step_reward: float = 0.\n",
    "\n",
    "    burn_reward: float = -1\n",
    "    goal_reward: float = 1.\n",
    "\n",
    "    @property\n",
    "    def horizon(self):\n",
    "        return self._horizon\n",
    "\n",
    "    @property\n",
    "    def discount(self):\n",
    "        return self._discount\n",
    "\n",
    "    @property\n",
    "    def initial_robot_loc(self):\n",
    "        return self.pickup_problem.initial_robot_loc\n",
    "\n",
    "    def robot_burned(self, state) -> bool:\n",
    "        return bool(state.fire_grid[state.robot_loc])\n",
    "\n",
    "    def succeeded(self, state) -> bool:\n",
    "        return self.pickup_problem.goal_test(state)\n",
    "\n",
    "    def reward(self, state1, action, state2) -> float:\n",
    "        if self.robot_burned(state2):\n",
    "            return self.burn_reward\n",
    "        if self.succeeded(state2):\n",
    "            return self.goal_reward\n",
    "        return self.step_reward\n",
    "\n",
    "    def terminal(self, state) -> bool:\n",
    "        return (self.robot_burned(state) or self.succeeded(state))\n",
    "\n",
    "    @property\n",
    "    def grid_shape(self):\n",
    "        return self.pickup_problem.grid_shape\n",
    "\n",
    "    @classmethod\n",
    "    def from_str(cls: Type[T],\n",
    "                 env_s: str,\n",
    "                 fire_process_kargs: Optional[Dict[str, Any]] = None,\n",
    "                 **kwargs) -> T:\n",
    "        \"\"\"Create a problem from a grid string.\n",
    "\n",
    "        Legend:\n",
    "            . = empty\n",
    "            F = fire\n",
    "            < = one way block (can go left)\n",
    "            > = one way block (can go right)\n",
    "            ^ = one way block (can go up)\n",
    "            v = one way block (can go down)\n",
    "            X = wall (two way block)\n",
    "            R = robot\n",
    "            P = patient\n",
    "            H = hospital\n",
    "        Each line must be the same length, and starts and ends with a `|`.\n",
    "        Each character is separated by a space.\n",
    "\n",
    "        Warning:\n",
    "            The user needs to make ssure that no cell location is a dead end (due to roadblocks)\n",
    "            since our `terminal` condition does not check for empty available actions.\n",
    "        \"\"\"\n",
    "        if fire_process_kargs is None:\n",
    "            fire_process_kargs = {}\n",
    "\n",
    "        lines = env_s.splitlines()\n",
    "        assert all(len(line) == len(lines[0]) for line in lines)\n",
    "        assert all(line[0] == '|' and line[-1] == '|' for line in lines)\n",
    "        # remove the first and last character of each line\n",
    "        lines = [line[1:-1] for line in lines]\n",
    "        # split each line into a list of characters\n",
    "        lines = [line[::2] for line in lines]\n",
    "        w = len(lines[0]) // 2 + 1\n",
    "        h = len(lines) // 2 + 1\n",
    "        robot_loc = None\n",
    "        patient_loc = None\n",
    "        hospital_loc = None\n",
    "        fire_grid = np.zeros((h, w), dtype=bool)\n",
    "        one_ways = []\n",
    "        for i, line in enumerate(lines):\n",
    "            i2 = i // 2\n",
    "            if i % 2 == 0:\n",
    "                for j, c in enumerate(line):\n",
    "                    j2 = j // 2\n",
    "                    if j % 2 == 0:\n",
    "                        if c == 'F':\n",
    "                            fire_grid[i2, j2] = True\n",
    "                        elif c == 'R':\n",
    "                            robot_loc = (i2, j2)\n",
    "                        elif c == 'P':\n",
    "                            patient_loc = (i2, j2)\n",
    "                        elif c == 'H':\n",
    "                            hospital_loc = (i2, j2)\n",
    "                        else:\n",
    "                            assert c == '.'\n",
    "                    else:\n",
    "                        if c in '<X':\n",
    "                            one_ways.append(OneWayBlock((i2, j2), (i2, j2 + 1)))\n",
    "                        if c in '>X':\n",
    "                            one_ways.append(OneWayBlock((i2, j2 + 1), (i2, j2)))\n",
    "            if i % 2 == 1:\n",
    "                for j, c in enumerate(line[::2]):\n",
    "                    if c in 'vX':\n",
    "                        one_ways.append(\n",
    "                            OneWayBlock(loc=(i2 + 1, j), dest=(i2, j)))\n",
    "                    if c in '^X':\n",
    "                        one_ways.append(\n",
    "                            OneWayBlock(loc=(i2, j), dest=(i2 + 1, j)))\n",
    "\n",
    "        if robot_loc is None:\n",
    "            raise ValueError(\"No robot location specified\")\n",
    "        if patient_loc is None:\n",
    "            raise ValueError(\"No patient location specified\")\n",
    "        if hospital_loc is None:\n",
    "            raise ValueError(\"No hospital location specified\")\n",
    "\n",
    "        pickup_problem = PickupProblem(fire_grid.shape, robot_loc, patient_loc,\n",
    "                                       hospital_loc, one_ways)\n",
    "        fire_process = FireProcess(fire_grid, **fire_process_kargs)\n",
    "        return cls(pickup_problem, fire_process, **kwargs)\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class FireMDP(FireProblemCommon, MDP):\n",
    "    \"\"\"The completely observable fire problem.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def initial(self) -> FireMDPState:\n",
    "        return FireMDPState(*dataclasses.astuple(self.pickup_problem.initial),\n",
    "                            self.fire_process.initial_fire_grid)\n",
    "\n",
    "    def actions(self, state: State) -> Iterable[Action]:\n",
    "        return self.pickup_problem.actions(state)\n",
    "\n",
    "    def step(self, state: FireMDPState, action: Action) -> FireMDPState:\n",
    "        return FireMDPState(\n",
    "            *dataclasses.astuple(self.pickup_problem.step(state, action)),\n",
    "            self.fire_process.sample(state.fire_grid))\n",
    "\n",
    "    def render(self, state: FireMDPState, ax=None):\n",
    "        self.pickup_problem.render(state, ax=ax)\n",
    "        self.fire_process.render(state.fire_grid, ax=ax)\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True, eq=True)\n",
    "class FirePOMDPState(FireMDPState):\n",
    "    drone_loc: Tuple[int, int]\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, FireMDPState):\n",
    "            return False\n",
    "        return (self.robot_loc == other.robot_loc and\n",
    "                self.carried_patient == other.carried_patient and\n",
    "                np.array_equal(self.fire_grid, other.fire_grid))\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((super().__hash__(), self.drone_loc))\n",
    "\n",
    "\n",
    "class FirePOMDPObservation(np.ndarray):\n",
    "    \"\"\"Observation of the fire grid at the drone's location. \n",
    "\n",
    "    For each entry in the fire grid:\n",
    "        0 = no fire\n",
    "        1 = fire\n",
    "        np.nan = not observed\n",
    "    \"\"\"\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.tobytes())\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return np.array_equal(self, other)\n",
    "\n",
    "\n",
    "def mask_centered_at(shape: Tuple[int, int], loc: Tuple[int, int],\n",
    "                     dist: int) -> np.ndarray:\n",
    "    \"\"\"Return a mask array centered at loc and extending dist away to each direction.\"\"\"\n",
    "    mask = np.zeros(shape, dtype=bool)\n",
    "    mask[max(0, loc[0] - dist):min(shape[0], loc[0] + dist + 1),\n",
    "         max(0, loc[1] - dist):min(shape[1], loc[1] + dist + 1)] = True\n",
    "    return mask\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class FirePOMDP(FireProblemCommon, POMDP):\n",
    "    \"\"\"The whole transition distribution is too much to deal with, so not defining step_dist method\"\"\"\n",
    "\n",
    "    # robot can see this many cells away\n",
    "    # 0 means the robot can only see the cell it is in\n",
    "    robot_view_distance: int = 0\n",
    "\n",
    "    # drone can see this many cells away\n",
    "    # 0 means the drone can only see the cell it is in\n",
    "    drone_view_distance: int = 0\n",
    "\n",
    "    # drone can fly this many cells away\n",
    "    drone_fly_distance: float = np.inf\n",
    "\n",
    "    @property\n",
    "    def initial_drone_loc(self):\n",
    "        \"\"\"Initially the drone starts off at the same location as the robot.\"\"\"\n",
    "        return self.initial_robot_loc\n",
    "\n",
    "    @property\n",
    "    def initial(self) -> FirePOMDPState:\n",
    "        return FirePOMDPState(*dataclasses.astuple(self.pickup_problem.initial),\n",
    "                              self.fire_process.initial_fire_grid,\n",
    "                              drone_loc=self.initial_drone_loc)\n",
    "\n",
    "    def drone_actions(self, robot_loc: Tuple[int, int]) -> List[Action]:\n",
    "        \"\"\"A drone can peek at any square within the flying distance.\"\"\"\n",
    "        # return state.robot_loc\n",
    "        grid_coords = itertools.product(range(self.grid_shape[0]),\n",
    "                                        range(self.grid_shape[1]))\n",
    "        return [(r, c)\n",
    "                for r, c in grid_coords\n",
    "                if (math.hypot(robot_loc[0] - r, robot_loc[1] -\n",
    "                               c) <= self.drone_fly_distance)]\n",
    "\n",
    "    def actions(self, state: FirePOMDPState) -> Iterable[Action]:\n",
    "        \"\"\"The robot can move to a neighboring square and the drone can move \n",
    "        to any square in the grid.\"\"\"\n",
    "        for robot_act in self.pickup_problem.actions(state):\n",
    "            robot_loc = self.pickup_problem.step(state, robot_act).robot_loc\n",
    "            for drone_act in self.drone_actions(robot_loc):\n",
    "                yield (robot_act, drone_act)\n",
    "\n",
    "    def step(self, state: FirePOMDPState, action: Action) -> FirePOMDPState:\n",
    "        robot_action, drone_action = action\n",
    "        return FirePOMDPState(\n",
    "            *dataclasses.astuple(self.pickup_problem.step(state, robot_action)),\n",
    "            self.fire_process.sample(state.fire_grid), drone_action)\n",
    "\n",
    "    def get_observation(self, state: FirePOMDPState) -> FirePOMDPObservation:\n",
    "        # Create a mask around the self.robot_view_distance of the robot\n",
    "        robot_mask = mask_centered_at(self.grid_shape, state.robot_loc,\n",
    "                                      self.robot_view_distance)\n",
    "        # Create a mask around the self.drone_view_distance of the drone\n",
    "        drone_mask = mask_centered_at(self.grid_shape, state.drone_loc,\n",
    "                                      self.drone_view_distance)\n",
    "        # Combine the masks\n",
    "        mask = np.logical_or(robot_mask, drone_mask)\n",
    "        # Return the masked fire grid\n",
    "        return np.where(mask, state.fire_grid,\n",
    "                        np.nan).view(FirePOMDPObservation)\n",
    "\n",
    "    def render(self, state: FireMDPState, ax=None):\n",
    "        \"\"\"Render the fire grid and the robot and drone locations, \n",
    "        then highlight the observed region.\"\"\"\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "        if ax is None:\n",
    "            ax = plt.gca()\n",
    "\n",
    "        self.pickup_problem.render(state, ax=ax)\n",
    "        self.fire_process.render(state.fire_grid, ax=ax)\n",
    "\n",
    "        overlay = np.isnan(self.get_observation(state)).astype(float)\n",
    "        # Draw gray overlay for the observed location\n",
    "        ax.imshow(overlay,\n",
    "                  alpha=overlay * 0.5,\n",
    "                  cmap=\"binary\",\n",
    "                  vmin=0,\n",
    "                  vmax=1,\n",
    "                  interpolation='nearest')\n",
    "        # Draw a blue rectangle around the drone location to highlight it\n",
    "        drone_rect = plt.Rectangle(\n",
    "            (state.drone_loc[1] - 0.5, state.drone_loc[0] - 0.5),\n",
    "            1,\n",
    "            1,\n",
    "            fill=False,\n",
    "            edgecolor='blue',\n",
    "            lw=4)\n",
    "        ax.add_patch(drone_rect)\n",
    "\n",
    "\n",
    "def get_problem(name: str, kind: str = \"mdp\") -> Union[POMDP, MDP]:\n",
    "    \"\"\"Return a problem instance by name and kind.\"\"\"\n",
    "\n",
    "    params = {\n",
    "        \"maze\":\n",
    "            dict(env_s=\"\"\"\\\n",
    "                |R < . X . > H|\n",
    "                |            X|\n",
    "                |. X .   .   .|\n",
    "                |        X   X|\n",
    "                |. X .   . X .|\n",
    "                |    X   ^    |\n",
    "                |. X .   . X .|\n",
    "                |    X   ^   ^|\n",
    "                |P   . > . > .|\n",
    "                \"\"\",\n",
    "                 fire_process_kargs=dict(fire_weights=np.array([\n",
    "                     [0, 1, 0],\n",
    "                     [1, 10, 1],\n",
    "                     [0, 1, 0],\n",
    "                 ])),\n",
    "                 _horizon=20),\n",
    "        \"just_wait\":\n",
    "            dict(env_s=\"\"\"\\\n",
    "                |.   R   .   H|\n",
    "                |X   v   X   ^|\n",
    "                |. X . X .   .|\n",
    "                |    v       ^|\n",
    "                |. X . X .   .|\n",
    "                |    v       ^|\n",
    "                |. X . X .   .|\n",
    "                |    v       ^|\n",
    "                |F X . X F   .|\n",
    "                |    v       ^|\n",
    "                |. X . X .   F|\n",
    "                |    v       ^|\n",
    "                |. X . X .   .|\n",
    "                |X   v   X   ^|\n",
    "                |P   .   .   .|\n",
    "                \"\"\",\n",
    "                 fire_process_kargs=dict(fire_weights=np.array([\n",
    "                     [0, 1, 0],\n",
    "                     [1, 20, 1],\n",
    "                     [0, 1, 0],\n",
    "                 ]))),\n",
    "        \"the_circle\":\n",
    "            dict(env_s=\"\"\"\\\n",
    "                |R   .   .   H|\n",
    "                |    X   X   v|\n",
    "                |. X .   . X .|\n",
    "                |            v|\n",
    "                |. X F   . X .|\n",
    "                |            v|\n",
    "                |. X F   . X .|\n",
    "                |    X   X   v|\n",
    "                |P   . < . < .|\n",
    "                \"\"\",\n",
    "                 fire_process_kargs=dict(fire_weights=np.array([\n",
    "                     [0, 1, 0],\n",
    "                     [1, 4, 1],\n",
    "                     [0, 1, 0],\n",
    "                 ]))),\n",
    "        \"the_choice\":\n",
    "            dict(\n",
    "                env_s=\"\"\"\\\n",
    "                |.   .   F   F   F   F|\n",
    "                |.   X   X   X   X   X|\n",
    "                |. X .   .   .   .   .|\n",
    "                |X                    |\n",
    "                |R > .   .   F   .   .|\n",
    "                |v                    |\n",
    "                |. X .   .   .   .   .|\n",
    "                |v   X   X   X   X   v|\n",
    "                |. X .   F   F   . X .|\n",
    "                |v                   v|\n",
    "                |. X F   .   .   . X .|\n",
    "                |v                   v|\n",
    "                |. X F   .   .   . X .|\n",
    "                |v                   v|\n",
    "                |. X F   F   .   . X .|\n",
    "                |v   X   X   X   X   v|\n",
    "                |. > . > H   P < . < .|\n",
    "                \"\"\",\n",
    "                fire_process_kargs=dict(fire_weights=np.array([\n",
    "                    [0, 1, 0],\n",
    "                    [1, 4, 1],\n",
    "                    [0, 1, 0],\n",
    "                ]),),\n",
    "            )\n",
    "    }\n",
    "\n",
    "    if name not in params:\n",
    "        raise ValueError(f\"Unknown problem name: {name}\")\n",
    "\n",
    "    params[name][\"env_s\"] = textwrap.dedent(params[name][\"env_s\"])\n",
    "    claz = FirePOMDP if kind == \"pomdp\" else FireMDP\n",
    "    return claz.from_str(**params[name])\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"An agent that can act in an MDP or POMDP.\n",
    "\n",
    "    A derived agent must keep track of its own internal state.\n",
    "    \"\"\"\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the agent's internal state.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def act(self, obs: Union[Observation, State]) -> Action:\n",
    "        \"\"\"Return the agent's action given an observation. \n",
    "        For MDP agents, `obs` will be the complete state\"\"\"\n",
    "        ...\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class OpenLoopAgent(Agent):\n",
    "    \"\"\"Agent that just follows a fixed sequence of actions.\"\"\"\n",
    "\n",
    "    actions: Sequence[Action]\n",
    "\n",
    "    t: int = dataclasses.field(default=0, init=False)\n",
    "\n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "\n",
    "    def act(self, obs) -> Action:\n",
    "        del obs  # observation is not used\n",
    "        assert self.t < len(self.actions)\n",
    "        a = self.actions[self.t]\n",
    "        self.t += 1\n",
    "        return self.actions\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class RolloutLookaheadAgent(Agent):\n",
    "    \"\"\"MDP Agent that uses a rollout lookahead to decide what to do.\"\"\"\n",
    "\n",
    "    problem: MDP\n",
    "    n_rollout_per_action: int = 10\n",
    "\n",
    "    receding_horizon: int = None\n",
    "    t: int = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "\n",
    "    @property\n",
    "    def planning_horizon(self):\n",
    "        if self.receding_horizon is None:\n",
    "            return self.problem.horizon - self.t\n",
    "        return self.receding_horizon\n",
    "\n",
    "    def act(self, state: State) -> Action:\n",
    "        \"\"\"Return the action that maximizes the expected reward.\"\"\"\n",
    "        self.t += 1\n",
    "        actions = list(self.problem.actions(state))\n",
    "        random.shuffle(actions)\n",
    "        return max(actions, key=lambda a: self._rollout(state, a))\n",
    "\n",
    "    def _rollout(self, state: State, action: Action) -> float:\n",
    "        \"\"\"Return the expected reward of taking action in state.\"\"\"\n",
    "        return sum(\n",
    "            self._rollout_single(state, action) for _ in range(\n",
    "                self.n_rollout_per_action)) / self.n_rollout_per_action\n",
    "\n",
    "    def _rollout_single(self, state: State, action: Action) -> float:\n",
    "        \"\"\"simulate the utility of current state by taking a rollout policy.\"\"\"\n",
    "        total_reward = 0\n",
    "        disc = 1\n",
    "        t = 0\n",
    "        planning_horizon = self.planning_horizon\n",
    "        while t < planning_horizon and not self.problem.terminal(state):\n",
    "            if t > 0:\n",
    "                action = self.rollout_policy(state)\n",
    "            next_state = self.problem.step(state, action)\n",
    "            reward = self.problem.reward(state, action, next_state)\n",
    "            total_reward += disc * reward\n",
    "            state = next_state\n",
    "            disc = disc * self.problem.discount\n",
    "            t += 1\n",
    "        return total_reward\n",
    "\n",
    "    def rollout_policy(self, state: State) -> Action:\n",
    "        \"\"\"Return the action to take in state during rollout. \n",
    "\n",
    "        Subclass may override to implement rollout policy with preferred actions.\"\"\"\n",
    "\n",
    "        return random.choice(list(self.problem.actions(state)))\n",
    "\n",
    "\n",
    "def benchmark_agent(problem: Union[FireMDP, FirePOMDP],\n",
    "                    agent: Agent,\n",
    "                    n_repeats: int = 100,\n",
    "                    verbose: bool = False) -> List[float]:\n",
    "    \"\"\"Bencmark an agent on a problem by performing repeated experiments.\"\"\"\n",
    "    import tqdm\n",
    "    total_rewards = []\n",
    "    for _ in tqdm.tqdm(range(n_repeats)):\n",
    "        *_, total_reward = run_agent_on_problem(problem, agent, verbose=verbose)\n",
    "        total_rewards.append(total_reward)\n",
    "    return total_rewards\n",
    "\n",
    "\n",
    "def compare_agents(problem: Union[POMDP, MDP],\n",
    "                   agents: Dict[str, Agent],\n",
    "                   n_repeats: int = 30,\n",
    "                   verbose: bool = False):\n",
    "    \"\"\"Compare the performance of multiple agents on a problem.\"\"\"\n",
    "    for agent in agents:\n",
    "        if isinstance(agent, tuple):\n",
    "            agent, name = agent\n",
    "        else:\n",
    "            name = agent.__class__.__name__\n",
    "        print(f\"Running {name}...\")\n",
    "        rewards = benchmark_agent(problem,\n",
    "                                  agent,\n",
    "                                  n_repeats=n_repeats,\n",
    "                                  verbose=verbose)\n",
    "        print(f\"Mean reward: {np.mean(rewards):.2f} +- {np.std(rewards):.2f}\")\n",
    "        print(f\"Median reward: {np.median(rewards):.2f}\")\n",
    "        print(f\"Min reward: {np.min(rewards):.2f}\")\n",
    "        print(f\"Max reward: {np.max(rewards):.2f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cf74b3",
   "metadata": {},
   "source": [
    "## Determinized Min-cost Path Problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58619ff0",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf7ca78",
   "metadata": {},
   "source": [
    "\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c36f566",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True, eq=True, order=True)\n",
    "class DeterminizedFireMDPState(PickupProblemState):\n",
    "    \"\"\"A state for the DeterminizedFireMDP.\n",
    "\n",
    "    The state is a pair of the PickupProblemState and a time step $t$.\n",
    "    \"\"\"\n",
    "    time: int = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933acb0d",
   "metadata": {},
   "source": [
    "### Question\n",
    "Please complete the implementation of `DeterminizedFireMDP`. In particular, you should:\n",
    "- Write code to compute the log-likelihood of each cell being on fire at time $t$ given the true fire state at time $0$.\n",
    "- Write the heuristic function `h` based on description above.\n",
    "    \n",
    "\n",
    "For reference, our solution is **68** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e2e180",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass(frozen=True)\n",
    "class DeterminizedFireMDP(PathCostProblem):\n",
    "    \"\"\"Determinized version of the fire MDP --- tries to find the solution path \n",
    "    that is most likely to succeed.    \n",
    "    \"\"\"\n",
    "    pickup_problem: PickupProblem\n",
    "    fire_process: FireProcess\n",
    "\n",
    "    # Additional cost for each step.\n",
    "    # Can be 0 but we might have 0-cost arcs if the success probability is 1.\n",
    "    action_cost = 1e-6\n",
    "\n",
    "    # Use this to cache precomputed fire distributions, so we don't have to recompute them.\n",
    "    fire_dists_cache: Dict[int, np.ndarray] = dataclasses.field(\n",
    "        init=False,\n",
    "        default_factory=dict,\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert (self.pickup_problem.grid_shape ==\n",
    "                self.fire_process.initial_fire_grid.shape)\n",
    "\n",
    "    @property\n",
    "    def initial(self) -> DeterminizedFireMDPState:\n",
    "        return DeterminizedFireMDPState(\n",
    "            *dataclasses.astuple(self.pickup_problem.initial),\n",
    "            time=0,\n",
    "        )\n",
    "\n",
    "    def actions(self, state: DeterminizedFireMDPState) -> Iterable[Action]:\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    def step(self, state: DeterminizedFireMDPState,\n",
    "             action: Action) -> State:\n",
    "        \"\"\"We automatically pick up patient if we're on that square.\"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    def goal_test(self, state: DeterminizedFireMDPState) -> bool:\n",
    "        \"\"\"True if at hospital and holding patient.\"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    def step_cost(self, state1: DeterminizedFireMDPState, action: Action,\n",
    "                  state2: DeterminizedFireMDPState) -> float:\n",
    "        \"\"\"Sum of the negative log probs of start and dest being safe.\"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    def fire_dist_at_time(self, t: int) -> np.ndarray:\n",
    "        \"\"\"Return the marginal distribution of fire grid at time $t$.\"\"\"\n",
    "\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "        if t not in self.fire_dists_cache:\n",
    "            if t == 0:\n",
    "                dist = ...  # TODO: Implement this.\n",
    "            else:\n",
    "                dist = ...  # TODO: Implement this.\n",
    "            self.fire_dists_cache[t] = dist\n",
    "        return self.fire_dists_cache[t]\n",
    "\n",
    "    def h(self, state: DeterminizedFireMDPState) -> float:\n",
    "        \"\"\"heuristic based on the manhattan distance to the patient and hospital.\"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e8c34b",
   "metadata": {},
   "source": [
    "## Determinized Fire MDP Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6bb14f",
   "metadata": {},
   "source": [
    "### Question\n",
    "Please complete the implementation of `FireMDPDeterminizedAStarAgent`. \n",
    "Note that we have filled in most of the implementation for you --- including \n",
    "the call to `run_astar_search` from HW01. \n",
    "All you need to implement is the `determinized_problem` method.\n",
    "\n",
    "\n",
    "For reference, our solution is **27** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f34c74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass(frozen=True)\n",
    "class FireMDPDeterminizedAStarAgent(Agent):\n",
    "    \"\"\"Agent that uses A* to plan a path to the goal in a determinized \n",
    "    version of the problem. Does not need any internal state since we \n",
    "    re-determinize the problem at each step.\n",
    "    \"\"\"\n",
    "\n",
    "    problem: FireMDP\n",
    "    step_budget: int = 10000\n",
    "\n",
    "    def determinized_problem(self,\n",
    "                             state: FireMDPState) -> DeterminizedFireMDP:\n",
    "        \"\"\"Returns a determinized approximation of the fire MDP.\"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    def act(self, state: FireMDPState) -> Action:\n",
    "        problem = self.determinized_problem(state)\n",
    "        try:\n",
    "            plan = run_astar_search(problem, self.step_budget)\n",
    "        except SearchFailed:\n",
    "            print(\"Search failed, performing a random action\")\n",
    "            return random.choice(list(self.problem.actions(state)))\n",
    "        return plan[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b809ee",
   "metadata": {},
   "source": [
    "## MCTS Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43719c0",
   "metadata": {},
   "source": [
    "### Question\n",
    "Please complete the implementation of `MCTSAgent`. \n",
    "\n",
    "_Hint: You can pass in the `self.planning_horizon` to `run_mcts_search`, \n",
    "to handle both infinite-horizon problems (by receding-horizon planning) and finite-horizon problems._\n",
    "\n",
    "\n",
    "For reference, our solution is **27** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3583da",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass(frozen=True)\n",
    "class FireMDPDeterminizedAStarAgent(Agent):\n",
    "    \"\"\"Agent that uses A* to plan a path to the goal in a determinized \n",
    "    version of the problem. Does not need any internal state since we \n",
    "    re-determinize the problem at each step.\n",
    "    \"\"\"\n",
    "\n",
    "    problem: FireMDP\n",
    "    step_budget: int = 10000\n",
    "\n",
    "    def determinized_problem(self,\n",
    "                             state: FireMDPState) -> DeterminizedFireMDP:\n",
    "        \"\"\"Returns a determinized approximation of the fire MDP.\"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    def act(self, state: FireMDPState) -> Action:\n",
    "        problem = self.determinized_problem(state)\n",
    "        try:\n",
    "            plan = run_astar_search(problem, self.step_budget)\n",
    "        except SearchFailed:\n",
    "            print(\"Search failed, performing a random action\")\n",
    "            return random.choice(list(self.problem.actions(state)))\n",
    "        return plan[1][0]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mp03.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
