{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcceef09",
   "metadata": {},
   "source": [
    "# Miniproject 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca6ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install backports.cached_property\n",
    "\n",
    "# Setup matplotlib animation\n",
    "import matplotlib\n",
    "matplotlib.rc('animation', html='jshtml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6d3c2b",
   "metadata": {},
   "source": [
    "## Imports and Utilities\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a68dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import (Callable, Iterable, List, Sequence, Tuple, Dict, Optional,\n",
    "                    Any, Union, Set, ClassVar, Type, TypeVar)\n",
    "\n",
    "from abc import abstractmethod, ABC\n",
    "import collections\n",
    "import textwrap\n",
    "import math\n",
    "import functools\n",
    "import itertools\n",
    "import random\n",
    "import dataclasses\n",
    "import numpy as np\n",
    "import heapq as hq\n",
    "\n",
    "try:\n",
    "    from functools import cached_property\n",
    "except ImportError:\n",
    "    # Import for Colab (Python==3.7)\n",
    "    from backports.cached_property import cached_property\n",
    "\n",
    "import scipy.signal\n",
    "\n",
    "\n",
    "def heatmap(data, ax=None, cbar_kw=None, cbarlabel=\"\", **kwargs):\n",
    "    \"\"\"\n",
    "    Create a heatmap from a numpy array and two lists of labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data\n",
    "        A 2D numpy array of shape (M, N).\n",
    "    ax\n",
    "        A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If\n",
    "        not provided, use current axes or create a new one.  Optional.\n",
    "    cbar_kw\n",
    "        A dictionary with arguments to `matplotlib.Figure.colorbar`.  Optional.\n",
    "    cbarlabel\n",
    "        The label for the colorbar.  Optional.\n",
    "    **kwargs\n",
    "        All other arguments are forwarded to `imshow`.\n",
    "    \"\"\"\n",
    "    import matplotlib.pylab as plt\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    # Plot the heatmap\n",
    "    im = ax.imshow(data, **kwargs)\n",
    "\n",
    "    # Create colorbar\n",
    "    if cbar_kw is not None:\n",
    "        cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n",
    "        cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n",
    "\n",
    "    # Show all ticks and label them with the respective list entries.\n",
    "    ax.set_xticks(np.arange(data.shape[1]))\n",
    "    ax.set_yticks(np.arange(data.shape[0]))\n",
    "\n",
    "    # Let the horizontal axes labeling appear on top.\n",
    "    ax.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(),\n",
    "             rotation=-30,\n",
    "             ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Turn spines off and create white grid.\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1] + 1) - .5, minor=True)\n",
    "    ax.set_yticks(np.arange(data.shape[0] + 1) - .5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"black\", linestyle='-', linewidth=1.5)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "    return im\n",
    "\n",
    "\n",
    "def annotate_heatmap(im,\n",
    "                     data=None,\n",
    "                     valfmt=\"{x:.2f}\",\n",
    "                     textcolors=(\"black\", \"white\"),\n",
    "                     threshold=None,\n",
    "                     **textkw):\n",
    "    \"\"\"\n",
    "    A function to annotate a heatmap.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    im\n",
    "        The AxesImage to be labeled.\n",
    "    data\n",
    "        Data used to annotate.  If None, the image's data is used.  Optional.\n",
    "    valfmt\n",
    "        The format of the annotations inside the heatmap.  This should either\n",
    "        use the string format method, e.g. \"$ {x:.2f}\", or be a\n",
    "        `matplotlib.ticker.Formatter`.  Optional.\n",
    "    textcolors\n",
    "        A pair of colors.  The first is used for values below a threshold,\n",
    "        the second for those above.  Optional.\n",
    "    threshold\n",
    "        Value in data units according to which the colors from textcolors are\n",
    "        applied.  If None (the default) uses the middle of the colormap as\n",
    "        separation.  Optional.\n",
    "    **kwargs\n",
    "        All other arguments are forwarded to each call to `text` used to create\n",
    "        the text labels.\n",
    "    \"\"\"\n",
    "\n",
    "    import matplotlib.ticker\n",
    "\n",
    "    if not isinstance(data, (list, np.ndarray)):\n",
    "        data = im.get_array()\n",
    "\n",
    "    # Normalize the threshold to the images color range.\n",
    "    if threshold is not None:\n",
    "        threshold = im.norm(threshold)\n",
    "    else:\n",
    "        threshold = im.norm(data.max()) / 2.\n",
    "\n",
    "    # Set default alignment to center, but allow it to be\n",
    "    # overwritten by textkw.\n",
    "    kw = dict(horizontalalignment=\"center\", verticalalignment=\"center\")\n",
    "    kw.update(textkw)\n",
    "\n",
    "    # Get the formatter in case a string is supplied\n",
    "    if isinstance(valfmt, str):\n",
    "        valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)\n",
    "\n",
    "    # Loop over the data and create a `Text` for each \"pixel\".\n",
    "    # Change the text's color depending on the data.\n",
    "    texts = []\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
    "            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)\n",
    "            texts.append(text)\n",
    "\n",
    "    return texts\n",
    "\n",
    "\n",
    "State = Any\n",
    "Observation = Any\n",
    "Action = Any\n",
    "\n",
    "\n",
    "class Problem(ABC):\n",
    "    \"\"\"The abstract base class for either a path cost problem or a reward problem.\"\"\"\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def initial(self) -> State:\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def actions(self, state: State) -> Iterable[Action]:\n",
    "        \"\"\"Returns the allowed actions in a given state.\n",
    "\n",
    "        The result would typically be a list. But if there are many\n",
    "        actions, consider yielding them one at a time in an iterator,\n",
    "        rather than building them all at once.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self, state: State, action: Action) -> State:\n",
    "        \"\"\"Returns the next state when executing a given action in a given\n",
    "        state.\n",
    "\n",
    "        The action must be one of self.actions(state).\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "\n",
    "class PathCostProblem(Problem):\n",
    "    \"\"\"An abstract class for a path cost problem, based on AIMA.\n",
    "\n",
    "    To formalize a path cost problem, you should subclass from this and\n",
    "    implement the abstract methods. Then you will create instances of\n",
    "    your subclass and solve them with the various search functions.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def goal_test(self, state: State) -> bool:\n",
    "        \"\"\"Checks if the state is a goal.\"\"\"\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def step_cost(self, state1: State, action: Action, state2: State) -> float:\n",
    "        \"\"\"Returns the cost incurred at state2 from state1 via action.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def h(self, state: State) -> float:\n",
    "        \"\"\"Returns the heuristic value, a lower bound on the distance to goal.\"\"\"\n",
    "        return 0\n",
    "\n",
    "\n",
    "class POMDP(Problem):\n",
    "    \"\"\"A generative-model-based POMDP.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def discount(self) -> float:\n",
    "        \"\"\"The discount factor.\"\"\"\n",
    "        return 1.\n",
    "\n",
    "    @property\n",
    "    def horizon(self) -> int:\n",
    "        \"\"\"The planning horizon.\"\"\"\n",
    "        return np.inf\n",
    "\n",
    "    @abstractmethod\n",
    "    def terminal(self, state: State) -> bool:\n",
    "        \"\"\"If this state is terminating (absorbing state).\"\"\"\n",
    "        return False\n",
    "\n",
    "    @abstractmethod\n",
    "    def reward(self, state1: State, action: Action, state2: State) -> float:\n",
    "        \"\"\"Returns the reward given at state2 from state1 via action.\"\"\"\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_observation(self, state: State) -> State:\n",
    "        \"\"\"Sample an observation from current state.\"\"\"\n",
    "        ...\n",
    "\n",
    "\n",
    "class MDP(POMDP):\n",
    "    \"\"\"An generative-model-based MDP.\"\"\"\n",
    "\n",
    "    def get_observation(self, state: State) -> State:\n",
    "        \"\"\"An MDP is fully observable.\"\"\"\n",
    "        return state\n",
    "\n",
    "\n",
    "class SearchFailed(ValueError):\n",
    "    \"\"\"Raise this exception whenever a search must fail.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# A useful data structure for best-first search\n",
    "BFSNode = collections.namedtuple(\"BFSNode\",\n",
    "                                 [\"state\", \"parent\", \"action\", \"cost\", \"g\"])\n",
    "\n",
    "\n",
    "def run_best_first_search(\n",
    "    problem: PathCostProblem,\n",
    "    get_priority: Callable[[BFSNode], float],\n",
    "    step_budget: int = 10000\n",
    ") -> Tuple[List[State], List[Action], List[float], int]:\n",
    "    \"\"\"A generic heuristic search implementation.\n",
    "\n",
    "    Depending on `get_priority`, can implement A*, GBFS, or UCS.\n",
    "\n",
    "    The `get_priority` function here should determine the order\n",
    "    in which nodes are expanded. For example, if you want to\n",
    "    use path cost as part of this determination, then the\n",
    "    path cost (node.g) should appear inside of get_priority,\n",
    "    rather than in this implementation of `run_best_first_search`.\n",
    "\n",
    "    Important: for determinism (and to make sure our tests pass),\n",
    "    please break ties using the state itself. For example,\n",
    "    if you would've otherwise sorted by `get_priority(node)`, you\n",
    "    should now sort by `(get_priority(node), node.state)`.\n",
    "\n",
    "    Args:\n",
    "      problem: a path cost problem.\n",
    "      get_priority: a callable taking in a search Node and returns the priority\n",
    "      step_budget: maximum number of `problem.step` before giving up.\n",
    "\n",
    "    Returns:\n",
    "      state_sequence: A list of states.\n",
    "      action_sequence: A list of actions.\n",
    "      cost_sequence: A list of costs.\n",
    "      num_steps: number of taken `problem.step`s. Must be less than or equal to `step_budget`.\n",
    "\n",
    "    Raises:\n",
    "      error: SearchFailed, if no plan is found.\n",
    "    \"\"\"\n",
    "    num_steps = 0\n",
    "    frontier = []\n",
    "    reached = {}\n",
    "\n",
    "    root_node = BFSNode(state=problem.initial,\n",
    "                        parent=None,\n",
    "                        action=None,\n",
    "                        cost=None,\n",
    "                        g=0)\n",
    "    hq.heappush(frontier, (get_priority(root_node), problem.initial, root_node))\n",
    "    reached[problem.initial] = root_node\n",
    "    num_expansions = 0\n",
    "\n",
    "    while frontier:\n",
    "        pri, s, node = hq.heappop(frontier)\n",
    "        # If reached the goal, return\n",
    "        if problem.goal_test(node.state):\n",
    "            return (*finish_plan(node), num_steps)\n",
    "\n",
    "        num_expansions += 1\n",
    "        # Generate successors\n",
    "        for action in problem.actions(node.state):\n",
    "            if num_steps >= step_budget:\n",
    "                raise SearchFailed(\n",
    "                    f\"Failed to find a plan in {step_budget} steps\")\n",
    "            child_state = problem.step(node.state, action)\n",
    "            num_steps += 1\n",
    "            cost = problem.step_cost(node.state, action, child_state)\n",
    "            path_cost = node.g + cost\n",
    "            # If the state is already in explored or reached, don't bother\n",
    "            if not child_state in reached or path_cost < reached[child_state].g:\n",
    "                # Add new node\n",
    "                child_node = BFSNode(state=child_state,\n",
    "                                     parent=node,\n",
    "                                     action=action,\n",
    "                                     cost=cost,\n",
    "                                     g=path_cost)\n",
    "                priority = get_priority(child_node)\n",
    "                hq.heappush(frontier, (priority, child_state, child_node))\n",
    "                reached[child_state] = child_node\n",
    "    raise SearchFailed(f\"Frontier exhausted after {num_steps} steps\")\n",
    "\n",
    "\n",
    "def finish_plan(node: BFSNode):\n",
    "    \"\"\"Helper for run_best_first_search.\"\"\"\n",
    "    state_sequence = []\n",
    "    action_sequence = []\n",
    "    cost_sequence = []\n",
    "\n",
    "    while node.parent is not None:\n",
    "        action_sequence.insert(0, node.action)\n",
    "        state_sequence.insert(0, node.state)\n",
    "        cost_sequence.insert(0, node.cost)\n",
    "        node = node.parent\n",
    "    state_sequence.insert(0, node.state)\n",
    "\n",
    "    return state_sequence, action_sequence, cost_sequence\n",
    "\n",
    "\n",
    "def run_astar_search(problem: PathCostProblem, step_budget: int = 10000):\n",
    "    \"\"\"A* search.\n",
    "\n",
    "    Use your implementation of `run_best_first_search`.\n",
    "    \"\"\"\n",
    "    get_priority = lambda node: node.g + problem.h(node.state)\n",
    "    return run_best_first_search(problem, get_priority, step_budget=step_budget)\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=False, eq=False)\n",
    "class MCTStateNode:\n",
    "    \"\"\"Node in the Monte Carlo search tree, keeps track of the children states.\"\"\"\n",
    "    # For MDP, this is a state; for POMDP, this is an observation\n",
    "    obs: Union[State, Observation]\n",
    "    N: int\n",
    "    horizon: int\n",
    "    parent: Optional['MCTChanceNode']\n",
    "    children: Dict['MCTChanceNode',\n",
    "                   Action] = dataclasses.field(default_factory=dict)\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=False, eq=False)\n",
    "class MCTChanceNode:\n",
    "    U: float\n",
    "    N: int\n",
    "    parent: MCTStateNode\n",
    "    children: Dict[State,\n",
    "                   MCTStateNode] = dataclasses.field(default_factory=dict)\n",
    "\n",
    "\n",
    "def ucb(n: MCTStateNode, C: float = 1.4) -> float:\n",
    "    \"\"\"UCB for a node, note the C argument\"\"\"\n",
    "    return (np.inf if n.N == 0 else\n",
    "            (n.U / n.N + C * np.sqrt(np.log(n.parent.N) / n.N)))\n",
    "\n",
    "\n",
    "RolloutPolicy = Callable[[State], Action]\n",
    "\n",
    "\n",
    "def random_rollout_policy(problem: MDP, state: State) -> Action:\n",
    "    return random.choice(list(problem.actions(state)))\n",
    "\n",
    "\n",
    "def run_mcts_search(problem: POMDP,\n",
    "                    state: Optional[State] = None,\n",
    "                    state_sampler: Iterable[State] = None,\n",
    "                    horizon: Optional[int] = None,\n",
    "                    C: float = np.sqrt(2),\n",
    "                    iteration_budget: int = 100,\n",
    "                    n_simulations: int = 10,\n",
    "                    max_backup: bool = True,\n",
    "                    rollout_policy: RolloutPolicy = None,\n",
    "                    verbose: bool = False) -> Action:\n",
    "    \"\"\"A generic MCTS search implementation for MDP and POMDPs.\n",
    "\n",
    "    For MDP, this is a standard MCTS implementation based on description in AIMA \n",
    "    (with some additional features). \n",
    "    For POMDP, this is an implementation of the POMCP algorithm.\n",
    "\n",
    "    Args:\n",
    "        problem: an MDP or POMDP.\n",
    "        state: the initial state. If None, then `state_sampler` must be provided.\n",
    "        state_sampler: an iterable of states. If None, then `state` must be provided. This is used to sample the initial state for POMCP.\n",
    "        horizon: the horizon of the search. If None, then the horizon is set to the problem's horizon.\n",
    "        C: the C parameter for UCB.\n",
    "        iteration_budget: the maximum number of iterations.\n",
    "        n_simulations: the number of simulations to run at each MCTS iteration. In AIMA, \n",
    "            this is set 1, but in general, we run multiple simulations to reduce variance.\n",
    "        max_backup: whether to use max backup or sum backup.\n",
    "        rollout_policy: the rollout policy. If None, then the random rollout policy is used.\n",
    "        verbose: whether to print debug information.\n",
    "\n",
    "    Returns:\n",
    "        action: the best action to take at the initial state according to the search.\n",
    "    \"\"\"\n",
    "    if state is None:\n",
    "        state = problem.initial\n",
    "\n",
    "    if state_sampler is None:\n",
    "        state_sampler = itertools.repeat(state)\n",
    "\n",
    "    if horizon is None:\n",
    "        horizon = problem.horizon\n",
    "\n",
    "    if rollout_policy is None:\n",
    "        rollout_policy = functools.partial(random_rollout_policy, problem)\n",
    "\n",
    "    ucb_fixed_C = functools.partial(ucb, C=C)\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    def select(n: MCTStateNode, state: State) -> Tuple[MCTStateNode, State]:\n",
    "        \"\"\"select a leaf node in the tree.\"\"\"\n",
    "        if n.children:\n",
    "            # Select the best child, break ties randomly\n",
    "            children = list(n.children.keys())\n",
    "            random.shuffle(children)\n",
    "            ucb_pick: MCTChanceNode = max(children, key=ucb_fixed_C)\n",
    "            act = n.children[ucb_pick]\n",
    "            next_state = problem.step(state, act)\n",
    "            rewards.append(problem.reward(state, act, next_state))\n",
    "            next_obs = problem.get_observation(\n",
    "                next_state)  # For MDP this is same as next_state\n",
    "            if next_obs not in ucb_pick.children:\n",
    "                new_leaf = MCTStateNode(next_obs,\n",
    "                                        horizon=n.horizon - 1,\n",
    "                                        parent=ucb_pick,\n",
    "                                        N=0)\n",
    "                ucb_pick.children[next_obs] = new_leaf\n",
    "                return new_leaf, next_state\n",
    "            return select(ucb_pick.children[next_obs], next_state)\n",
    "        return n, state\n",
    "\n",
    "    def expand(n: MCTStateNode, state: State) -> Tuple[MCTStateNode, State]:\n",
    "        \"\"\"expand the leaf node by adding all its children actions.\"\"\"\n",
    "        assert not n.children\n",
    "        if n.horizon == 0 or problem.terminal(state):\n",
    "            return n, state\n",
    "        for action in problem.actions(state):\n",
    "            new_chance_node = MCTChanceNode(parent=n, U=0, N=0)\n",
    "            n.children[new_chance_node] = action\n",
    "        chance_node, action = random.choice(list(n.children.items()))\n",
    "        next_state = problem.step(state, action)\n",
    "        rewards.append(problem.reward(state, action, next_state))\n",
    "        next_obs = problem.get_observation(\n",
    "            next_state)  # For MDP this is same as next_state\n",
    "        new_node = MCTStateNode(next_obs,\n",
    "                                N=0,\n",
    "                                horizon=n.horizon - 1,\n",
    "                                parent=chance_node)\n",
    "        chance_node.children[next_obs] = new_node\n",
    "        return new_node, next_state\n",
    "\n",
    "    def simulate(node: MCTStateNode, state: State) -> float:\n",
    "        \"\"\"simulate the utility of current state by taking a rollout policy.\"\"\"\n",
    "        total_reward = 0\n",
    "        disc = 1\n",
    "        h = node.horizon\n",
    "        while h > 0 and not problem.terminal(state):\n",
    "            action = rollout_policy(state)\n",
    "            next_state = problem.step(state, action)\n",
    "            reward = problem.reward(state, action, next_state)\n",
    "            total_reward += disc * reward\n",
    "            state = next_state\n",
    "            disc = disc * problem.discount\n",
    "            h -= 1\n",
    "        return total_reward\n",
    "\n",
    "    def backup(state_node: MCTStateNode, value: float) -> None:\n",
    "        \"\"\"passing the utility back to all parent nodes.\"\"\"\n",
    "        state_node.N += 1\n",
    "        if state_node.parent:\n",
    "            # Need to include the reward on the action *into* n\n",
    "            parent_chance_node = state_node.parent\n",
    "            parent_state_node = parent_chance_node.parent\n",
    "            r = rewards.pop()\n",
    "            future_val = r + problem.discount * value\n",
    "            parent_chance_node.U += future_val\n",
    "            parent_chance_node.N += 1\n",
    "            if max_backup:\n",
    "                bk_val = max(0 if n.N == 0 else n.U / n.N\n",
    "                             for n in parent_state_node.children)\n",
    "            else:\n",
    "                bk_val = future_val\n",
    "            backup(parent_state_node, bk_val)\n",
    "\n",
    "    state = next(state_sampler)\n",
    "    root = MCTStateNode(obs=problem.get_observation(state),\n",
    "                        horizon=horizon,\n",
    "                        parent=None,\n",
    "                        N=0)\n",
    "\n",
    "    i = 0\n",
    "    while i < iteration_budget:\n",
    "        state = next(state_sampler)\n",
    "        assert len(rewards) == 0\n",
    "        leaf, state = select(root, state)\n",
    "        child, state = expand(leaf, state)\n",
    "        value = np.mean([simulate(child, state) for _ in range(n_simulations)])\n",
    "        backup(child, value)\n",
    "        i += 1\n",
    "\n",
    "    children = list(root.children.keys())\n",
    "    random.shuffle(children)\n",
    "    act = root.children[max(children, key=lambda p: p.U / p.N)]\n",
    "    if verbose:\n",
    "        print(\n",
    "            {\n",
    "                act: (c.U / c.N if c.N > 0 else 0, c.N)\n",
    "                for c, act in root.children.items()\n",
    "            }, act)\n",
    "    return act\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True, eq=True, order=True)\n",
    "class PickupProblemState:\n",
    "    robot_loc: Tuple[int, int]\n",
    "    carried_patient: bool\n",
    "\n",
    "\n",
    "OneWayBlock = collections.namedtuple('OneWayBlock', ['loc', 'dest'])\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class PickupProblem(PathCostProblem):\n",
    "    \"\"\"A simple deterministic pickup problem in a grid.\n",
    "\n",
    "    The robot starts at some location and can move up, down, left, or right.\n",
    "    There is a patient at some location.\n",
    "    The goal is to pick up the patient and drop them off at the hospital.\n",
    "    The robot can only move to a location if there is a path from the robot's\n",
    "    current location to the destination that does not pass through any roadblocks.\n",
    "    \"\"\"\n",
    "    grid_shape: Tuple[int, int]\n",
    "\n",
    "    initial_robot_loc: Tuple[int, int] = (0, 0)\n",
    "\n",
    "    patient_loc: Tuple[int, int] = (4, 0)  # initial location of the patient\n",
    "    hospital_loc: Tuple[int, int] = (0, 3)\n",
    "\n",
    "    one_ways: List[OneWayBlock] = dataclasses.field(default_factory=list)\n",
    "\n",
    "    grid_act_to_delta: ClassVar = {\n",
    "        \"up\": (-1, 0),\n",
    "        \"down\": (1, 0),\n",
    "        \"left\": (0, -1),\n",
    "        \"right\": (0, 1)\n",
    "    }\n",
    "    all_grid_actions: ClassVar = tuple(grid_act_to_delta.keys())\n",
    "\n",
    "    @property\n",
    "    def initial(self) -> State:\n",
    "        return PickupProblemState(self.initial_robot_loc,\n",
    "                                  self.initial_robot_loc == self.patient_loc)\n",
    "\n",
    "    @cached_property\n",
    "    def _one_way_set(self) -> Set[OneWayBlock]:\n",
    "        \"\"\"Set of one-way blocks. Helps with fast lookup.\"\"\"\n",
    "        return set(self.one_ways)\n",
    "\n",
    "    def actions(self, state: PickupProblemState) -> Iterable[Action]:\n",
    "        \"\"\"Actions from the current state: move up, down, left, or right unless blocked.\"\"\"\n",
    "        (r, c) = state.robot_loc\n",
    "        actions = []\n",
    "        for act in self.all_grid_actions:\n",
    "            dr, dc = self.grid_act_to_delta[act]\n",
    "            new_r, new_c = r + dr, c + dc\n",
    "            if (new_r in range(self.grid_shape[0]) and\n",
    "                    new_c in range(self.grid_shape[1]) and\n",
    "                    OneWayBlock(state.robot_loc,\n",
    "                                (new_r, new_c)) not in self._one_way_set):\n",
    "                actions.append(act)\n",
    "        return actions\n",
    "\n",
    "    def step(self, state: PickupProblemState,\n",
    "             action: Action) -> PickupProblemState:\n",
    "        \"\"\"We automatically pick up patient if we're on that square.\"\"\"\n",
    "        (r, c) = state.robot_loc\n",
    "        dr, dc = self.grid_act_to_delta[action]\n",
    "        return PickupProblemState(\n",
    "            (r + dr, c + dc),\n",
    "            state.carried_patient or self.patient_loc == (r + dr, c + dc),\n",
    "        )\n",
    "\n",
    "    def step_cost(self, state1, action, state2) -> float:\n",
    "        \"\"\"Cost of taking an action in a state. \n",
    "\n",
    "        Actually not used in this project, but we keep it here for completeness.\n",
    "        \"\"\"\n",
    "        return 1.\n",
    "\n",
    "    def goal_test(self, state: PickupProblemState) -> bool:\n",
    "        \"\"\"True if at hospital and holding patient.\"\"\"\n",
    "        return state.robot_loc == self.hospital_loc and state.carried_patient\n",
    "\n",
    "    def render(self, state: PickupProblemState, ax=None):\n",
    "        \"\"\"Render the state as a grid.\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        if ax is None:\n",
    "            ax = plt.gca()\n",
    "\n",
    "        heatmap(np.zeros(self.grid_shape),\n",
    "                ax=ax,\n",
    "                cmap=\"YlOrRd\",\n",
    "                vmin=0,\n",
    "                vmax=1,\n",
    "                origin=\"upper\")\n",
    "\n",
    "        # Render the robot\n",
    "        robot = plt.Circle(state.robot_loc[::-1], 0.5, color='blue')\n",
    "        ax.add_patch(robot)\n",
    "\n",
    "        # Render the patient\n",
    "        patient_loc = (state.robot_loc\n",
    "                       if state.carried_patient else self.patient_loc)\n",
    "        patient = plt.Circle(patient_loc[::-1], 0.3, color='orange')\n",
    "        ax.add_patch(patient)\n",
    "\n",
    "        # Render the hospital\n",
    "        plt.plot([self.hospital_loc[1]], [self.hospital_loc[0]],\n",
    "                 marker='P',\n",
    "                 color='r',\n",
    "                 markersize=15)\n",
    "\n",
    "        # Render the walls and one-way doors\n",
    "        one_ways = set(self._one_way_set)\n",
    "        while one_ways:\n",
    "            one_way = one_ways.pop()\n",
    "            rev = OneWayBlock(one_way.dest, one_way.loc)\n",
    "            if rev in one_ways:\n",
    "                one_ways.remove(rev)\n",
    "                src, dst = one_way.loc, one_way.dest\n",
    "                if src < dst:\n",
    "                    dst, src = src, dst\n",
    "                src, dst = np.array(src), np.array(dst)\n",
    "                mid_pt = (src + dst) / 2\n",
    "                delta = dst - src\n",
    "                wall = plt.Rectangle(mid_pt[::-1] - delta / 2 +\n",
    "                                     delta[::-1] * 0.05,\n",
    "                                     delta[0] if delta[0] != 0 else 0.1,\n",
    "                                     delta[1] if delta[1] != 0 else 0.1,\n",
    "                                     color='black')\n",
    "                ax.add_patch(wall)\n",
    "            else:\n",
    "                src, dst = np.array(one_way.loc), np.array(one_way.dest)\n",
    "                mid_pt = (src + dst) / 2\n",
    "                delta = dst - src\n",
    "                base = mid_pt + delta * 0.07\n",
    "                length = -delta * 0.2\n",
    "                arrow = plt.arrow(base[1],\n",
    "                                  base[0],\n",
    "                                  length[1],\n",
    "                                  length[0],\n",
    "                                  width=0.2,\n",
    "                                  length_includes_head=True,\n",
    "                                  head_length=0.2,\n",
    "                                  color='black')\n",
    "                ax.add_patch(arrow)\n",
    "\n",
    "\n",
    "conv2D = functools.partial(scipy.signal.convolve2d,\n",
    "                           mode='same',\n",
    "                           boundary='fill',\n",
    "                           fillvalue=0)\n",
    "\n",
    "FireGridT = np.ndarray  # boolean array\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class FireProcess:\n",
    "    \"\"\"A probabilistic model for the evolution of fire in a grid.\n",
    "\n",
    "    At time step $t$, the probability of a cell being on fire is weighted probability \n",
    "    of the neighboring cell being on fire at time step $t-1$.\n",
    "    \"\"\"\n",
    "\n",
    "    initial_fire_grid: np.ndarray\n",
    "\n",
    "    fire_weights: np.ndarray = np.array([[0, 1, 0], [1, 4, 1], [0, 1, 0]])\n",
    "    attenuation: float = 1.0\n",
    "\n",
    "    rng: np.random.Generator = dataclasses.field(\n",
    "        default_factory=np.random.default_rng)\n",
    "\n",
    "    @cached_property\n",
    "    def normalized_fire_weights(self) -> np.ndarray:\n",
    "        return self.attenuation * self.fire_weights / np.sum(self.fire_weights)\n",
    "\n",
    "    def dist(self, fire_grid: FireGridT) -> np.ndarray:\n",
    "        \"\"\"Given the fire grid at time t, return a new grid with marginal distributions of fire for t + 1.\"\"\"\n",
    "        next_fire_dist = conv2D(fire_grid, self.normalized_fire_weights)\n",
    "        return np.clip(next_fire_dist, 0, 1)  # clip for numerical stability\n",
    "\n",
    "    def sample(self, fire_grid: FireGridT) -> FireGridT:\n",
    "        \"\"\"Given the fire grid at time t, return a new grid that's a sample from the distribution.\"\"\"\n",
    "        return self.rng.binomial(1, self.dist(fire_grid)).astype(bool)\n",
    "\n",
    "    def render(self, fire_grid: FireGridT, ax=None):\n",
    "        heatmap(fire_grid, ax=ax, cmap=\"YlOrRd\", vmin=0, vmax=1, origin=\"upper\")\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True, eq=True)\n",
    "class FireMDPState(PickupProblemState):\n",
    "    \"\"\"A state in the fire MDP extends the pickup problem state with a fire grid.\"\"\"\n",
    "\n",
    "    fire_grid: np.ndarray\n",
    "\n",
    "    # Below we implement __eq__ and __hash__ to make FireMDPState hashable.\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, FireMDPState):\n",
    "            return False\n",
    "        return (self.robot_loc == other.robot_loc and\n",
    "                self.carried_patient == other.carried_patient and\n",
    "                np.all(self.fire_grid == other.fire_grid))\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((super().__hash__(), self.fire_grid.tobytes()))\n",
    "\n",
    "\n",
    "T = TypeVar('T', bound='FireProblemCommon')\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class FireProblemCommon:\n",
    "    \"\"\"Common code for the fire MDP and POMDP problems.\"\"\"\n",
    "\n",
    "    pickup_problem: PickupProblem\n",
    "    fire_process: FireProcess\n",
    "\n",
    "    _horizon: int = np.inf\n",
    "    _discount: float = 0.999\n",
    "\n",
    "    step_reward: float = 0.\n",
    "\n",
    "    burn_reward: float = -1\n",
    "    goal_reward: float = 1.\n",
    "\n",
    "    @property\n",
    "    def horizon(self):\n",
    "        return self._horizon\n",
    "\n",
    "    @property\n",
    "    def discount(self):\n",
    "        return self._discount\n",
    "\n",
    "    @property\n",
    "    def initial_robot_loc(self):\n",
    "        return self.pickup_problem.initial_robot_loc\n",
    "\n",
    "    def robot_burned(self, state) -> bool:\n",
    "        return bool(state.fire_grid[state.robot_loc])\n",
    "\n",
    "    def succeeded(self, state) -> bool:\n",
    "        return self.pickup_problem.goal_test(state)\n",
    "\n",
    "    def reward(self, state1, action, state2) -> float:\n",
    "        if self.robot_burned(state2):\n",
    "            return self.burn_reward\n",
    "        if self.succeeded(state2):\n",
    "            return self.goal_reward\n",
    "        return self.step_reward\n",
    "\n",
    "    def terminal(self, state) -> bool:\n",
    "        return (self.robot_burned(state) or self.succeeded(state))\n",
    "\n",
    "    @property\n",
    "    def grid_shape(self):\n",
    "        return self.pickup_problem.grid_shape\n",
    "\n",
    "    @classmethod\n",
    "    def from_str(cls: Type[T],\n",
    "                 env_s: str,\n",
    "                 fire_process_kargs: Optional[Dict[str, Any]] = None,\n",
    "                 **kwargs) -> T:\n",
    "        \"\"\"Create a problem from a grid string.\n",
    "\n",
    "        Legend:\n",
    "            . = empty\n",
    "            F = fire\n",
    "            < = one way block (can go left)\n",
    "            > = one way block (can go right)\n",
    "            ^ = one way block (can go up)\n",
    "            v = one way block (can go down)\n",
    "            X = wall (two way block)\n",
    "            R = robot\n",
    "            P = patient\n",
    "            H = hospital\n",
    "        Each line must be the same length, and starts and ends with a `|`.\n",
    "        Each character is separated by a space.\n",
    "\n",
    "        Warning:\n",
    "            The user needs to make ssure that no cell location is a dead end (due to roadblocks)\n",
    "            since our `terminal` condition does not check for empty available actions.\n",
    "        \"\"\"\n",
    "        if fire_process_kargs is None:\n",
    "            fire_process_kargs = {}\n",
    "\n",
    "        lines = env_s.splitlines()\n",
    "        assert all(len(line) == len(lines[0]) for line in lines)\n",
    "        assert all(line[0] == '|' and line[-1] == '|' for line in lines)\n",
    "        # remove the first and last character of each line\n",
    "        lines = [line[1:-1] for line in lines]\n",
    "        # split each line into a list of characters\n",
    "        lines = [line[::2] for line in lines]\n",
    "        w = len(lines[0]) // 2 + 1\n",
    "        h = len(lines) // 2 + 1\n",
    "        robot_loc = None\n",
    "        patient_loc = None\n",
    "        hospital_loc = None\n",
    "        fire_grid = np.zeros((h, w), dtype=bool)\n",
    "        one_ways = []\n",
    "        for i, line in enumerate(lines):\n",
    "            i2 = i // 2\n",
    "            if i % 2 == 0:\n",
    "                for j, c in enumerate(line):\n",
    "                    j2 = j // 2\n",
    "                    if j % 2 == 0:\n",
    "                        if c == 'F':\n",
    "                            fire_grid[i2, j2] = True\n",
    "                        elif c == 'R':\n",
    "                            robot_loc = (i2, j2)\n",
    "                        elif c == 'P':\n",
    "                            patient_loc = (i2, j2)\n",
    "                        elif c == 'H':\n",
    "                            hospital_loc = (i2, j2)\n",
    "                        else:\n",
    "                            assert c == '.'\n",
    "                    else:\n",
    "                        if c in '<X':\n",
    "                            one_ways.append(OneWayBlock((i2, j2), (i2, j2 + 1)))\n",
    "                        if c in '>X':\n",
    "                            one_ways.append(OneWayBlock((i2, j2 + 1), (i2, j2)))\n",
    "            if i % 2 == 1:\n",
    "                for j, c in enumerate(line[::2]):\n",
    "                    if c in 'vX':\n",
    "                        one_ways.append(\n",
    "                            OneWayBlock(loc=(i2 + 1, j), dest=(i2, j)))\n",
    "                    if c in '^X':\n",
    "                        one_ways.append(\n",
    "                            OneWayBlock(loc=(i2, j), dest=(i2 + 1, j)))\n",
    "\n",
    "        if robot_loc is None:\n",
    "            raise ValueError(\"No robot location specified\")\n",
    "        if patient_loc is None:\n",
    "            raise ValueError(\"No patient location specified\")\n",
    "        if hospital_loc is None:\n",
    "            raise ValueError(\"No hospital location specified\")\n",
    "\n",
    "        pickup_problem = PickupProblem(fire_grid.shape, robot_loc, patient_loc,\n",
    "                                       hospital_loc, one_ways)\n",
    "        fire_process = FireProcess(fire_grid, **fire_process_kargs)\n",
    "        return cls(pickup_problem, fire_process, **kwargs)\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class FireMDP(FireProblemCommon, MDP):\n",
    "    \"\"\"The completely observable fire problem.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def initial(self) -> FireMDPState:\n",
    "        return FireMDPState(*dataclasses.astuple(self.pickup_problem.initial),\n",
    "                            self.fire_process.initial_fire_grid)\n",
    "\n",
    "    def actions(self, state: State) -> Iterable[Action]:\n",
    "        return self.pickup_problem.actions(state)\n",
    "\n",
    "    def step(self, state: FireMDPState, action: Action) -> FireMDPState:\n",
    "        return FireMDPState(\n",
    "            *dataclasses.astuple(self.pickup_problem.step(state, action)),\n",
    "            self.fire_process.sample(state.fire_grid))\n",
    "\n",
    "    def render(self, state: FireMDPState, ax=None):\n",
    "        self.pickup_problem.render(state, ax=ax)\n",
    "        self.fire_process.render(state.fire_grid, ax=ax)\n",
    "\n",
    "\n",
    "def get_problem(name: str) -> MDP:\n",
    "    \"\"\"Return a problem instance by name.\"\"\"\n",
    "\n",
    "    params = {\n",
    "        \"maze\":\n",
    "            dict(env_s=\"\"\"\\\n",
    "                |R < . X . > H|\n",
    "                |            X|\n",
    "                |. X .   .   .|\n",
    "                |        X   X|\n",
    "                |. X .   . X .|\n",
    "                |    X   ^    |\n",
    "                |. X .   . X .|\n",
    "                |    X   ^   ^|\n",
    "                |P   . > . > .|\n",
    "                \"\"\",\n",
    "                 fire_process_kargs=dict(fire_weights=np.array([\n",
    "                     [0, 1, 0],\n",
    "                     [1, 10, 1],\n",
    "                     [0, 1, 0],\n",
    "                 ])),\n",
    "                 _horizon=20),\n",
    "        \"just_wait\":\n",
    "            dict(env_s=\"\"\"\\\n",
    "                |.   R   .   H|\n",
    "                |X   v   X   ^|\n",
    "                |. X . X .   .|\n",
    "                |    v       ^|\n",
    "                |. X . X .   .|\n",
    "                |    v       ^|\n",
    "                |. X . X .   .|\n",
    "                |    v       ^|\n",
    "                |F X . X F   .|\n",
    "                |    v       ^|\n",
    "                |. X . X .   F|\n",
    "                |    v       ^|\n",
    "                |. X . X .   .|\n",
    "                |X   v   X   ^|\n",
    "                |P   .   .   .|\n",
    "                \"\"\",\n",
    "                 fire_process_kargs=dict(fire_weights=np.array([\n",
    "                     [0, 1, 0],\n",
    "                     [1, 20, 1],\n",
    "                     [0, 1, 0],\n",
    "                 ]))),\n",
    "        \"the_circle\":\n",
    "            dict(env_s=\"\"\"\\\n",
    "                |R   .   .   H|\n",
    "                |    X   X   v|\n",
    "                |. X .   . X .|\n",
    "                |            v|\n",
    "                |. X F   . X .|\n",
    "                |            v|\n",
    "                |. X F   . X .|\n",
    "                |    X   X   v|\n",
    "                |P   . < . < .|\n",
    "                \"\"\",\n",
    "                 fire_process_kargs=dict(fire_weights=np.array([\n",
    "                     [0, 0, 0],\n",
    "                     [0, 1, 0],\n",
    "                     [0, 0, 0],\n",
    "                 ]))),\n",
    "        \"the_choice\":\n",
    "            dict(\n",
    "                env_s=\"\"\"\\\n",
    "                |.   .   F   F   F   F|\n",
    "                |.   X   X   X   X   X|\n",
    "                |. X .   .   .   .   .|\n",
    "                |X                    |\n",
    "                |R > .   .   F   .   .|\n",
    "                |v                    |\n",
    "                |. X .   .   .   .   .|\n",
    "                |v   X   X   X   X   v|\n",
    "                |. X .   F   F   . X .|\n",
    "                |v                   v|\n",
    "                |. X F   .   .   . X .|\n",
    "                |v                   v|\n",
    "                |. X F   .   .   . X .|\n",
    "                |v                   v|\n",
    "                |. X F   F   .   . X .|\n",
    "                |v   X   X   X   X   v|\n",
    "                |. > . > H   P < . < .|\n",
    "                \"\"\",\n",
    "                fire_process_kargs=dict(fire_weights=np.array([\n",
    "                    [0, 1, 0],\n",
    "                    [1, 4, 1],\n",
    "                    [0, 1, 0],\n",
    "                ]),),\n",
    "            )\n",
    "    }\n",
    "\n",
    "    if name not in params:\n",
    "        raise ValueError(f\"Unknown problem name: {name}\")\n",
    "\n",
    "    params[name][\"env_s\"] = textwrap.dedent(params[name][\"env_s\"])\n",
    "    return FireMDP.from_str(**params[name])\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"An agent that can act in an MDP or POMDP.\n",
    "\n",
    "    A derived agent must keep track of its own internal state.\n",
    "    \"\"\"\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the agent's internal state.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def act(self, obs: Union[Observation, State]) -> Action:\n",
    "        \"\"\"Return the agent's action given an observation. \n",
    "        For MDP agents, `obs` will be the complete state\"\"\"\n",
    "        ...\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class OpenLoopAgent(Agent):\n",
    "    \"\"\"Agent that just follows a fixed sequence of actions.\"\"\"\n",
    "\n",
    "    actions: Sequence[Action]\n",
    "\n",
    "    t: int = dataclasses.field(default=0, init=False)\n",
    "\n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "\n",
    "    def act(self, obs) -> Action:\n",
    "        del obs  # observation is not used\n",
    "        assert self.t < len(self.actions)\n",
    "        a = self.actions[self.t]\n",
    "        self.t += 1\n",
    "        return self.actions\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class RolloutLookaheadAgent(Agent):\n",
    "    \"\"\"MDP Agent that uses a rollout lookahead to decide what to do.\"\"\"\n",
    "\n",
    "    problem: MDP\n",
    "    n_rollout_per_action: int = 10\n",
    "\n",
    "    receding_horizon: int = None\n",
    "    t: int = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "\n",
    "    @property\n",
    "    def planning_horizon(self):\n",
    "        if self.receding_horizon is None:\n",
    "            return self.problem.horizon - self.t\n",
    "        return self.receding_horizon\n",
    "\n",
    "    def act(self, state: State) -> Action:\n",
    "        \"\"\"Return the action that maximizes the expected reward.\"\"\"\n",
    "        self.t += 1\n",
    "        actions = list(self.problem.actions(state))\n",
    "        random.shuffle(actions)\n",
    "        return max(actions, key=lambda a: self._rollout(state, a))\n",
    "\n",
    "    def _rollout(self, state: State, action: Action) -> float:\n",
    "        \"\"\"Return the expected reward of taking action in state.\"\"\"\n",
    "        return sum(\n",
    "            self._rollout_single(state, action) for _ in range(\n",
    "                self.n_rollout_per_action)) / self.n_rollout_per_action\n",
    "\n",
    "    def _rollout_single(self, state: State, action: Action) -> float:\n",
    "        \"\"\"simulate the utility of current state by taking a rollout policy.\"\"\"\n",
    "        total_reward = 0\n",
    "        disc = 1\n",
    "        t = 0\n",
    "        planning_horizon = self.planning_horizon\n",
    "        while t < planning_horizon and not self.problem.terminal(state):\n",
    "            if t > 0:\n",
    "                action = self.rollout_policy(state)\n",
    "            next_state = self.problem.step(state, action)\n",
    "            reward = self.problem.reward(state, action, next_state)\n",
    "            total_reward += disc * reward\n",
    "            state = next_state\n",
    "            disc = disc * self.problem.discount\n",
    "            t += 1\n",
    "        return total_reward\n",
    "\n",
    "    def rollout_policy(self, state: State) -> Action:\n",
    "        \"\"\"Return the action to take in state during rollout. \n",
    "\n",
    "        Subclass may override to implement rollout policy with preferred actions.\"\"\"\n",
    "\n",
    "        return random.choice(list(self.problem.actions(state)))\n",
    "\n",
    "\n",
    "def benchmark_agent(problem: Union[MDP, POMDP],\n",
    "                    agent: Agent,\n",
    "                    n_repeats: int = 100,\n",
    "                    verbose: bool = False,\n",
    "                    max_steps: int = 50) -> List[float]:\n",
    "    \"\"\"Bencmark an agent on a problem by performing repeated experiments.\"\"\"\n",
    "    import tqdm\n",
    "    total_rewards = []\n",
    "    for _ in tqdm.tqdm(range(n_repeats)):\n",
    "        *_, total_reward = run_agent_on_problem(problem,\n",
    "                                                agent,\n",
    "                                                max_steps=max_steps,\n",
    "                                                verbose=verbose)\n",
    "        total_rewards.append(total_reward)\n",
    "    return total_rewards\n",
    "\n",
    "\n",
    "def compare_agents(problem: Union[POMDP, MDP],\n",
    "                   agents: Dict[str, Agent],\n",
    "                   n_repeats: int = 30,\n",
    "                   max_steps: int = 50,\n",
    "                   verbose: bool = False):\n",
    "    \"\"\"Compare the performance of multiple agents on a problem.\n",
    "\n",
    "    You probably want to extend this function for more detailed analysis of performance.\n",
    "\n",
    "    Args:\n",
    "        problem: The problem to run the agents on.\n",
    "        agents: A dictionary mapping agent names to agents.\n",
    "        n_repeats: The number of experiments to run for each setting.\n",
    "        max_steps: The maximum number of steps to run each experiment. Note that this is different from the horizon of the problem,\n",
    "            since we may do receding horizon planning, but don't want to run each epsiode forever.\n",
    "        verbose: If True, print the reward for each experiment.s\n",
    "    \"\"\"\n",
    "    for agent in agents:\n",
    "        if isinstance(agent, tuple):\n",
    "            agent, name = agent\n",
    "        else:\n",
    "            name = agent.__class__.__name__\n",
    "        print(f\"Running {name}...\")\n",
    "        rewards = benchmark_agent(problem,\n",
    "                                  agent,\n",
    "                                  max_steps=max_steps,\n",
    "                                  n_repeats=n_repeats,\n",
    "                                  verbose=verbose)\n",
    "        print(f\"Mean reward: {np.mean(rewards):.2f} +- {np.std(rewards):.2f}\")\n",
    "        print(f\"Median reward: {np.median(rewards):.2f}\")\n",
    "        print(f\"Min reward: {np.min(rewards):.2f}\")\n",
    "        print(f\"Max reward: {np.max(rewards):.2f}\")\n",
    "\n",
    "\n",
    "def run_agent_on_problem(\n",
    "    problem: Union[MDP, POMDP],\n",
    "    agent: Agent,\n",
    "    verbose: bool = True,\n",
    "    max_steps: int = np.inf,\n",
    ") -> Tuple[Sequence[State], Sequence[Action], float]:\n",
    "    \"\"\"Runs the agent on the problem and returns the trajectory.\"\"\"\n",
    "    agent.reset()\n",
    "    state = problem.initial\n",
    "    obs = problem.get_observation(state)\n",
    "    state_sequence = [state]\n",
    "    action_sequence = []\n",
    "    total_reward = 0\n",
    "    while not problem.terminal(state) and len(state_sequence) < min(\n",
    "            problem.horizon, max_steps):\n",
    "        action = agent.act(obs)\n",
    "        next_state = problem.step(state, action)\n",
    "        reward = problem.reward(state, action, next_state)\n",
    "        total_reward += reward * problem.discount**len(state_sequence)\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"Action={action} reward={reward} total_reward={total_reward}\")\n",
    "        obs = problem.get_observation(next_state)\n",
    "        state = next_state\n",
    "        action_sequence.append(action)\n",
    "        state_sequence.append(state)\n",
    "    return state_sequence, action_sequence, total_reward\n",
    "\n",
    "\n",
    "def animate_trajectory(problem: Union[MDP, POMDP],\n",
    "                       trajectory: Tuple[Sequence[State], Sequence[Action]]):\n",
    "    \"\"\"Visualizes a trajectory.\n",
    "\n",
    "    Args:\n",
    "        problem: The problem.\n",
    "        trajectory: A tuple of state and action sequences.\n",
    "\n",
    "    Returns:\n",
    "        A matplotlib animation.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.animation\n",
    "\n",
    "    state_sequence, action_sequence, *_ = trajectory\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    total_reward = 0.\n",
    "\n",
    "    def animate(i):\n",
    "        ax.clear()\n",
    "        ax.set_aspect('equal')\n",
    "        nonlocal total_reward\n",
    "        if i == 0:\n",
    "            total_reward = 0\n",
    "            ax.set_title(f\"Step {i}: begin, total_reward={total_reward:.2f}\")\n",
    "        elif i < len(state_sequence):\n",
    "            action = action_sequence[i - 1]\n",
    "            reward = problem.reward(state_sequence[i - 1], action,\n",
    "                                    state_sequence[i])\n",
    "            total_reward += reward * problem.discount**i\n",
    "            ax.set_title(\n",
    "                f\"Step {i}: action={action}, \"\n",
    "                f\"reward={reward:.2f}, total_reward={total_reward:.2f}\")\n",
    "\n",
    "        problem.render(state_sequence[i], ax=ax)\n",
    "\n",
    "    anim = matplotlib.animation.FuncAnimation(fig,\n",
    "                                              animate,\n",
    "                                              frames=len(state_sequence),\n",
    "                                              interval=500)\n",
    "    return anim\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True, eq=True)\n",
    "class FirePOMDPState(FireMDPState):\n",
    "    \"\"\"A state in the Fire POMDP.\"\"\"\n",
    "\n",
    "    drone_loc: Tuple[int, int]\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, FireMDPState):\n",
    "            return False\n",
    "        return (self.robot_loc == other.robot_loc and\n",
    "                self.carried_patient == other.carried_patient and\n",
    "                np.array_equal(self.fire_grid, other.fire_grid))\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((super().__hash__(), self.drone_loc))\n",
    "\n",
    "\n",
    "class FirePOMDPObservation(np.ndarray):\n",
    "    \"\"\"Observation of the fire grid at the drone's location. \n",
    "\n",
    "    For each entry in the fire grid:\n",
    "        0 = no fire\n",
    "        1 = fire\n",
    "        np.nan = not observed\n",
    "    \"\"\"\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.tobytes())\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return np.array_equal(self, other, equal_nan=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def unknown(shape: Tuple[int, int]) -> \"FirePOMDPObservation\":\n",
    "        \"\"\"Returns an completely unknown observation grid of the given shape.\"\"\"\n",
    "        return np.full(shape, np.nan).view(FirePOMDPObservation)\n",
    "\n",
    "\n",
    "def mask_centered_at(shape: Tuple[int, int], loc: Tuple[int, int],\n",
    "                     dist: int) -> np.ndarray:\n",
    "    \"\"\"Return a mask array centered at loc and extending dist away to each direction.\"\"\"\n",
    "    mask = np.zeros(shape, dtype=bool)\n",
    "    mask[max(0, loc[0] - dist):min(shape[0], loc[0] + dist + 1),\n",
    "         max(0, loc[1] - dist):min(shape[1], loc[1] + dist + 1)] = True\n",
    "    return mask\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class FirePOMDP(FireProblemCommon, POMDP):\n",
    "    \"\"\"A POMDP version of the Fire problem.\n",
    "\n",
    "    In this setup, prior to each episode start, fire propagations for an unknown amount of time \n",
    "    according to the fire process dynamics as in the MDP case.\n",
    "    And when the epsiode starts, fire stops propagating. \n",
    "    But, the agent does not know where is fire. \n",
    "    It can only observe the fire grid cells within certain distances to the robot and the drone.\n",
    "    \"\"\"\n",
    "    # Fire has propogated ~Geomtric(initial_fire_spread_param) - 1 number of steps\n",
    "    # Must be in (0, 1], where 1 means the fire never spreads even before the agent starts\n",
    "    initial_fire_spread_param: float = 1\n",
    "\n",
    "    # robot can see this many cells away\n",
    "    # 0 means the robot can only see the cell it is in\n",
    "    robot_view_distance: int = 1\n",
    "\n",
    "    # drone can see this many cells away\n",
    "    # 0 means the drone can only see the cell it is in\n",
    "    drone_view_distance: int = 0\n",
    "\n",
    "    # drone can fly this many cells away\n",
    "    drone_fly_distance: float = 3\n",
    "\n",
    "    @property\n",
    "    def initial_drone_loc(self):\n",
    "        \"\"\"Initially the drone starts off at the same location as the robot.\"\"\"\n",
    "        return self.initial_robot_loc\n",
    "\n",
    "    @property\n",
    "    def initial(self) -> FirePOMDPState:\n",
    "        # Fire has propagated for an unknown amount of time, before the agent starts\n",
    "        spread_time = self.fire_process.rng.geometric(\n",
    "            self.initial_fire_spread_param) - 1\n",
    "        fire_grid = self.fire_process.initial_fire_grid\n",
    "        for _ in range(spread_time):\n",
    "            fire_grid = self.fire_process.sample(fire_grid)\n",
    "\n",
    "        return FirePOMDPState(*dataclasses.astuple(self.pickup_problem.initial),\n",
    "                              fire_grid=fire_grid,\n",
    "                              drone_loc=self.initial_drone_loc)\n",
    "\n",
    "    def drone_actions(self, robot_loc: Tuple[int, int]) -> List[Action]:\n",
    "        \"\"\"A drone can fly to any cell within the flying distance.\"\"\"\n",
    "        # return state.robot_loc\n",
    "        grid_coords = itertools.product(range(self.grid_shape[0]),\n",
    "                                        range(self.grid_shape[1]))\n",
    "        return [(r, c)\n",
    "                for r, c in grid_coords\n",
    "                if (math.hypot(robot_loc[0] - r, robot_loc[1] -\n",
    "                               c) <= self.drone_fly_distance)]\n",
    "\n",
    "    def actions(self, state: FirePOMDPState) -> Iterable[Action]:\n",
    "        \"\"\"The robot can move to a neighboring cell and the drone can move to any cell.\"\"\"\n",
    "        for robot_act in self.pickup_problem.actions(state):\n",
    "            robot_loc = self.pickup_problem.step(state, robot_act).robot_loc\n",
    "            for drone_act in self.drone_actions(robot_loc):\n",
    "                yield (robot_act, drone_act)\n",
    "\n",
    "    def step(self, state: FirePOMDPState, action: Action) -> FirePOMDPState:\n",
    "        robot_action, drone_action = action\n",
    "        return FirePOMDPState(\n",
    "            *dataclasses.astuple(self.pickup_problem.step(state, robot_action)),\n",
    "            state.fire_grid, drone_action)\n",
    "\n",
    "    def get_observation(self, state: FirePOMDPState) -> FirePOMDPObservation:\n",
    "        \"\"\"Returns the observation of the fire grid.\"\"\"\n",
    "        # Create a mask around the self.robot_view_distance of the robot\n",
    "        robot_mask = mask_centered_at(self.grid_shape, state.robot_loc,\n",
    "                                      self.robot_view_distance)\n",
    "        # Create a mask around the self.drone_view_distance of the drone\n",
    "        drone_mask = mask_centered_at(self.grid_shape, state.drone_loc,\n",
    "                                      self.drone_view_distance)\n",
    "        # Combine the masks\n",
    "        mask = np.logical_or(robot_mask, drone_mask)\n",
    "        # Return the masked fire grid\n",
    "        return np.where(mask, state.fire_grid,\n",
    "                        np.nan).view(FirePOMDPObservation)\n",
    "\n",
    "    def render(self, state: FireMDPState, ax=None):\n",
    "        \"\"\"Render the fire grid and the robot and drone locations, \n",
    "        then highlight the observed region.\"\"\"\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "        if ax is None:\n",
    "            ax = plt.gca()\n",
    "\n",
    "        self.pickup_problem.render(state, ax=ax)\n",
    "        self.fire_process.render(state.fire_grid, ax=ax)\n",
    "\n",
    "        overlay = np.isnan(self.get_observation(state)).astype(float)\n",
    "        # Draw gray overlay for the observed location\n",
    "        ax.imshow(overlay,\n",
    "                  alpha=overlay * 0.5,\n",
    "                  cmap=\"binary\",\n",
    "                  vmin=0,\n",
    "                  vmax=1,\n",
    "                  interpolation='nearest')\n",
    "        # Draw a blue rectangle around the drone location to highlight it\n",
    "        drone_rect = plt.Rectangle(\n",
    "            (state.drone_loc[1] - 0.5, state.drone_loc[0] - 0.5),\n",
    "            1,\n",
    "            1,\n",
    "            fill=False,\n",
    "            edgecolor='blue',\n",
    "            lw=4)\n",
    "        ax.add_patch(drone_rect)\n",
    "\n",
    "\n",
    "def get_problem_part2(name: str) -> FirePOMDP:\n",
    "    \"\"\"Return a problem instance by name.\"\"\"\n",
    "\n",
    "    params = {\n",
    "        \"only_fire\":\n",
    "            dict(\n",
    "                env_s=\"\"\"\\\n",
    "                |R   .   H|\n",
    "                |         |\n",
    "                |.   .   .|\n",
    "                |         |\n",
    "                |.   F   .|\n",
    "                |         |\n",
    "                |P   .   F|\n",
    "                \"\"\",\n",
    "                fire_process_kargs=dict(fire_weights=np.array([\n",
    "                    [0, 1, 0],\n",
    "                    [1, 10, 1],\n",
    "                    [0, 1, 0],\n",
    "                ])),\n",
    "                initial_fire_spread_param=0.3,\n",
    "            ),\n",
    "        \"the_circle2\":\n",
    "            dict(\n",
    "                env_s=\"\"\"\\\n",
    "                |R   .   H|\n",
    "                |    X    |\n",
    "                |. X . X .|\n",
    "                |         |\n",
    "                |F X F X .|\n",
    "                |    X    |\n",
    "                |P   .   .|\n",
    "                \"\"\",\n",
    "                fire_process_kargs=dict(fire_weights=np.array([\n",
    "                    [0, 1, 0],\n",
    "                    [1, 4, 1],\n",
    "                    [0, 1, 0],\n",
    "                ])),\n",
    "                initial_fire_spread_param=\n",
    "                1.,  # fire doesn't spread before the agent starts\n",
    "            ),\n",
    "        \"the_choice2\":\n",
    "            dict(env_s=\"\"\"\\\n",
    "                |.   .   F   F   F|\n",
    "                |X   X   X   X   X|\n",
    "                |. > .   .   .   .|\n",
    "                |                 |\n",
    "                |. > .   .   F   .|\n",
    "                |                 |\n",
    "                |R > .   .   .   P|\n",
    "                |    X   X   X    |\n",
    "                |. > .   F   .   H|\n",
    "                \"\"\",\n",
    "                 fire_process_kargs=dict(fire_weights=np.array([\n",
    "                     [0, 1, 0],\n",
    "                     [1, 5, 1],\n",
    "                     [0, 1, 0],\n",
    "                 ]),),\n",
    "                 initial_fire_spread_param=0.4),\n",
    "    }\n",
    "\n",
    "    if name not in params:\n",
    "        raise ValueError(f\"Unknown problem name: {name}\")\n",
    "\n",
    "    params[name][\"env_s\"] = textwrap.dedent(params[name][\"env_s\"])\n",
    "    return FirePOMDP.from_str(**params[name])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a33ec1e",
   "metadata": {},
   "source": [
    "## Determinized Min-cost Path Problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fede8369",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a6a83c",
   "metadata": {},
   "source": [
    "\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ab9f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True, eq=True, order=True)\n",
    "class DeterminizedFireMDPState(PickupProblemState):\n",
    "    \"\"\"A state for the DeterminizedFireMDP.\n",
    "\n",
    "    The state is a pair of the PickupProblemState and a time step $t$.\n",
    "    \"\"\"\n",
    "    time: int = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e85f02",
   "metadata": {},
   "source": [
    "\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec0c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scipy.special\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class FireMRF:\n",
    "    \"\"\"A Markov Random Field for the fire grid.\"\"\"\n",
    "\n",
    "    unitary_potentials: np.ndarray\n",
    "    correlation_potential: np.ndarray\n",
    "\n",
    "    @cached_property\n",
    "    def log_unitary_potentials(self) -> np.ndarray:\n",
    "        \"\"\"Return the log of the unitary potentials.\"\"\"\n",
    "        return np.log(self.unitary_potentials)\n",
    "\n",
    "    @cached_property\n",
    "    def log_correlation_potential(self) -> np.ndarray:\n",
    "        \"\"\"Return the log of the correlation potentials.\"\"\"\n",
    "        return np.log(self.correlation_potential)\n",
    "\n",
    "    @staticmethod\n",
    "    def default(shape: Tuple[int, int]) -> \"FireMRF\":\n",
    "        \"\"\"Return a default MRF for a fire grid of the given shape.\"\"\"\n",
    "        return FireMRF(\n",
    "            unitary_potentials=np.full((*shape, 2), 0.5),\n",
    "            correlation_potential=np.array([[0.7, 0.3], [0.3, 0.7]]),\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def grid_shape(self) -> Tuple[int, int]:\n",
    "        \"\"\"Return the shape of the fire grid.\"\"\"\n",
    "        return self.unitary_potentials.shape[:2]\n",
    "\n",
    "\n",
    "def shift(array: np.ndarray,\n",
    "          offset: Sequence[int],\n",
    "          constant_values: float = 0) -> np.ndarray:\n",
    "    \"\"\"Returns copy of array shifted by offset, with fill using constant.\n",
    "\n",
    "    Taken from https://stackoverflow.com/a/70297929.\n",
    "    \"\"\"\n",
    "    array = np.asarray(array)\n",
    "    offset = np.atleast_1d(offset)\n",
    "    assert len(offset) == array.ndim\n",
    "    new_array = np.empty_like(array)\n",
    "\n",
    "    def slice1(o):\n",
    "        return slice(o, None) if o >= 0 else slice(0, o)\n",
    "\n",
    "    new_array[tuple(slice1(o) for o in offset)] = (array[tuple(\n",
    "        slice1(-o) for o in offset)])\n",
    "\n",
    "    for axis, o in enumerate(offset):\n",
    "        new_array[(slice(None),) * axis + (\n",
    "            slice(0, o) if o >= 0 else slice(o, None),)] = constant_values\n",
    "\n",
    "    return new_array\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class LBPResult:\n",
    "    marginals: np.ndarray\n",
    "    niter: int\n",
    "    converged: bool\n",
    "    msgs: Optional[np.ndarray] = None\n",
    "\n",
    "\n",
    "def fire_mrf_lbp_marginals(log_unitary_potentials: np.ndarray,\n",
    "                           log_correlation_potential: np.ndarray,\n",
    "                           initial_msgs: Optional[np.ndarray] = None,\n",
    "                           max_iters: int = 20,\n",
    "                           return_msgs: bool = False,\n",
    "                           rtol=1e-5) -> LBPResult:\n",
    "    \"\"\"Run loopy belief propagation on the fire MRF.\n",
    "\n",
    "    This procedure assumes a 2D grid of random variables of shape (h, w). with\n",
    "    pairwise interactions between adjacent variables and a grid of unitary potentials.\n",
    "    This function aims to be as efficient as possible, so it does not use \n",
    "    any loops. It uses vectorized numpy to compute the messages and the marginals.\n",
    "\n",
    "    Args:\n",
    "        log_unitary_potentials: The log of the unitary potentials of a FireMRF.\n",
    "        log_correlation_potential: The log of the correlation potential of a FireMRF.\n",
    "        initial_msgs: The initial messages. If None, will be initialized automatically.\n",
    "        max_iters: The maximum number of iterations to run.\n",
    "        return_msgs: Whether to return the passed messages, useful for \"warm-starting\" the messages.\n",
    "        rtol: The relative tolerance for convergence test.\n",
    "\n",
    "    Returns:\n",
    "        LBPResult: The marginal probabilities of the random variables.\n",
    "    \"\"\"\n",
    "    h, w, x = log_unitary_potentials.shape\n",
    "\n",
    "    if initial_msgs is None:\n",
    "        msgs = [np.zeros((h, w, x)) for _ in range(4)]\n",
    "    else:\n",
    "        msgs = initial_msgs\n",
    "\n",
    "    del initial_msgs\n",
    "\n",
    "    def _msgs_incoming(msgs):\n",
    "        dirs = ((0, 1, 0), (1, 0, 0), (-1, 0, 0), (0, -1, 0))\n",
    "        return [shift(msg, dir) for msg, dir in zip(msgs, dirs)]\n",
    "\n",
    "    logsumexp = scipy.special.logsumexp\n",
    "    log_correlation_pot_rev = log_correlation_potential.T\n",
    "\n",
    "    log_rtol = np.log1p(rtol)\n",
    "\n",
    "    prev_sum_msgs_incoming = None\n",
    "\n",
    "    converged = False\n",
    "    for niter in range(max_iters):\n",
    "\n",
    "        # incoming messages from all neighbors, shape: [(h, w, x)] * 4\n",
    "        msgs_incoming = _msgs_incoming(msgs)\n",
    "        sum_msgs_incoming = sum(msgs_incoming)\n",
    "\n",
    "        # Convergence test\n",
    "        if prev_sum_msgs_incoming is not None:\n",
    "            rerr = np.linalg.norm(\n",
    "                (sum_msgs_incoming - prev_sum_msgs_incoming).ravel(), ord=1)\n",
    "            if rerr < log_rtol:\n",
    "                converged = True\n",
    "                break\n",
    "        prev_sum_msgs_incoming = sum_msgs_incoming\n",
    "\n",
    "        # Temp terms involving all incoming messages\n",
    "        unitary_term = (log_unitary_potentials[:, :, :, np.newaxis] +\n",
    "                        sum_msgs_incoming[:, :, :, np.newaxis])\n",
    "        tmp1 = unitary_term + log_correlation_potential  # shape: (h, w, xi, xj)\n",
    "        tmp2 = unitary_term + log_correlation_pot_rev  # shape: (h, w, xi, xj)\n",
    "\n",
    "        # compute next messages\n",
    "        msgs = [\n",
    "            logsumexp(t - mi[..., np.newaxis], axis=2)\n",
    "            for t, mi in zip([tmp1, tmp1, tmp2, tmp2], reversed(msgs_incoming))\n",
    "        ]\n",
    "\n",
    "        # Normalize msgs\n",
    "        msgs -= logsumexp(msgs, axis=-1, keepdims=True)\n",
    "\n",
    "    # incoming messages from all neighbors, shape: (h, w, x)\n",
    "    sum_msgs_incoming = sum(_msgs_incoming(msgs))\n",
    "    marginals = log_unitary_potentials + sum_msgs_incoming\n",
    "    marginals -= logsumexp(marginals, axis=-1, keepdims=True)\n",
    "    marginals = np.exp(marginals)\n",
    "\n",
    "    return LBPResult(marginals=marginals,\n",
    "                     niter=niter,\n",
    "                     converged=converged,\n",
    "                     msgs=msgs if return_msgs else None)\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class FireMRFGibbsSampler:\n",
    "    \"\"\"A Gibbs sampler for the FireMRF.\"\"\"\n",
    "\n",
    "    fire_mrf: FireMRF\n",
    "\n",
    "    initial_fire_grid: Optional[np.ndarray] = None\n",
    "    observation_grid: Optional[np.ndarray] = None\n",
    "\n",
    "    burn_in_steps: int = 200\n",
    "    n_parellel_chains: int = 5\n",
    "    num_steps_per_sample: int = 100\n",
    "\n",
    "    def sampling_step(self, fire_grid: np.ndarray,\n",
    "                      var_loc: Tuple[int, int]) -> np.ndarray:\n",
    "        \"\"\"Performs a single step of Gibbs sampling on the fire grid.\n",
    "\n",
    "        Given the current configuration of the fire grid, this function samples a \n",
    "        new configuration for the variable at `var_loc`, conditioned on the neighbors \n",
    "        of the variable. It returns the new fire grid.\n",
    "\n",
    "        Args:\n",
    "            fire_grid: The current state of the fire grid.\n",
    "            var_loc: The location of the variable to update.\n",
    "        \"\"\"\n",
    "        w, h = fire_grid.shape\n",
    "        r, c = var_loc\n",
    "        neighbor_pots = []\n",
    "        log_correlation_pot = self.fire_mrf.log_correlation_potential\n",
    "        if r - 1 >= 0:\n",
    "            neighbor_pots.append(log_correlation_pot[fire_grid[r - 1, c], :])\n",
    "        if r + 1 < w:\n",
    "            neighbor_pots.append(log_correlation_pot[:, fire_grid[r + 1, c]])\n",
    "        if c - 1 >= 0:\n",
    "            neighbor_pots.append(log_correlation_pot[fire_grid[r, c - 1], :])\n",
    "        if c + 1 < h:\n",
    "            neighbor_pots.append(log_correlation_pot[:, fire_grid[r, c + 1]])\n",
    "        neighbor_pots = np.sum(neighbor_pots, axis=0)\n",
    "        log_posterior = (neighbor_pots +\n",
    "                         self.fire_mrf.log_unitary_potentials[var_loc])\n",
    "        log_posterior -= scipy.special.logsumexp(log_posterior)\n",
    "        new_fire_grid = fire_grid.copy()\n",
    "        new_val = np.random.choice(2, p=np.exp(log_posterior))\n",
    "        new_fire_grid[r, c] = new_val\n",
    "        return new_fire_grid\n",
    "\n",
    "    def samples_stream(self) -> Iterable[np.ndarray]:\n",
    "        \"\"\"Performs Gibbs sampling on a fire MRF, yields an infinite stream of samples.\"\"\"\n",
    "\n",
    "        if np.all(~np.isnan(self.observation_grid)):\n",
    "            # If we observed everything, don't need to sample!\n",
    "            while True:\n",
    "                yield self.observation_grid\n",
    "\n",
    "        # If no initial fire grid is provided, we initialize it randomly\n",
    "        if self.initial_fire_grid is None:\n",
    "            fire_grids = list(\n",
    "                np.random.binomial(\n",
    "                    1,\n",
    "                    0.5,\n",
    "                    size=(self.n_parellel_chains, *self.fire_mrf.grid_shape),\n",
    "                ))\n",
    "        else:\n",
    "            fire_grids = [\n",
    "                self.initial_fire_grid.copy()\n",
    "                for i in range(self.n_parellel_chains)\n",
    "            ]\n",
    "\n",
    "        if self.observation_grid is not None:\n",
    "            sample_coords = np.array(np.where(np.isnan(\n",
    "                self.observation_grid))).T\n",
    "            observation = self.observation_grid[~np.isnan(self.observation_grid\n",
    "                                                         )]\n",
    "            for fire_grid in fire_grids:\n",
    "                fire_grid[~np.isnan(self.observation_grid)] = observation\n",
    "        else:\n",
    "            sample_coords = np.meshgrid(\n",
    "                *[np.arange(s) for s in self.fire_mrf.grid_shape])\n",
    "\n",
    "        # Generate the infinite stream of samples\n",
    "        steps = 0\n",
    "        while True:\n",
    "            for i in range(self.n_parellel_chains):\n",
    "                r, c = sample_coords[np.random.choice(len(sample_coords))]\n",
    "                fire_grids[i] = self.sampling_step(fire_grids[i], (r, c))\n",
    "\n",
    "            steps += 1\n",
    "            if (steps > self.burn_in_steps and\n",
    "                    steps % self.num_steps_per_sample == 0):\n",
    "                for i in range(self.n_parellel_chains):\n",
    "                    yield fire_grids[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8cbdc4",
   "metadata": {},
   "source": [
    "### Question\n",
    "Please complete the implementation of `DeterminizedFireMDP`. In particular, you should:\n",
    "- Complete the function `fire_dist_at_time` to compute the log-likelihood of each cell being on fire at time $t$ given the true fire state at time $0$. \n",
    "- Using your implementation of `fire_dist_at_time`, complete the function `step_cost`.\n",
    "- Complete the rest of the `DeterminizedFireMDP` and code the heuristic function `h` based on description above.\n",
    "    \n",
    "\n",
    "For reference, our solution is **68** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9823e28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass(frozen=True)\n",
    "class DeterminizedFireMDP(PathCostProblem):\n",
    "    \"\"\"Determinized version of the fire MDP --- tries to find the solution path \n",
    "    that is most likely to succeed.    \n",
    "    \"\"\"\n",
    "    pickup_problem: PickupProblem\n",
    "    fire_process: FireProcess\n",
    "\n",
    "    # Additional cost for each step.\n",
    "    # Can be 0 but we might have 0-cost arcs if the success probability is 1.\n",
    "    action_cost = 1e-6\n",
    "\n",
    "    # Use this to cache precomputed fire distributions, so we don't have to recompute them.\n",
    "    fire_dists_cache: Dict[int, np.ndarray] = dataclasses.field(\n",
    "        init=False,\n",
    "        default_factory=dict,\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert (self.pickup_problem.grid_shape ==\n",
    "                self.fire_process.initial_fire_grid.shape)\n",
    "\n",
    "    @property\n",
    "    def initial(self) -> DeterminizedFireMDPState:\n",
    "        return DeterminizedFireMDPState(\n",
    "            *dataclasses.astuple(self.pickup_problem.initial),\n",
    "            time=0,\n",
    "        )\n",
    "\n",
    "    def actions(self, state: DeterminizedFireMDPState) -> Iterable[Action]:\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    def step(self, state: DeterminizedFireMDPState,\n",
    "             action: Action) -> State:\n",
    "        \"\"\"We automatically pick up patient if we're on that square.\"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    def goal_test(self, state: DeterminizedFireMDPState) -> bool:\n",
    "        \"\"\"True if at hospital and holding patient.\"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    def step_cost(self, state1: DeterminizedFireMDPState, action: Action,\n",
    "                  state2: DeterminizedFireMDPState) -> float:\n",
    "        \"\"\"Induce a small action_cost and the negative log-likelihood of no fire.\"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    def fire_dist_at_time(self, t: int) -> np.ndarray:\n",
    "        \"\"\"Return the marginal distribution of fire grid at time $t$.\"\"\"\n",
    "\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "        if t not in self.fire_dists_cache:\n",
    "            if t == 0:\n",
    "                dist = ...  # TODO: Implement this.\n",
    "            else:\n",
    "                dist = ...  # TODO: Implement this.\n",
    "            self.fire_dists_cache[t] = dist\n",
    "        return self.fire_dists_cache[t]\n",
    "\n",
    "    def h(self, state: DeterminizedFireMDPState) -> float:\n",
    "        \"\"\"heuristic based on the manhattan distance to the patient and hospital.\"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18a0d51",
   "metadata": {},
   "source": [
    "## Determinized Fire MDP Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9090201",
   "metadata": {},
   "source": [
    "### Question\n",
    "Please complete the implementation of `FireMDPDeterminizedAStarAgent`. \n",
    "Note that we have filled in most of the implementation for you --- including \n",
    "the call to `run_astar_search` from HW01. \n",
    "All you need to implement is the `determinized_problem` method.\n",
    "\n",
    "\n",
    "For reference, our solution is **27** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be4a18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass(frozen=True)\n",
    "class FireMDPDeterminizedAStarAgent(Agent):\n",
    "    \"\"\"Agent that uses A* to plan a path to the goal in a determinized \n",
    "    version of the problem. Does not need any internal state since we \n",
    "    re-determinize the problem at each step.\n",
    "    \"\"\"\n",
    "\n",
    "    problem: FireMDP\n",
    "    step_budget: int = 10000\n",
    "\n",
    "    def determinized_problem(self,\n",
    "                             state: FireMDPState) -> DeterminizedFireMDP:\n",
    "        \"\"\"Returns a determinized approximation of the fire MDP.\"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    def act(self, state: FireMDPState) -> Action:\n",
    "        problem = self.determinized_problem(state)\n",
    "        try:\n",
    "            plan = run_astar_search(problem, self.step_budget)\n",
    "        except SearchFailed:\n",
    "            print(\"Search failed, performing a random action\")\n",
    "            return random.choice(list(self.problem.actions(state)))\n",
    "        return plan[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9c3f8e",
   "metadata": {},
   "source": [
    "## MCTS Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08dbf44",
   "metadata": {},
   "source": [
    "### Question\n",
    "Please complete the implementation of `MCTSAgent`. \n",
    "\n",
    "_Hint: You can pass in the `self.planning_horizon` to `run_mcts_search`, \n",
    "to handle both infinite-horizon problems (by receding-horizon planning) and finite-horizon problems._\n",
    "\n",
    "\n",
    "For reference, our solution is **42** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146bda68",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class MCTSAgent(Agent):\n",
    "    \"\"\"Agent that uses Monte Carlo Tree Search to plan a path to the goal.\n",
    "\n",
    "    The agent simply wraps `run_mcts_search`, and it should work for any MDP.\n",
    "    \"\"\"\n",
    "\n",
    "    problem: MDP\n",
    "\n",
    "    # An optional receding horizon to use for the planning\n",
    "    # If not provided, the problem must have a finite horizon\n",
    "    receding_horizon: Optional[int] = None\n",
    "\n",
    "    C: float = np.sqrt(2)\n",
    "    iteration_budget: int = 1000\n",
    "\n",
    "    t: int = dataclasses.field(default=0, init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.receding_horizon is None:\n",
    "            assert self.problem.horizon != np.inf\n",
    "\n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "\n",
    "    @property\n",
    "    def planning_horizon(self) -> int:\n",
    "        \"\"\"Returns the planning horizon for the current time step.\"\"\"\n",
    "        if self.receding_horizon is None:\n",
    "            return self.problem.horizon - self.t\n",
    "        return self.receding_horizon\n",
    "\n",
    "    def act(self, state: State) -> Action:\n",
    "        \"\"\"Return the action to take at state.\"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98da903",
   "metadata": {},
   "source": [
    "## Exact Marginalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a36d0c2",
   "metadata": {},
   "source": [
    "### Question\n",
    "Write a function `fire_mrf_exact_marginalize` to compute the exact marginals by multiplying all potentials and then marginalizing.\n",
    "We don't expect this function to work for large grids, but it should work for grids that are smaller than 3 x 3.\n",
    "\n",
    "For reference, our solution is **37** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5657a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fire_mrf_exact_marginalize(fire_mrf: FireMRF) -> np.ndarray:\n",
    "    \"\"\"Computes the exact marginal distributions of a given FireMRF.\n",
    "\n",
    "    This function simply sums over all possible configurations of the fire grid to compute the\n",
    "    marginal distributions of each variables. It won't work for large grids.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f7ccca",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d2bb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explicit_marginalize_test(fire_mrf_exact_marginalize, fire_mrf: FireMRF,\n",
    "                              results: np.ndarray) -> bool:\n",
    "    marginals = fire_mrf_exact_marginalize(fire_mrf)\n",
    "    assert np.allclose(marginals, results, atol=1e-4)\n",
    "\n",
    "explicit_marginalize_test(fire_mrf_exact_marginalize, FireMRF(unitary_potentials=np.array([[[0.5, 0.5],   [0.3, 0.7]]], dtype=np.float64), correlation_potential=np.array([[0.7, 0.3],  [0.3, 0.7]], dtype=np.float64)), np.array([0.58, 0.7 ], dtype=np.float64))\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad17322e",
   "metadata": {},
   "source": [
    "## Loopy Belief Propagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e86d77",
   "metadata": {},
   "source": [
    "### Question\n",
    "Our exact marginalization is very slow for larger grids. \n",
    "So instead, we will use loopy belief propagation to compute the marginals.\n",
    "\n",
    "We have provided you with a function `fire_mrf_lbp_marginals` that implements loopy belief \n",
    "propagation on a `FireMRF` to compute the marginals approximately.\n",
    "This function is the same as the belief propagation you saw from the lecture, except that it is \n",
    "works on our FireMRF model only. \n",
    "By specializing to our 2D grid model, we have taken care to make it very efficient \n",
    "by avoiding Python loops and using NumPy operations instead.\n",
    "But, this function does not handle any observation. \n",
    "\n",
    "Please complete the implementation of `fire_mrf_lbp_conditionals`.\n",
    "This function should take the same arguments as `fire_mrf_lbp_marginals`, with the addition of an\n",
    "argument `observations` that is a `FirePOMDPObservation`.\n",
    "A `FirePOMDPObservation` is just a 2d NumPy array --- please see the docstring \n",
    "for `FirePOMDPObservation` for more details.\n",
    "\n",
    "_Hints:_ \n",
    "- You can use `np.isnan(observation)` to find the indices of the unobserved cells.\n",
    "- You can reduce `fire_mrf_lbp_conditionals` into a call to `fire_mrf_lbp_marginals` by\n",
    "incorporating the observation into the unitary potentials.\n",
    "\n",
    "\n",
    "For reference, our solution is **17** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599de7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fire_mrf_lbp_conditionals(fire_mrf: FireMRF,\n",
    "                              observation: FirePOMDPObservation,\n",
    "                              max_iters: int = 20,\n",
    "                              return_msgs: bool = False,\n",
    "                              rtol=1e-5) -> np.ndarray:\n",
    "    \"\"\"Wrapper around `fire_mrf_lbp_marginals` that additionally handles \n",
    "    observation, to compute the conditional probabilities of a FireMRF.\n",
    "\n",
    "    This function incorprates the observation into the unitary potential, and then \n",
    "    runs loopy belief propagation on the resulting MRF.\n",
    "    In order to be computatioinally efficient, this function avoids the use of \n",
    "    Python loops, and uses numpy operations to prepare the unitary potentials.\n",
    "\n",
    "    Args:\n",
    "        fire_mrf: The FireMRF.\n",
    "        observation: The observed observation. shape (h, w)\n",
    "        max_iter, return_msgs, rtol: Same as `fire_mrf_lbp_marginals`.\n",
    "\n",
    "    Returns:\n",
    "        The marginal probabilities of fire in each cell, shape (h, w).\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656c4bb5",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5db39f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fire_mrf_lbp_test(fire_mrf_lbp_conditionals, fire_mrf: FireMRF,\n",
    "                      observation: np.ndarray, results: np.ndarray) -> bool:\n",
    "    marginals = fire_mrf_lbp_conditionals(fire_mrf, observation)\n",
    "    assert np.allclose(marginals, results, atol=1e-4)\n",
    "\n",
    "fire_mrf_lbp_test(fire_mrf_lbp_conditionals, FireMRF(unitary_potentials=np.array([[[0.5, 0.5],   [0.3, 0.7],   [0.5, 0.5]],   [[0.5, 0.5],   [0.5, 0.5],   [0.5, 0.5]],   [[0.5, 0.5],   [0.5, 0.5],   [0.5, 0.5]]], dtype=np.float64), correlation_potential=np.array([[0.7, 0.3],  [0.3, 0.7]], dtype=np.float64)), np.array([[    0., np.nan, np.nan],  [np.nan,     1., np.nan],  [np.nan, np.nan,     0.]], dtype=np.float64), np.array([[0.   , 0.7  , 0.58 ],  [0.5  , 1.   , 0.532],  [0.5  , 0.5  , 0.   ]], dtype=np.float64))\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfb26b8",
   "metadata": {},
   "source": [
    "## MLO Determinized FirePOMDP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b12ab6b",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3a7b46",
   "metadata": {},
   "source": [
    "\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7db7855",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True, eq=True, order=True)\n",
    "class FirePOMDPBeliefState(PickupProblemState):\n",
    "    fire_grid: FirePOMDPObservation\n",
    "    drone_loc: Tuple[int, int]\n",
    "\n",
    "\n",
    "class RewardProblem(MDP):\n",
    "    \"\"\"An abstract class for a finite-horizon reward problem, potentially discounted.\n",
    "\n",
    "    It is essentially a deterministic MDP.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class FirePOMDPBeliefStateAgent(Agent):\n",
    "    \"\"\"An abstract base agent that tracks the belief state at each step. \n",
    "\n",
    "    The agent tracks the belief state as a FirePOMDPBeliefState instance.\n",
    "    Further, it assumes that the fire is distributed according to a `FireMRF`.\n",
    "    \"\"\"\n",
    "\n",
    "    problem: FirePOMDP\n",
    "    fire_mrf: FireMRF\n",
    "\n",
    "    # An optional receding horizon to use for the planning\n",
    "    # If not provided, the problem must have a finite horizon\n",
    "    receding_horizon: Optional[int] = None\n",
    "\n",
    "    t: int = dataclasses.field(default=0, init=False)\n",
    "\n",
    "    belief_state: FirePOMDPBeliefState = dataclasses.field(\n",
    "        default=None,\n",
    "        init=False,\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.receding_horizon is None:\n",
    "            assert self.problem.horizon != np.inf\n",
    "\n",
    "    @property\n",
    "    def planning_horizon(self):\n",
    "        if self.receding_horizon is None:\n",
    "            return self.problem.horizon - self.t\n",
    "        return self.receding_horizon\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the agent to its initial state.\n",
    "\n",
    "        In particular, we need to:\n",
    "        - Reset the `self.t` to 0\n",
    "        - Reset the belief state to the initial belief state\n",
    "        \"\"\"\n",
    "        self.t = 0\n",
    "\n",
    "        # Initial belief state\n",
    "        fire_belief = FirePOMDPObservation.unknown(\n",
    "            self.problem.pickup_problem.grid_shape)\n",
    "        pickup_problem_initial = self.problem.pickup_problem.initial\n",
    "\n",
    "        # No fire at the robot's location\n",
    "        fire_belief[pickup_problem_initial.robot_loc] = 0\n",
    "\n",
    "        self.belief_state = FirePOMDPBeliefState(\n",
    "            *dataclasses.astuple(self.problem.pickup_problem.initial),\n",
    "            fire_grid=fire_belief,\n",
    "            drone_loc=self.problem.initial_drone_loc,\n",
    "        )\n",
    "\n",
    "    def act(self, obs: FirePOMDPObservation) -> Action:\n",
    "        \"\"\"Take an action while maintaining the belief state.\"\"\"\n",
    "        self.t += 1\n",
    "\n",
    "        # Update belief state by observation\n",
    "        next_fire_belief = np.where(\n",
    "            np.isnan(self.belief_state.fire_grid), obs,\n",
    "            self.belief_state.fire_grid).view(FirePOMDPObservation)\n",
    "        self.belief_state = dataclasses.replace(self.belief_state,\n",
    "                                                fire_grid=next_fire_belief)\n",
    "\n",
    "        robot_action, drone_action = self._act(self.belief_state)\n",
    "\n",
    "        # Update belief state by action\n",
    "        self.belief_state = FirePOMDPBeliefState(\n",
    "            *dataclasses.astuple(\n",
    "                self.problem.pickup_problem.step(self.belief_state,\n",
    "                                                 robot_action)),\n",
    "            self.belief_state.fire_grid,\n",
    "            drone_action,\n",
    "        )\n",
    "\n",
    "        return (robot_action, drone_action)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _act(self, belief_state: FirePOMDPBeliefState) -> Action:\n",
    "        \"\"\"Take an action given the current belief state.\n",
    "\n",
    "        Subclasses must implement this method.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "\n",
    "def firepomdp_random_rollout_policy(problem, state: FirePOMDPState) -> Action:\n",
    "    \"\"\"A random rollout policy for the fire POMDP.\n",
    "\n",
    "    This policy is different from the simple `random_rollout_policy` in that it \n",
    "    first uniformly samples a robot action and then samples a drone action that \n",
    "    is consistent with the robot action.\n",
    "    We do this because a robot action might have different number of available drone\n",
    "    actions. If we sampled uniformly from all possible actions, then some robot \n",
    "    action is preferred over others.\n",
    "\n",
    "    Args:\n",
    "        problem: a problem instance similar to the `FirePOMDP`. It must have an \n",
    "            `actions` method that returns a sequence of tuples of robot and \n",
    "            drone actions.\n",
    "        state: The current complete FirePOMDP state.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of robot action and drone action.\n",
    "    \"\"\"\n",
    "    actions = collections.defaultdict(list)\n",
    "    for ra, da in problem.actions(state):\n",
    "        actions[ra].append(da)\n",
    "    robot_action = random.choice(list(actions.keys()))\n",
    "    drone_action = random.choice(actions[robot_action])\n",
    "    return (robot_action, drone_action)\n",
    "\n",
    "\n",
    "def state_sampler(fire_mrf: FireMRF,\n",
    "                  belief_state: FirePOMDPBeliefState) -> Iterable[FireMDPState]:\n",
    "    \"\"\"Sample a stream of completely observed states from the belief state, \n",
    "    assuming that fire is distributed according to the given MRF.\n",
    "\n",
    "    Args:\n",
    "        fire_mrf: The MRF that describes the distribution of fire.\n",
    "        belief_state: A belief state.\n",
    "\n",
    "    Yields:\n",
    "        An infinite sequence complete state sampled from the belief state.\n",
    "    \"\"\"\n",
    "    gibbs = FireMRFGibbsSampler(fire_mrf=fire_mrf,\n",
    "                                observation_grid=belief_state.fire_grid)\n",
    "    for fire_belief in gibbs.samples_stream():\n",
    "        yield dataclasses.replace(belief_state, fire_grid=fire_belief)\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class FirePOMDPRolloutLookaheadAgent(FirePOMDPBeliefStateAgent):\n",
    "    \"\"\"POMDP Agent that uses a rollout lookahead to decide what to do.\"\"\"\n",
    "\n",
    "    n_rollout_per_action: int = 10\n",
    "\n",
    "    def _act(self, belief_state: FirePOMDPState) -> Action:\n",
    "        sampler = state_sampler(self.fire_mrf, belief_state)\n",
    "        # Initialize actions to the set of all possible actions\n",
    "        # Note that this is specific to the FirePOMDP\n",
    "        # In general, we cannot assume that the set of actions\n",
    "        # is the same for all states sampled from the current belief state.\n",
    "        state = next(sampler)\n",
    "        actions = collections.defaultdict(list)\n",
    "        for ra, da in problem.actions(state):\n",
    "            actions[ra].append(da)\n",
    "        robot_actions = list(actions.keys())\n",
    "        random.shuffle(robot_actions)\n",
    "        # Put back the state into the sampler so that we don't waste it\n",
    "        sampler = itertools.chain([state], sampler)\n",
    "\n",
    "        action_rewards = {}\n",
    "        for robot_action in robot_actions:\n",
    "            drone_actions = actions[robot_action]\n",
    "            total_rewards = []\n",
    "            for _ in range(self.n_rollout_per_action):\n",
    "                action = (robot_action, random.choice(drone_actions))\n",
    "                # Sample a state from the belief state\n",
    "                state = next(sampler)\n",
    "                total_rewards.append(self._rollout_single(state, action))\n",
    "            # Compute the average reward for this action\n",
    "            action_rewards[robot_action] = np.mean(total_rewards)\n",
    "\n",
    "        # Return the action with the highest average reward\n",
    "        best_robot_action = max(robot_actions, key=lambda a: action_rewards[a])\n",
    "        random_drone_action = random.choice(actions[best_robot_action])\n",
    "        return (best_robot_action, random_drone_action)\n",
    "\n",
    "    def _rollout_single(self, state: FireMDPState, action: Action) -> float:\n",
    "        \"\"\"simulate the utility of current state by taking a rollout policy.\"\"\"\n",
    "        total_reward = 0\n",
    "        disc = 1\n",
    "        t = 0\n",
    "        planning_horizon = self.planning_horizon\n",
    "        while t < planning_horizon and not self.problem.terminal(state):\n",
    "            if t > 0:\n",
    "                action = firepomdp_random_rollout_policy(self.problem, state)\n",
    "            next_state = self.problem.step(state, action)\n",
    "            reward = self.problem.reward(state, action, next_state)\n",
    "            total_reward += disc * reward\n",
    "            state = next_state\n",
    "            disc = disc * self.problem.discount\n",
    "            t += 1\n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30a978b",
   "metadata": {},
   "source": [
    "### Question\n",
    "Please complete the implementation of `MLODeterminiziedFirePOMDP`, \n",
    "a deterministic reward problem that is the MLO approximation of the `FirePOMDP`.\n",
    "\n",
    "Recall that in MLO approximation, the next state is determined by the most likely observation.\n",
    "Also, recall that our agent can observe one or more grid cells at each step.\n",
    "Technically, the most likely observation is the \n",
    "configuration of the observed grid cells with the highest probability.\n",
    "However, computing the joint distribution of the observed cells is expensive. \n",
    "Therefore, we make another approximation: We consider each cell individually and use the most likely configuration of that cell as its most likely observation.\n",
    "By using this approximation, we can use loopy belief propagation from previous \n",
    "section to compute the most likely observation for each cell and then use \n",
    "those observations to determine the next state.\n",
    "\n",
    "\n",
    "For reference, our solution is **72** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3a7424",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass(frozen=True)\n",
    "class MLODeterminiziedFirePOMDP(RewardProblem):\n",
    "    \"\"\"Determinized version of the fire MDP --- tries to find the solution path \n",
    "    that is most likely to succeed.    \n",
    "    \"\"\"\n",
    "\n",
    "    problem: FirePOMDP\n",
    "\n",
    "    _initial: FirePOMDPBeliefState\n",
    "\n",
    "    fire_mrf: FireMRF\n",
    "    lbp_niters: int = 10\n",
    "\n",
    "    # Use this to cache precomputed fire distributions, so we don't have to recompute them.\n",
    "    fire_marginals_cache: Dict[int, np.ndarray] = dataclasses.field(\n",
    "        default_factory=dict)\n",
    "\n",
    "    @property\n",
    "    def pickup_problem(self) -> PickupProblem:\n",
    "        return self.problem.pickup_problem\n",
    "\n",
    "    @property\n",
    "    def horizon(self) -> int:\n",
    "        return self.problem.horizon\n",
    "\n",
    "    @property\n",
    "    def discount(self) -> float:\n",
    "        return self.problem.discount\n",
    "\n",
    "    def actions(self, state: FirePOMDPBeliefState) -> Iterable[Action]:\n",
    "        return self.problem.actions(state)\n",
    "\n",
    "    def get_fire_marginals(self,\n",
    "                           fire_grid: FirePOMDPObservation) -> np.ndarray:\n",
    "        if fire_grid not in self.fire_marginals_cache:\n",
    "            self.fire_marginals_cache[\n",
    "                fire_grid] = fire_mrf_lbp_conditionals(\n",
    "                    self.fire_mrf, fire_grid, self.lbp_niters)\n",
    "        return self.fire_marginals_cache[fire_grid]\n",
    "\n",
    "    @property\n",
    "    def initial(self) -> FirePOMDPBeliefState:\n",
    "        return self._initial\n",
    "\n",
    "    def step(self, state: FirePOMDPBeliefState,\n",
    "             action: Action) -> FirePOMDPBeliefState:\n",
    "        \"\"\"Transition according to the MLO approximation.\"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    def reward(self, state1: FirePOMDPBeliefState, action: Action,\n",
    "               state2: FirePOMDPBeliefState) -> float:\n",
    "        \"\"\"Reward according to the MLO approximation.\"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    def terminal(self, state: FirePOMDPBeliefState) -> bool:\n",
    "        \"\"\"Return True if all states in the belief state are terminal.\"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400db534",
   "metadata": {},
   "source": [
    "## MLO Determinized FirePOMDP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a9ece4",
   "metadata": {},
   "source": [
    "### Question\n",
    "Please complete the implementation of `FirePOMDP_MLO_MCTSAgent`.\n",
    "At each step, this agent:\n",
    "- Applies MLO approximation by casting the POMDP to an `MLODeterminiziedFirePOMDP` and \n",
    "- Uses MCTS to search for a good action under the MLO approximation.\n",
    "\n",
    "Once you have the implementation, try to run the agent on the problems defined in `get_problem_part2`, \n",
    "and see how it performs. \n",
    "You may use the same utilities (e.g., `run_agent_on_problem` and `animate_trajectory`) as in part 1.\n",
    "\n",
    "As a sanity check for correctness, you should see that the agent achieves a reward $>0.9$ \n",
    "most of the time in `get_problem_part2(\"only_fire\")`, as long as there exists a path that \n",
    "allows the agent to pick up the patient and then reach the goal.\n",
    "\n",
    "_Tips for using MCTS_:\n",
    "- We recommend using `pomdp_random_rollout_policy` as the rollout policy for MCTS instead of the plain random policy. Please see the docstring of that function for more details.\n",
    "- You would want to use `n_simulations=1` since the environment is deterministic. We also recommend setting `max_backup=False` to perform the Monte-Carlo backup instead of the Bellman backup.\n",
    "\n",
    "\n",
    "For reference, our solution is **44** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b6217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class FirePOMDP_MLO_MCTSAgent(FirePOMDPBeliefStateAgent):\n",
    "    \"\"\"Agent that uses A* to plan a path to the goal in a determinized \n",
    "    version of the problem. Does not need any internal state since we \n",
    "    re-determinize the problem at each step.\n",
    "    \"\"\"\n",
    "\n",
    "    lbp_niters: int = 20\n",
    "\n",
    "    C: float = np.sqrt(2)\n",
    "    iteration_budget: int = 100\n",
    "\n",
    "    # Use this to cache precomputed fire distributions, so we don't have to recompute them.\n",
    "    fire_marginals_cache: Dict[int, np.ndarray] = dataclasses.field(\n",
    "        default_factory=dict,\n",
    "        init=False,\n",
    "    )\n",
    "\n",
    "    def _act(self, belief_state: FirePOMDPBeliefState) -> Action:\n",
    "        \"\"\"Returns the best action according to MLO and searching using MCTS.\"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mp03pt2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
