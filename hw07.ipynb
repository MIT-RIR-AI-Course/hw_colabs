{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7518046",
   "metadata": {},
   "source": [
    "# Homework 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc75d59",
   "metadata": {},
   "source": [
    "## Imports and Utilities\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e7a171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import abc\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MDP:\n",
    "    \"\"\"A Markov Decision Process.\"\"\"\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def state_space(self):\n",
    "        \"\"\"Representation of the MDP state set.\n",
    "\n",
    "        Unless otherwise stated, assume this is a set.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def action_space(self):\n",
    "        \"\"\"Representation of the MDP action set.\n",
    "\n",
    "        Unless otherwise stated, assume this is a set.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @property\n",
    "    def temporal_discount_factor(self):\n",
    "        \"\"\"Gamma, defaults to 1.\n",
    "        \"\"\"\n",
    "        return 1.\n",
    "\n",
    "    @property\n",
    "    def horizon(self):\n",
    "        \"\"\"H, defaults to inf.\n",
    "        \"\"\"\n",
    "        return float(\"inf\")\n",
    "\n",
    "    def state_is_terminal(self, state):\n",
    "        \"\"\"Designate certain states as terminal (done) states.\n",
    "\n",
    "        Defaults to False.\n",
    "\n",
    "        Args:\n",
    "            state: A state.\n",
    "\n",
    "        Returns:\n",
    "            is_terminal : A bool.\n",
    "        \"\"\"\n",
    "        return False\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        \"\"\"Return (deterministic) reward for executing action\n",
    "        in state.\n",
    "\n",
    "        Args:\n",
    "            state: A current state.\n",
    "            action: An action.\n",
    "            next_state: A next state.\n",
    "\n",
    "        Returns:\n",
    "            reward : Single time step reward.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_transition_distribution(self, state, action):\n",
    "        \"\"\"Return a distribution over next states.\n",
    "\n",
    "        Unless otherwise stated, assume that this returns\n",
    "        a dictionary mapping states to probabilities. For\n",
    "        example, if the state space were {0, 1, 2}, then\n",
    "        this function might return {0: 0.3, 1: 0.2, 2: 0.5}.\n",
    "\n",
    "        Args:\n",
    "            state: A current state.\n",
    "            action: An action.\n",
    "\n",
    "        Returns:\n",
    "            next_state_distribution: Distribution over next states.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    def sample_next_state(self, state, action, rng=np.random):\n",
    "        \"\"\"Sample a next state from the transition distribution.\n",
    "\n",
    "        This function may be overwritten by subclasses when the explicit\n",
    "        distribution is too large to enumerate.\n",
    "\n",
    "        Args:\n",
    "            state: A state from the state space.\n",
    "            action: An action from the action space.\n",
    "            rng: A random number generator.\n",
    "\n",
    "        Returns:\n",
    "            next_state: A sampled next state from the state space.\n",
    "        \"\"\"\n",
    "        next_state_dist = self.get_transition_distribution(state, action)\n",
    "        next_states, probs = zip(*next_state_dist.items())\n",
    "        next_state_index = rng.choice(len(next_states), p=probs)\n",
    "        next_state = next_states[next_state_index]\n",
    "        return next_state\n",
    "\n",
    "\n",
    "class MarshmallowMDP(MDP):\n",
    "    \"\"\"The Marshmallow MDP.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def state_space(self):\n",
    "        # (hunger level, marshmallow remains)\n",
    "        return {(h, m) for h in {0, 1, 2} for m in {True, False}}\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return {\"eat\", \"wait\"}\n",
    "\n",
    "    @property\n",
    "    def horizon(self):\n",
    "        return 4\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        next_hunger_level = next_state[0]\n",
    "        return -(next_hunger_level**2)\n",
    "\n",
    "    def get_transition_distribution(self, state, action):\n",
    "        # Update marshmallow deterministically\n",
    "        if action == \"eat\":\n",
    "            next_m = False\n",
    "        else:\n",
    "            next_m = state[1]\n",
    "\n",
    "        # Initialize next state distribution dict\n",
    "        # Any state not included assumed to have 0 prob\n",
    "        dist = defaultdict(float)\n",
    "\n",
    "        # Update hunger\n",
    "        if action == \"wait\" or state[1] == False:\n",
    "            # With 0.75 probability, hunger stays the same\n",
    "            dist[(state[0], next_m)] += 0.75\n",
    "            # With 0.25 probability, hunger increases by 1\n",
    "            dist[(min(state[0] + 1, 2), next_m)] += 0.25\n",
    "\n",
    "        else:\n",
    "            assert action == \"eat\" and state[1] == True\n",
    "            # Hunger deterministically set to 1 after eating\n",
    "            dist[(0, next_m)] = 1.0\n",
    "\n",
    "        return dist\n",
    "\n",
    "class ChaseMDP(MDP):\n",
    "    \"\"\"A 2D grid bunny chasing MDP.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def obstacles(self):\n",
    "        return np.zeros((2, 3))  # by default, 2x3 grid with no obstacles\n",
    "\n",
    "    @property\n",
    "    def goal_reward(self):\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def living_reward(self):\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def height(self):\n",
    "        return self.obstacles.shape[0]\n",
    "\n",
    "    @property\n",
    "    def width(self):\n",
    "        return self.obstacles.shape[1]\n",
    "\n",
    "    @property\n",
    "    def state_space(self):\n",
    "        pos = [(r, c) for r in range(self.height) for c in range(self.width)]\n",
    "        return {(p1, p2) for p1 in pos for p2 in pos}\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return {'up', 'down', 'left', 'right'}\n",
    "\n",
    "    @property\n",
    "    def temporal_discount_factor(self):\n",
    "        return 0.9\n",
    "\n",
    "    def action_to_delta(self, action):\n",
    "        return {\n",
    "            'up': (-1, 0),  # up,\n",
    "            'down': (1, 0),  # down,\n",
    "            'left': (0, -1),  # left,\n",
    "            'right': (0, 1),  # right,\n",
    "        }[action]\n",
    "\n",
    "    def get_transition_distribution(self, state, action):\n",
    "        # Discrete distributions, represented with a dict\n",
    "        # mapping next states to probs.\n",
    "        next_state_dist = defaultdict(float)\n",
    "\n",
    "        agent_pos, goal_pos = state\n",
    "\n",
    "        # Get next agent state\n",
    "        row, col = agent_pos\n",
    "        dr, dc = self.action_to_delta(action)\n",
    "        r, c = row + dr, col + dc\n",
    "        # Stay in place if out of bounds or obstacle\n",
    "        if not (0 <= r < self.height and 0 <= c < self.width):\n",
    "            r, c = row, col\n",
    "        elif self.obstacles[r, c]:\n",
    "            r, c = row, col\n",
    "        next_agent_pos = (r, c)\n",
    "\n",
    "        # Get next bunny state\n",
    "        # Stay in same place with probability 0.5\n",
    "        next_state_dist[(next_agent_pos, goal_pos)] += 0.5\n",
    "        # Otherwise move\n",
    "        row, col = goal_pos\n",
    "        for (dr, dc) in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
    "            r, c = row + dr, col + dc\n",
    "            # Stay in place if out of bounds or obstacle\n",
    "            if not (0 <= r < self.height and 0 <= c < self.width):\n",
    "                r, c = row, col\n",
    "            elif self.obstacles[r, c]:\n",
    "                r, c = row, col\n",
    "            next_goal_pos = (r, c)\n",
    "            next_state_dist[(next_agent_pos, next_goal_pos)] += 0.5*0.25\n",
    "\n",
    "        return next_state_dist\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        agent_pos, goal_pos = next_state\n",
    "        if agent_pos == goal_pos:\n",
    "            return self.goal_reward\n",
    "        return self.living_reward\n",
    "\n",
    "    def state_is_terminal(self, state):\n",
    "        agent_pos, goal_pos = state\n",
    "        return agent_pos == goal_pos\n",
    "\n",
    "\n",
    "class LargeChaseMDP(ChaseMDP):\n",
    "    \"\"\"A larger 2D grid bunny chasing MDP.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def obstacles(self):\n",
    "        return np.array([\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 0, 0, 0, 0, 1, 1],\n",
    "            [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 1, 0, 1, 1, 0, 1, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "            [0, 1, 1, 0, 0, 0, 0, 1, 1, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a54738d",
   "metadata": {},
   "source": [
    "## Expectimax Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad49423c",
   "metadata": {},
   "source": [
    "### Question\n",
    "Complete the implementation of expectimax search for a finite horizon MDP.\n",
    "\n",
    "For reference, our solution is **16** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94bac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectimax_search(initial_state, mdp, horizon):\n",
    "    \"\"\"Use expectimax search to determine a next action.\n",
    "\n",
    "    Note that we're just computing the single next action to\n",
    "    take, we do not need to store the entire partial V.\n",
    "\n",
    "    Horizon is given as a separate argument so that we can use\n",
    "    expectimax search with receding horizon control, for example,\n",
    "    even if mdp.horizon is inf.\n",
    "\n",
    "    Args:\n",
    "        initial_state: A state in the mdp.\n",
    "        mdp: An MDP.\n",
    "        horizon: An int horizon.\n",
    "\n",
    "    Returns:\n",
    "        action: An action in the mdp.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3c8711",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39939ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test1_expectimax_search():\n",
    "    mdp = MarshmallowMDP()\n",
    "    assert expectimax_search((0, True), mdp, mdp.horizon) == \"wait\"\n",
    "    assert expectimax_search((0, True), mdp, 1) == \"eat\"\n",
    "    assert expectimax_search((1, True), mdp, mdp.horizon) == \"eat\"\n",
    "    assert expectimax_search((2, True), mdp, mdp.horizon) == \"eat\"\n",
    "    assert expectimax_search((1, True), mdp, 10) == \"wait\"\n",
    "\n",
    "test1_expectimax_search()\n",
    "\n",
    "\n",
    "def test2_expectimax_search():\n",
    "    mdp = ChaseMDP()\n",
    "    assert expectimax_search(((0, 0), (0, 1)), mdp, 1) == \"right\"\n",
    "    assert expectimax_search(((0, 0), (0, 2)), mdp, 2) == \"right\"\n",
    "    assert expectimax_search(((0, 0), (1, 0)), mdp, 1) == \"down\"\n",
    "    assert expectimax_search(((0, 0), (1, 2)), mdp, 2) in [\"right\", \"down\"]\n",
    "    assert expectimax_search(((1, 2), (0, 0)), mdp, 2) in [\"up\", \"left\"]\n",
    "\n",
    "test2_expectimax_search()\n",
    "\n",
    "print('Tests passed.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw07.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
