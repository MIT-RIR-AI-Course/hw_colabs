{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96ee3473",
   "metadata": {},
   "source": [
    "# Homework 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7e3e85",
   "metadata": {},
   "source": [
    "## Imports and Utilities\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a97f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import abc\n",
    "import numpy as np\n",
    "import sys\n",
    "import pdb\n",
    "\n",
    "def press(event):\n",
    "  \"\"\"matplotlib helper function. It processes the keyboard event. If the user hits q, it exit the program.\"\"\"\n",
    "  import matplotlib.pyplot as plt\n",
    "  if event.key == 'q':\n",
    "    sys.exit(0)\n",
    "  plt.close()\n",
    "\n",
    "\n",
    "def plot_value_function(V, mdp):\n",
    "  \"\"\"matplotlib helper function. It takes a value function assumed to be a\n",
    "  maze and plots it.\n",
    "\n",
    "  Args:\n",
    "    V: a dict of {(r, c) : value}\n",
    "  \"\"\"\n",
    "\n",
    "  import matplotlib.pyplot as plt\n",
    "  image = np.zeros((mdp.height, mdp.width))\n",
    "  for state in V.keys():\n",
    "    image[state[0], state[1]] = V[state]\n",
    "\n",
    "  cmap = plt.cm.binary\n",
    "  norm = plt.Normalize(min(V.values()), max(V.values()))\n",
    "  rgba = cmap(norm(image))\n",
    "\n",
    "  for r in range(mdp.height):\n",
    "    for c in range(mdp.width):\n",
    "      if (mdp.obstacles[r][c]):\n",
    "        rgba[r, c, :3] = 1, 0, 0\n",
    "\n",
    "  if mdp.hazards:\n",
    "    for hazard in mdp.hazards:\n",
    "      rgba[hazard[0], hazard[1], :3] = 0, 0, 1\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  fig.canvas.mpl_connect('key_press_event', press)\n",
    "  ax.imshow(rgba)\n",
    "\n",
    "  #\n",
    "  # Note that the image coordinates are in (row, column) order, but the plt.arrow\n",
    "  # command is (x, y). If the rows are y-coordinates, then these two commands\n",
    "  # use different coordinate conventions. Never mind that image coordinates\n",
    "  # also have the origin in the top left corner, and is left-handed.\n",
    "  #\n",
    "  # Computer graphics has much to answer for.\n",
    "  #\n",
    "\n",
    "  for state in V.keys():\n",
    "    r, c = state\n",
    "    delta = mdp.action_to_delta(mdp.get_action(V, state))\n",
    "    plt.arrow(c, r, delta[1]*.35, delta[0]*.35, width = 0.05)\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def check_value_function(sub, sol):\n",
    "  assert set(sub) == set(sol), 'Sets of entries do not match.'\n",
    "  for k, v in sol.items():\n",
    "    assert abs(sub[k] - v) < 1e-5, f'Value do not match for {k}: expect {v}, got {sub[k]}'\n",
    "  return True\n",
    "\n",
    "\n",
    "class MDP:\n",
    "    \"\"\"A Markov Decision Process.\"\"\"\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def state_space(self):\n",
    "        \"\"\"Representation of the MDP state set.\n",
    "\n",
    "        Unless otherwise stated, assume this is a set.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def action_space(self):\n",
    "        \"\"\"Representation of the MDP action set.\n",
    "\n",
    "        Unless otherwise stated, assume this is a set.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @property\n",
    "    def temporal_discount_factor(self):\n",
    "        \"\"\"Gamma, defaults to 1.\n",
    "        \"\"\"\n",
    "        return 1.\n",
    "\n",
    "    @property\n",
    "    def horizon(self):\n",
    "        \"\"\"H, defaults to inf.\n",
    "        \"\"\"\n",
    "        return float(\"inf\")\n",
    "\n",
    "    def state_is_terminal(self, state):\n",
    "        \"\"\"Designate certain states as terminal (done) states.\n",
    "\n",
    "        Defaults to False.\n",
    "\n",
    "        Args:\n",
    "            state: A state.\n",
    "\n",
    "        Returns:\n",
    "            is_terminal : A bool.\n",
    "        \"\"\"\n",
    "        return False\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        \"\"\"Return (deterministic) reward for executing action\n",
    "        in state.\n",
    "\n",
    "        Args:\n",
    "            state: A current state.\n",
    "            action: An action.\n",
    "            next_state: A next state.\n",
    "\n",
    "        Returns:\n",
    "            reward : Single time step reward.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_transition_distribution(self, state, action):\n",
    "        \"\"\"Return a distribution over next states.\n",
    "\n",
    "        Unless otherwise stated, assume that this returns\n",
    "        a dictionary mapping states to probabilities. For\n",
    "        example, if the state space were {0, 1, 2}, then\n",
    "        this function might return {0: 0.3, 1: 0.2, 2: 0.5}.\n",
    "\n",
    "        Args:\n",
    "            state: A current state.\n",
    "            action: An action.\n",
    "\n",
    "        Returns:\n",
    "            next_state_distribution: Distribution over next states.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    def sample_next_state(self, state, action, rng=np.random):\n",
    "        \"\"\"Sample a next state from the transition distribution.\n",
    "\n",
    "        This function may be overwritten by subclasses when the explicit\n",
    "        distribution is too large to enumerate.\n",
    "\n",
    "        Args:\n",
    "            state: A state from the state space.\n",
    "            action: An action from the action space.\n",
    "            rng: A random number generator.\n",
    "\n",
    "        Returns:\n",
    "            next_state: A sampled next state from the state space.\n",
    "        \"\"\"\n",
    "        next_state_dist = self.get_transition_distribution(state, action)\n",
    "        next_states, probs = zip(*next_state_dist.items())\n",
    "        next_state_index = rng.choice(len(next_states), p=probs)\n",
    "        next_state = next_states[next_state_index]\n",
    "        return next_state\n",
    "\n",
    "\n",
    "class SingleRowMDP(MDP):\n",
    "    \"\"\"A 1D grid MDP for debugging. The grid is 1x5\n",
    "    and the agent is meant to start off in the middle.\n",
    "    There is +10 reward on the rightmost square, -10 on\n",
    "    the left. Actions are left and right. An action effect\n",
    "    is reversed with 10% probability.\n",
    "    \"\"\"\n",
    "    @property\n",
    "    def state_space(self):\n",
    "        return {0, 1, 2, 3, 4}  # position in grid\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return {0, 1}  # left, right\n",
    "\n",
    "    def get_transition_distribution(self, state, action):\n",
    "        # Discrete distributions, represented with a dict\n",
    "        # mapping next states to probs.\n",
    "        delta = 1 if action == 1 else -1\n",
    "        intended_effect = min(max(state + delta, 0), 4)\n",
    "        opposite_effect = min(max(state - delta, 0), 4)\n",
    "        assert (intended_effect != opposite_effect)\n",
    "        return {intended_effect: 0.9, opposite_effect: 0.1}\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        if next_state == 0:\n",
    "          return -10\n",
    "        if next_state == 4:\n",
    "          return 10\n",
    "        return -1  # living penalty\n",
    "\n",
    "    def state_is_terminal(self, state):\n",
    "        return state in {0, 4}\n",
    "\n",
    "\n",
    "class ZitsMDP(MDP):\n",
    "    \"\"\"The Zits MDP described in lecture.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def state_space(self):\n",
    "        return {0, 1, 2, 3, 4}\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return {\"apply\", \"sleep\"}\n",
    "\n",
    "    @property\n",
    "    def temporal_discount_factor(self):\n",
    "        return 0.9\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        if action == \"apply\":\n",
    "            return -1 - next_state\n",
    "        assert action == \"sleep\"\n",
    "        return -next_state\n",
    "\n",
    "    def get_transition_distribution(self, state, action):\n",
    "        if action == \"apply\":\n",
    "            return {\n",
    "                0: 0.8,\n",
    "                4: 0.2\n",
    "            }\n",
    "        assert action == \"sleep\"\n",
    "        return {\n",
    "            min(state + 1, 4): 0.4,\n",
    "            max(state - 1, 0): 0.6\n",
    "        }\n",
    "\n",
    "\n",
    "class ChaseMDP(MDP):\n",
    "    \"\"\"A 2D grid bunny chasing MDP.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def obstacles(self):\n",
    "        return np.zeros((2, 3))  # by default, 2x3 grid with no obstacles\n",
    "\n",
    "    @property\n",
    "    def goal_reward(self):\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def living_reward(self):\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def height(self):\n",
    "        return self.obstacles.shape[0]\n",
    "\n",
    "    @property\n",
    "    def width(self):\n",
    "        return self.obstacles.shape[1]\n",
    "\n",
    "    @property\n",
    "    def state_space(self):\n",
    "        pos = [(r, c) for r in range(self.height) for c in range(self.width)]\n",
    "        return {(p1, p2) for p1 in pos for p2 in pos}\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return {'up', 'down', 'left', 'right'}\n",
    "\n",
    "    @property\n",
    "    def temporal_discount_factor(self):\n",
    "        return 0.9\n",
    "\n",
    "    def action_to_delta(self, action):\n",
    "        return {\n",
    "            'up': (-1, 0),  # up,\n",
    "            'down': (1, 0),  # down,\n",
    "            'left': (0, -1),  # left,\n",
    "            'right': (0, 1),  # right,\n",
    "        }[action]\n",
    "\n",
    "    def get_transition_distribution(self, state, action):\n",
    "        # Discrete distributions, represented with a dict\n",
    "        # mapping next states to probs.\n",
    "        next_state_dist = defaultdict(float)\n",
    "\n",
    "        agent_pos, goal_pos = state\n",
    "\n",
    "        # Get next agent state\n",
    "        row, col = agent_pos\n",
    "        dr, dc = self.action_to_delta(action)\n",
    "        r, c = row + dr, col + dc\n",
    "        # Stay in place if out of bounds or obstacle\n",
    "        if not (0 <= r < self.height and 0 <= c < self.width):\n",
    "            r, c = row, col\n",
    "        elif self.obstacles[r, c]:\n",
    "            r, c = row, col\n",
    "        next_agent_pos = (r, c)\n",
    "\n",
    "        # Get next bunny state\n",
    "        # Stay in same place with probability 0.5\n",
    "        next_state_dist[(next_agent_pos, goal_pos)] += 0.5\n",
    "        # Otherwise move\n",
    "        row, col = goal_pos\n",
    "        for (dr, dc) in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
    "            r, c = row + dr, col + dc\n",
    "            # Stay in place if out of bounds or obstacle\n",
    "            if not (0 <= r < self.height and 0 <= c < self.width):\n",
    "                r, c = row, col\n",
    "            elif self.obstacles[r, c]:\n",
    "                r, c = row, col\n",
    "            next_goal_pos = (r, c)\n",
    "            next_state_dist[(next_agent_pos, next_goal_pos)] += 0.5*0.25\n",
    "\n",
    "        return next_state_dist\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        agent_pos, goal_pos = next_state\n",
    "        if agent_pos == goal_pos:\n",
    "            return self.goal_reward\n",
    "        return self.living_reward\n",
    "\n",
    "    def state_is_terminal(self, state):\n",
    "        agent_pos, goal_pos = state\n",
    "        return agent_pos == goal_pos\n",
    "\n",
    "\n",
    "class RescueMDP(MDP):\n",
    "    \"\"\"A 2D grid Rescue MDP. We know where the person to be rescued is, and we\n",
    "    have to go get them.\"\"\"\n",
    "\n",
    "    _goal_location = (4, 5)\n",
    "    goal_reward = 100\n",
    "    living_reward = 0\n",
    "    hazard_cost = -100\n",
    "    hazards = set()\n",
    "    temporal_discount_factor = .9\n",
    "    goal_is_terminal = True\n",
    "\n",
    "    \"\"\"This is the probability that the action has the intended effect. It\n",
    "    needs to be <= 1. If it is less than 1, the rest of the probability mass\n",
    "    is distributed among the other 3 actions. \"\"\"\n",
    "\n",
    "    _correct_transition_probability = .97\n",
    "    _noise_transition_probability = .01\n",
    "\n",
    "    @property\n",
    "    def obstacles(self):\n",
    "      return np.zeros((2, 3))  # by default, 2x3 grid with no obstacles\n",
    "\n",
    "    @property\n",
    "    def goal_location(self):\n",
    "      return self._goal_location\n",
    "\n",
    "    @goal_location.setter\n",
    "    def goal_location(self, location):\n",
    "      assert location[0] < self.width\n",
    "      assert location[1] < self.height\n",
    "      assert not self.obstacles[location[0]][location[1]]\n",
    "      self._goal_location = location\n",
    "\n",
    "    @property\n",
    "    def height(self):\n",
    "      return self.obstacles.shape[0]\n",
    "\n",
    "    @property\n",
    "    def width(self):\n",
    "      return self.obstacles.shape[1]\n",
    "\n",
    "    @property\n",
    "    def state_space(self):\n",
    "      return {(r, c) for r in range(self.height) for c in range(self.width)\n",
    "              if not self.obstacles[r][c]}\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "      return {'up', 'down', 'left', 'right'}\n",
    "\n",
    "    def action_to_delta(self, action):\n",
    "      return {\n",
    "        'up': (-1, 0),  # up,\n",
    "        'down': (1, 0),  # down,\n",
    "        'left': (0, -1),  # left,\n",
    "        'right': (0, 1),  # right,\n",
    "      }[action]\n",
    "\n",
    "    def get_action(self, V, s):\n",
    "\n",
    "      best_value = -float(\"inf\")\n",
    "      best_action = False\n",
    "\n",
    "      for a in self.action_space:\n",
    "        qsa = 0.\n",
    "        for ns, p in self.get_transition_distribution(s, a).items():\n",
    "          r = self.get_reward(s, a, ns)\n",
    "          qsa += p * (r + self.temporal_discount_factor * V[ns])\n",
    "        if qsa > best_value:\n",
    "          best_value = qsa\n",
    "          best_action = a\n",
    "\n",
    "      return best_action\n",
    "\n",
    "\n",
    "    @property\n",
    "    def correct_transition_probability(self):\n",
    "      return self._correct_transition_probability\n",
    "\n",
    "    @correct_transition_probability.setter\n",
    "    def correct_transition_probability(self, prob):\n",
    "      assert prob >= 0 and prob <= 1\n",
    "\n",
    "      # This setter function allows the user to specify a probability of an\n",
    "      # action having the intended effect, e.g., the likelihood that the 'up'\n",
    "      # action moves the agent 'up'. The probability has to be between 0 and\n",
    "      # 1. If it is less than 1, all the remaining probability mass is equally\n",
    "      # distributed among the other three directions.\n",
    "\n",
    "      self._correct_transition_probability = prob\n",
    "      self._noise_transition_probability = (1.0 - prob) / 3.0\n",
    "\n",
    "    @property\n",
    "    def noise_transition_probability(self):\n",
    "      return self._noise_transition_probability\n",
    "\n",
    "    def get_transition_distribution(self, state, action):\n",
    "      # Discrete distributions, represented with a dict\n",
    "      # mapping next states to probs.\n",
    "      next_state_dist = defaultdict(float)\n",
    "\n",
    "      if self.state_is_terminal(state):\n",
    "        return {state: 1.0}\n",
    "\n",
    "      row, col = state\n",
    "      for (dr, dc) in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
    "        r, c = row + dr, col + dc\n",
    "        # Stay in place if out of bounds or obstacle\n",
    "        if not (0 <= r < self.height and 0 <= c < self.width):\n",
    "          r, c = row, col\n",
    "        elif self.obstacles[r, c]:\n",
    "          r, c = row, col\n",
    "        next_agent_pos = (r, c)\n",
    "        if self.action_to_delta(action) == (dr, dc):\n",
    "          next_state_dist[next_agent_pos] += self.correct_transition_probability\n",
    "        else:\n",
    "          next_state_dist[next_agent_pos] += self.noise_transition_probability\n",
    "\n",
    "      return next_state_dist\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "      agent_pos = next_state\n",
    "      if agent_pos == self.goal_location:\n",
    "        return self.goal_reward\n",
    "      if self.hazards and next_state in self.hazards:\n",
    "        return self.hazard_cost\n",
    "      return self.living_reward\n",
    "\n",
    "    def state_is_terminal(self, state):\n",
    "      if not self.goal_is_terminal:\n",
    "        return False\n",
    "      return state == self.goal_location\n",
    "\n",
    "class SmallRescueMDP(RescueMDP):\n",
    "  \"\"\"A small 2D grid MDP.\"\"\"\n",
    "\n",
    "  goal_location = (0, 0)\n",
    "  @property\n",
    "  def obstacles(self):\n",
    "    return np.array([\n",
    "      [0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 1, 0],\n",
    "      [0, 0, 0, 1, 0],\n",
    "      [0, 1, 0, 1, 1]\n",
    "      ])\n",
    "\n",
    "class LargeRescueMDP(RescueMDP):\n",
    "  \"\"\"A larger 2D grid MDP.\"\"\"\n",
    "\n",
    "  @property\n",
    "  def obstacles(self):\n",
    "    return np.array([\n",
    "      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 1, 0, 0, 0, 0, 1, 1],\n",
    "      [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 1, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "      [0, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "      [0, 1, 1, 0, 0, 0, 0, 1, 0, 0],\n",
    "      [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "      ])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53078f0",
   "metadata": {},
   "source": [
    "## Wait, Bellman, Backup!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055d70b2",
   "metadata": {},
   "source": [
    "### Question\n",
    "Complete the implementation of the bellman backup for an infinite or indefinite horizon MDP.\n",
    "\n",
    "For reference, our solution is **13** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aac97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman_backup(s, V, mdp):\n",
    "    \"\"\"Look ahead one step and propose an update for the value of s.\n",
    "\n",
    "    You can assume that the mdp is either infinite or indefinite\n",
    "    horizon (that is, mdp.horizon is inf).\n",
    "\n",
    "    It is possible to handle terminal states either here or in\n",
    "    value iteration. For consistency with our solution, please\n",
    "    handle terminal states in value iteration, not here.\n",
    "\n",
    "    Args:\n",
    "        s: A state.\n",
    "        V: A dict, V[state] -> value.\n",
    "        mdp: An MDP.\n",
    "\n",
    "    Returns:\n",
    "        vs: new value estimate for s.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9e7ea2",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b739b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test1_bellman_backup():\n",
    "    mdp = SingleRowMDP()\n",
    "    s = 3\n",
    "    V = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}\n",
    "    new_V_s = bellman_backup(s, V, mdp)\n",
    "    # Bellman backup should not change V\n",
    "    assert V == {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}\n",
    "    assert new_V_s == 0.9 * 10 + 0.1 * -1\n",
    "    s = 2\n",
    "    new_V_s = bellman_backup(s, V, mdp)\n",
    "    assert new_V_s == -1.\n",
    "\n",
    "test1_bellman_backup()\n",
    "\n",
    "\n",
    "def test2_bellman_backup():\n",
    "    mdp = ZitsMDP()\n",
    "    V = {s : 0 for s in mdp.state_space}\n",
    "    assert bellman_backup(0, V, mdp) == -0.4\n",
    "    assert bellman_backup(1, V, mdp) == -0.8\n",
    "    assert bellman_backup(2, V, mdp) == -1.8\n",
    "    assert bellman_backup(3, V, mdp) == -1.8\n",
    "    assert bellman_backup(4, V, mdp) == -1.8\n",
    "\n",
    "test2_bellman_backup()\n",
    "\n",
    "\n",
    "def test3_bellman_backup():\n",
    "    mdp = ZitsMDP()\n",
    "    V = {0 : -0.1, 1: 0.1, 2: 5, 3: -4, 4: -2.2}\n",
    "    assert abs(bellman_backup(0, V, mdp) - -0.418) < 1e-5\n",
    "    assert abs(bellman_backup(1, V, mdp) - 0.946) < 1e-5\n",
    "    assert abs(bellman_backup(2, V, mdp) - -2.268) < 1e-5\n",
    "    assert abs(bellman_backup(3, V, mdp) - -0.892) < 1e-5\n",
    "    assert abs(bellman_backup(4, V, mdp) - -2.268) < 1e-5\n",
    "\n",
    "test3_bellman_backup()\n",
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2adf84",
   "metadata": {},
   "source": [
    "## There's Value in that Iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5806615c",
   "metadata": {},
   "source": [
    "### Question\n",
    "Complete the implementation of value iteration for an infinite or indefinite horizon MDP.\n",
    "\n",
    "For reference, our solution is **20** line(s) of code.\n",
    "\n",
    "In addition to all the utilities defined at the top of the Colab notebook, the following functions are available in this question environment: `bellman_backup`. You may not need to use all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b54861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp, max_num_iters=1000, change_threshold=1e-4):\n",
    "    \"\"\"Run value iteration for a certain number of iterations or until\n",
    "    the max change between iterations is below a threshold.\n",
    "\n",
    "    Specifically, you should terminate when:\n",
    "        (max_{s} |V(s) - V'(s)|) < change_threshold\n",
    "    where V is the old value function estimate, V' is the new one,\n",
    "    and |*| denotes absolute value.\n",
    "\n",
    "    You can assume that the mdp is either infinite or indefinite\n",
    "    horizon (that is, mdp.horizon is inf).\n",
    "\n",
    "    Make sure to handle terminal states! You will need to think about\n",
    "    what behavior we should expect from value iteration exactly to\n",
    "    deal with terminal states, and then implement that behavior.\n",
    "\n",
    "    Args:\n",
    "        mdp: An MDP.\n",
    "        max_num_iters: An int representing the maximum number of\n",
    "            iterations to run value iteration before giving up.\n",
    "        change_threshold: A float used to determine when value iteration\n",
    "            has converged and it is safe to terminate.\n",
    "\n",
    "    Returns: \n",
    "        V:  A dict, V[state] -> value.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684d925e",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4ebb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test1_value_iteration():\n",
    "    mdp = SingleRowMDP()\n",
    "    V = value_iteration(mdp)\n",
    "    expected_V = {0: 0.0, 1: 5.58531, 2: 8.31706, 3: 9.73170, 4: 0.0}\n",
    "    for s in mdp.state_space:\n",
    "        assert abs(V[s] - expected_V[s]) < 1e-4\n",
    "\n",
    "test1_value_iteration()\n",
    "\n",
    "\n",
    "def test2_value_iteration():\n",
    "    mdp = ZitsMDP()\n",
    "    V = value_iteration(mdp)\n",
    "    expected_V = {0: -6.40530, 1: -7.07368, 2: -7.81918, 3: -7.81918, 4: -7.81918}\n",
    "    for s in mdp.state_space:\n",
    "        assert abs(V[s] - expected_V[s]) < 1e-4\n",
    "\n",
    "test2_value_iteration()\n",
    "\n",
    "\n",
    "def test3_value_iteration():\n",
    "    mdp = SingleRowMDP()\n",
    "    expected_V = {0: 0.0, 1: -1.9, 2: -1.0, 3: 8.9, 4: 0.0}\n",
    "    V = value_iteration(mdp, max_num_iters=1)\n",
    "    for s in mdp.state_space:\n",
    "        assert abs(V[s] - expected_V[s]) < 1e-4\n",
    "    V = value_iteration(mdp, change_threshold=float(\"inf\"))\n",
    "    for s in mdp.state_space:\n",
    "        assert abs(V[s] - expected_V[s]) < 1e-4\n",
    "\n",
    "test3_value_iteration()\n",
    "\n",
    "\n",
    "def test4_value_iteration():\n",
    "    mdp = ChaseMDP()\n",
    "    V = value_iteration(mdp)\n",
    "    partial_expected_V = {((0, 1), (0, 1)): 0.0, ((0, 1), (1, 0)): 0.87506,\n",
    "                          ((1, 0), (0, 2)): 0.80601, ((0, 2), (1, 2)): 0.96536,\n",
    "                          ((1, 1), (0, 1)): 0.94896}\n",
    "    for s in partial_expected_V:\n",
    "        assert abs(V[s] - partial_expected_V[s]) < 1e-4\n",
    "\n",
    "test4_value_iteration()\n",
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7723ecea",
   "metadata": {},
   "source": [
    "## MDP Question 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51fdcca",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "We have provided a RescueMDP class for you, that models the robot's motion\n",
    "as noisy. This class is in many ways similar to the ChaseMDP class\n",
    "in that it is a maze MDP with obstacles. However, there's just the one\n",
    "agent (the robot, no bunny).\n",
    "\n",
    "Recall that an MDP is defined by the following tuple:\n",
    "* States: The state space is a set of coordinates. The set is defined over a\n",
    "grid, but states that aren't obstacles aren't in the state space.\n",
    "* Actions: The robot can take four actions, up, down, left, right.\n",
    "* A transition function: The probability the action succeeds at moving the\n",
    "robot in the given direction is given by the class field\n",
    "`correct_transition_probability`. If this probability is less than 1, then the\n",
    "rest of the probability mass is uniformly distributed among the other three\n",
    "directions. Any probability mass for a motion that would move the robot into\n",
    "an obstacle or out of the map is mapped to the robot staying the same\n",
    "place. If the robot is in the goal state and the MDP has the flag\n",
    "`goal_is_terminal = True`, it cannot transition out of the goal state. If the flag\n",
    "`goal_is_terminal = False`, the robot is free to leave the goal state and\n",
    "re-enter it.\n",
    "* Reward function: The robot gets a reward of `living_reward` for each action\n",
    "it takes. It gets reward of `goal_reward` every time it enters the goal\n",
    "state.\n",
    "\n",
    "Remember that arrays are row-major order, that is, the arrays are indexed by\n",
    "(row, column). The person to be rescued (goal_location) is at (0, 0).\n",
    "\n",
    "We also want you to be able to examine the policy that results from solving\n",
    "for the optimal value function. Please use the helper function\n",
    "`plot_value_function` to generate a plot of both the value function and the\n",
    "corresponding policy.\n",
    "\n",
    "Please create a LargeRescueMDP and compute the optimal value function.\n",
    "\n",
    "\n",
    "For reference, our solution is **4** line(s) of code.\n",
    "\n",
    "In addition to all the utilities defined at the top of the Colab notebook, the following functions are available in this question environment: `bellman_backup`, `value_iteration`. You may not need to use all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e02e334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDP_1():\n",
    "    \"\"\"Creates a LargeRescueMDP(), and returns the optimal value function.\n",
    "\n",
    "    Args:\n",
    "      None\n",
    "\n",
    "    Returns:\n",
    "      value function: a dict of states to values\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3c579b",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e85fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vi_test(function, V, rc):\n",
    "  import random; random.seed(0)\n",
    "  import numpy.random as npr; npr.seed(0)\n",
    "  Vsub = function()\n",
    "  assert check_value_function(Vsub, V)\n",
    "\n",
    "vi_test(MDP_1, {(4, 0): 26.97240030571446, (3, 4): 89.28524333009564, (4, 9): 46.28196469917531, (3, 7): 63.866705918797976, (5, 4): 89.19904388402188, (8, 0): 41.56749854170138, (0, 2): 51.77167752199026, (8, 3): 57.77191141593772, (8, 9): 46.46228722379235, (0, 5): 71.33026555348314, (2, 2): 41.5179757146076, (1, 0): 37.29501402141274, (1, 6): 71.46317864388475, (0, 8): 51.577016904150774, (2, 5): 89.01900243377555, (8, 6): 64.24856551191186, (2, 8): 63.866697606203566, (7, 4): 71.6813569329961, (7, 1): 41.52806356045833, (7, 7): 64.1906123496321, (6, 5): 88.9353734615529, (6, 8): 63.99437878087923, (4, 2): 33.35853220876574, (3, 0): 29.997978602266883, (4, 5): 0.0, (3, 9): 51.378797571401286, (5, 0): 30.00062894204498, (4, 8): 51.477861619010575, (5, 6): 89.10343277693175, (5, 3): 79.95573398571852, (5, 9): 51.427184372629654, (8, 2): 51.784353648073704, (8, 5): 71.26442203181726, (0, 1): 46.36201563185604, (0, 7): 57.53968483878395, (2, 4): 80.11518850941486, (1, 2): 46.361654557652294, (0, 4): 64.49822454430208, (2, 1): 37.294652947208995, (2, 7): 71.31884930507499, (1, 5): 79.64569348621423, (8, 8): 51.73109931662876, (6, 1): 37.22526933032567, (7, 0): 37.29908558822458, (6, 4): 79.96208134752649, (7, 3): 64.38378339850985, (7, 9): 51.63515062834477, (6, 7): 71.52909705147924, (7, 6): 71.6037666010765, (3, 2): 37.21529472183023, (4, 1): 29.90300033148703, (4, 7): 57.24942466484467, (3, 5): 99.50024037525212, (4, 4): 99.50187546410399, (3, 8): 57.25614447733788, (5, 5): 99.40514065110256, (8, 4): 64.31801372707054, (0, 0): 41.55773396442767, (5, 8): 57.30901926222007, (8, 1): 46.37295740578368, (1, 1): 41.5626015183776, (0, 3): 57.81353882364696, (0, 9): 46.27619295995178, (2, 0): 33.465368707503046, (1, 4): 71.88481819474575, (0, 6): 64.12430323875384, (8, 7): 57.651153477926165, (2, 9): 57.24850935593198, (1, 7): 64.12419955940373, (2, 6): 79.71607202444552, (6, 0): 33.4683494865608, (6, 6): 79.87591094828778, (7, 5): 79.57152694202794, (6, 3): 71.75171813607103, (6, 9): 57.31044771948743, (7, 8): 57.5446502393624})\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027a68a9",
   "metadata": {},
   "source": [
    "## MDP Question 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf20416",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "The default transition model for the LargeRescueMDP is pretty close\n",
    "to deterministic. There is a .97 chance that each action succeeds in the intended\n",
    "direction (unless there's an obstacle in the way or its the edge of the map)\n",
    "and only a .01 chance of ending up in one of the other 3 neighbouring grid\n",
    "cells.\n",
    "\n",
    "What happens if we make the transition model noisier? Setting the\n",
    "correct_transition_probability to .76 is not too noisy, but it has\n",
    "implications for our ability to solve for the optimal policy.\n",
    "\n",
    "Please create a LargeRescueMDP and set the correct_transition_probability to\n",
    "be .76. You can set the correct_transition_probability field of the\n",
    "LargeRescueMDP class directly, and the transition function will be adjusted\n",
    "according. (You can read the comment in the LargeRescueMDP setter function for\n",
    "more detail on how setting the correct_transition_probability works.)\n",
    "\n",
    "\n",
    "For reference, our solution is **5** line(s) of code.\n",
    "\n",
    "In addition to all the utilities defined at the top of the Colab notebook, the following functions are available in this question environment: `bellman_backup`, `value_iteration`. You may not need to use all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb714ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDP_2():\n",
    "    \"\"\"Creates a LargeRescueMDP(), sets the correct_transition_probability to\n",
    "    .76 and returns the optimal value function.\n",
    "\n",
    "    Args:\n",
    "      None\n",
    "\n",
    "    Returns:\n",
    "      value function: a dict of states to values\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97385ec",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5cd816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vi_test(function, V, rc):\n",
    "  import random; random.seed(0)\n",
    "  import numpy.random as npr; npr.seed(0)\n",
    "  Vsub = function()\n",
    "  assert check_value_function(Vsub, V)\n",
    "\n",
    "vi_test(MDP_2, {(4, 0): 17.193656448413645, (3, 4): 82.66821248865962, (4, 9): 33.24527236077266, (3, 7): 48.38047730522361, (5, 4): 81.57596106315178, (8, 0): 29.485520779767217, (0, 2): 39.54500010633741, (8, 3): 46.729294692291475, (8, 9): 34.81853504895561, (0, 5): 57.71031630612815, (2, 2): 28.67493693106001, (1, 0): 25.425642667196094, (1, 6): 58.371006293552576, (0, 8): 38.3562257828573, (2, 5): 79.50785163099987, (8, 6): 51.83291668257821, (2, 8): 48.37609286765003, (7, 4): 60.490236730134086, (7, 1): 29.15147584428534, (7, 7): 51.413594478870216, (6, 5): 78.66538264988357, (6, 8): 49.519712872964426, (4, 2): 21.29292539099053, (3, 0): 19.184941573721872, (4, 5): 0.0, (3, 9): 36.70592766228915, (5, 0): 19.36445038655641, (4, 8): 37.33611818768401, (5, 6): 80.39156750674351, (5, 3): 70.35461243079723, (5, 9): 37.09563198565326, (8, 2): 40.21565223450838, (8, 5): 57.16960149114739, (0, 1): 33.657954212109814, (0, 7): 44.47885137481426, (2, 4): 71.94570365595851, (1, 2): 33.63013421547188, (0, 4): 54.116379728735176, (2, 1): 25.397822670558156, (2, 7): 56.82786088085701, (1, 5): 67.23721597706378, (8, 8): 39.50173149602546, (6, 1): 25.180061683772845, (7, 0): 25.734404278265206, (6, 4): 70.24275282935724, (7, 3): 53.62383581844224, (7, 9): 38.68727231509817, (6, 7): 58.8166743393827, (7, 6): 59.54114719666043, (3, 2): 24.704121310787116, (4, 1): 18.46056909239355, (4, 7): 41.79958505037646, (3, 5): 94.47917738184584, (4, 4): 94.63963415294967, (3, 8): 41.906350224012456, (5, 5): 93.32556960074426, (8, 4): 52.64659733120905, (0, 0): 29.033502757353006, (5, 8): 42.2743189133091, (8, 1): 34.191129369373066, (1, 1): 29.10269089376744, (0, 3): 46.568742068171915, (0, 9): 33.46384586176194, (2, 0): 22.19943115828667, (1, 4): 62.444310480187774, (0, 6): 50.95192651060614, (8, 7): 45.25820887982636, (2, 9): 41.7430747582296, (1, 7): 50.88346010072386, (2, 6): 67.54061291729131, (6, 0): 22.424055347153732, (6, 6): 69.17350279458515, (7, 5): 66.56559260006958, (6, 3): 61.46654014072949, (6, 9): 42.379150021338276, (7, 8): 44.3619253645486})\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0eb7a4",
   "metadata": {},
   "source": [
    "## MDP Question 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124da8df",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "The default temporal_discount_factor is .9. This is in general quite a low\n",
    "discount factor. An action 20 steps away will have $.9^{20} \\approx .12$ impact on later\n",
    "actions.\n",
    "\n",
    "What happens if we increase the discount factor? Please create a\n",
    "LargeRescueMDP, set the temporal_discount_factor to be .99, and also set\n",
    "the correct_transition_probability to be .76. Then please solve for the\n",
    "optimal value function.\n",
    "\n",
    "\n",
    "For reference, our solution is **6** line(s) of code.\n",
    "\n",
    "In addition to all the utilities defined at the top of the Colab notebook, the following functions are available in this question environment: `bellman_backup`, `value_iteration`. You may not need to use all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b58191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDP_3():\n",
    "    \"\"\"Creates a LargeRescueMDP(), sets the correct_transition_probability to\n",
    "    .76 and the temporal_discount_factor to .99 and returns the optimal value function.\n",
    "\n",
    "    Args:\n",
    "      None\n",
    "\n",
    "    Returns:\n",
    "      value function: a dict of states to values\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33816205",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b733664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vi_test(function, V, rc):\n",
    "  import random; random.seed(0)\n",
    "  import numpy.random as npr; npr.seed(0)\n",
    "  Vsub = function()\n",
    "  assert check_value_function(Vsub, V)\n",
    "\n",
    "vi_test(MDP_3, {(4, 0): 84.0088571825647, (3, 4): 98.08785113514071, (4, 9): 89.54219638658633, (3, 7): 92.85684581202082, (5, 4): 97.92940667071218, (8, 0): 88.55907148092525, (0, 2): 91.15195158169959, (8, 3): 92.71010661836499, (8, 9): 90.02820552034618, (0, 5): 94.57557687282643, (2, 2): 88.2596929505786, (1, 0): 87.24220975351437, (1, 6): 94.66146525243018, (0, 8): 90.84234985893639, (2, 5): 97.64083406000782, (8, 6): 93.58873399735727, (2, 8): 92.85540170289251, (7, 4): 95.05380837452687, (7, 1): 88.4404389970416, (7, 7): 93.49470937074302, (6, 5): 97.51833426545349, (6, 8): 93.09749758158448, (4, 2): 85.70758382182571, (3, 0): 84.8709760689106, (4, 5): 0.0, (3, 9): 90.3793968171442, (5, 0): 84.979923830497, (4, 8): 90.54408402590663, (5, 6): 97.7688940677295, (5, 3): 96.51303468508517, (5, 9): 90.49294151104122, (8, 2): 91.34026747888913, (8, 5): 94.47412332570079, (0, 1): 89.68812885428089, (0, 7): 92.17514466317554, (2, 4): 96.75361408485887, (1, 2): 89.67672656160515, (0, 4): 94.06459700794483, (2, 1): 87.23080746083865, (2, 7): 94.36773185735558, (1, 5): 96.0088117943946, (8, 8): 91.131398346243, (6, 1): 87.17982482641227, (7, 0): 87.385190483955, (6, 4): 96.47708368340508, (7, 3): 93.97491750094179, (7, 9): 90.91508992377925, (6, 7): 94.7388560346764, (7, 6): 94.86949260973816, (3, 2): 86.97078685302058, (4, 1): 84.52926790610293, (4, 7): 91.53586068943369, (3, 5): 99.37197204832822, (4, 4): 99.39679280725164, (3, 8): 91.54877634429582, (5, 5): 99.22275748726061, (8, 4): 93.77006553865067, (0, 0): 88.39225620136497, (5, 8): 91.64279788012988, (8, 1): 89.85966251042251, (1, 1): 88.40199915122935, (0, 3): 92.67280996413947, (0, 9): 89.65081472852465, (2, 0): 86.08978494897735, (1, 4): 95.40750777342734, (0, 6): 93.41218842117453, (8, 7): 92.35290042089164, (2, 9): 91.51907133407133, (1, 7): 93.39431118581003, (2, 6): 96.04265401195448, (6, 0): 86.21161978985722, (6, 6): 96.30174912325705, (7, 5): 95.897070647761, (6, 3): 95.24349019068275, (6, 9): 91.67470906173104, (7, 8): 92.13682235192404})\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a9a1fe",
   "metadata": {},
   "source": [
    "## MDP Question 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0528fd9a",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Now let's make the dynamics really noisy. Once again, please create a\n",
    "LargeRescueMDP with a hazard at `{(1, 4)}', but with\n",
    "correct_transition_probability set to 0.5. You should see a very substantial\n",
    "change in both the policy and value function.\n",
    "\n",
    "\n",
    "For reference, our solution is **6** line(s) of code.\n",
    "\n",
    "In addition to all the utilities defined at the top of the Colab notebook, the following functions are available in this question environment: `bellman_backup`, `value_iteration`. You may not need to use all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea6b0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDP_4():\n",
    "    \"\"\"Creates a LargeRescueMDP(), and sets a hazard at `{(1, 4)}' and\n",
    "    correct_transition_probability to 0.5 and returns\n",
    "    the optimal value function.\n",
    "\n",
    "    Args:\n",
    "      None\n",
    "\n",
    "    Returns:\n",
    "      value function: a dict of states to values\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d837ba10",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3977e5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vi_test(function, V, rc):\n",
    "  import random; random.seed(0)\n",
    "  import numpy.random as npr; npr.seed(0)\n",
    "  Vsub = function()\n",
    "  assert check_value_function(Vsub, V)\n",
    "\n",
    "vi_test(MDP_4, {(4, 0): 3.9085329993652533, (3, 4): 58.805202984216365, (4, 9): 11.753646222356124, (3, 7): 14.723408455358632, (5, 4): 61.773838936434885, (8, 0): 10.878573027487132, (0, 2): -0.20781732009289544, (8, 3): 25.36339749449463, (8, 9): 15.238821727444563, (0, 5): 3.0643671550462237, (2, 2): 1.3841713034879826, (1, 0): 1.5389420673207934, (1, 6): 17.433410439493713, (0, 8): 9.345469276586272, (2, 5): 41.64410517348415, (8, 6): 27.34061864611354, (2, 8): 14.623495028538457, (7, 4): 36.3058589431468, (7, 1): 10.407323848359, (7, 7): 26.517051201155294, (6, 5): 53.88967949297427, (6, 8): 22.684327610380464, (4, 2): 2.317116216835473, (3, 0): 2.9670510404427017, (4, 5): 0.0, (3, 9): 10.690151502522395, (5, 0): 5.390778213003905, (4, 8): 13.032557691482545, (5, 6): 58.417081668751784, (5, 3): 48.1633084382923, (5, 9): 14.293773308570756, (8, 2): 19.299393310263866, (8, 5): 30.829638790982713, (0, 1): 0.8020744337498172, (0, 7): 11.988658922265932, (2, 4): 16.3730627200805, (1, 2): 0.9073556276657573, (0, 4): -21.315413465306385, (2, 1): 1.5783110450404056, (2, 7): 19.59916846774346, (1, 5): 3.022657227097599, (8, 8): 18.15310792716475, (6, 1): 8.20816411897359, (7, 0): 8.845736779684062, (6, 4): 47.243646625788344, (7, 3): 31.763122639369513, (7, 9): 16.655193698431397, (6, 7): 32.12324354829552, (7, 6): 33.91437634728944, (3, 2): 1.7861164466125632, (4, 1): 3.0091125874058258, (4, 7): 12.257729404123905, (3, 5): 76.54987697070835, (4, 4): 80.10218352076012, (3, 8): 12.377457524096181, (5, 5): 76.11208979327662, (8, 4): 29.13716588731284, (0, 0): 1.1610960979976075, (5, 8): 16.831639699010523, (8, 1): 13.973656429560908, (1, 1): 1.1974311585861637, (0, 3): -4.701281693660509, (0, 9): 7.646274417336397, (2, 0): 2.1208429059739875, (1, 4): -14.16789428451299, (0, 6): 11.885859730077536, (8, 7): 22.357412587191533, (2, 9): 11.691554230957815, (1, 7): 15.568152460906626, (2, 6): 28.582035217246688, (6, 0): 7.082840364895587, (6, 6): 44.27678096369422, (7, 5): 39.40783614141224, (6, 3): 39.44059244534516, (6, 9): 17.470929083719025, (7, 8): 20.556565474493652})\n",
    "print('Tests passed.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw08.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
