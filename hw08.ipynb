{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0a4487a",
   "metadata": {},
   "source": [
    "# Homework 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694f1dac",
   "metadata": {},
   "source": [
    "## Imports and Utilities\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a03a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import abc\n",
    "import numpy as np\n",
    "import sys\n",
    "import pdb\n",
    "\n",
    "def press(event):\n",
    "  \"\"\"matplotlib helper function. It processes the keyboard event. If the user hits q, it exit the program.\"\"\"\n",
    "  import matplotlib.pyplot as plt\n",
    "  if event.key == 'q':\n",
    "    sys.exit(0)\n",
    "  plt.close()\n",
    "\n",
    "\n",
    "def plot_value_function(V, mdp):\n",
    "  \"\"\"matplotlib helper function. It takes a value function assumed to be a\n",
    "  maze and plots it.\n",
    "\n",
    "  Args:\n",
    "    V: a dict of {(r, c) : value}\n",
    "  \"\"\"\n",
    "\n",
    "  import matplotlib.pyplot as plt\n",
    "  image = np.zeros((mdp.height, mdp.width))\n",
    "  for state in V.keys():\n",
    "    image[state[0], state[1]] = V[state]\n",
    "\n",
    "  cmap = plt.cm.binary\n",
    "  norm = plt.Normalize(min(V.values()), max(V.values()))\n",
    "  rgba = cmap(norm(image))\n",
    "\n",
    "  for r in range(mdp.height):\n",
    "    for c in range(mdp.width):\n",
    "      if (mdp.obstacles[r][c]):\n",
    "        rgba[r, c, :3] = 1, 0, 0\n",
    "\n",
    "  if mdp.hazards:\n",
    "    for hazard in mdp.hazards:\n",
    "      rgba[hazard[0], hazard[1], :3] = 0, 0, 1\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  fig.canvas.mpl_connect('key_press_event', press)\n",
    "  ax.imshow(rgba)\n",
    "\n",
    "  #\n",
    "  # Note that the image coordinates are in (row, column) order, but the plt.arrow\n",
    "  # command is (x, y). If the rows are y-coordinates, then these two commands\n",
    "  # use different coordinate conventions. Never mind that image coordinates\n",
    "  # also have the origin in the top left corner, and is left-handed.\n",
    "  #\n",
    "  # Computer graphics has much to answer for.\n",
    "  #\n",
    "\n",
    "  for state in V.keys():\n",
    "    r, c = state\n",
    "    delta = mdp.action_to_delta(mdp.get_action(V, state))\n",
    "    plt.arrow(c, r, delta[1]*.35, delta[0]*.35, width = 0.05)\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def check_value_function(sub, sol):\n",
    "  assert set(sub) == set(sol), 'Sets of entries do not match.'\n",
    "  for k, v in sol.items():\n",
    "    assert abs(sub[k] - v) < 1e-5, f'Value do not match for {k}: expect {v}, got {sub[k]}'\n",
    "  return True\n",
    "\n",
    "\n",
    "class MDP:\n",
    "    \"\"\"A Markov Decision Process.\"\"\"\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def state_space(self):\n",
    "        \"\"\"Representation of the MDP state set.\n",
    "\n",
    "        Unless otherwise stated, assume this is a set.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def action_space(self):\n",
    "        \"\"\"Representation of the MDP action set.\n",
    "\n",
    "        Unless otherwise stated, assume this is a set.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @property\n",
    "    def temporal_discount_factor(self):\n",
    "        \"\"\"Gamma, defaults to 1.\n",
    "        \"\"\"\n",
    "        return 1.\n",
    "\n",
    "    @property\n",
    "    def horizon(self):\n",
    "        \"\"\"H, defaults to inf.\n",
    "        \"\"\"\n",
    "        return float(\"inf\")\n",
    "\n",
    "    def state_is_terminal(self, state):\n",
    "        \"\"\"Designate certain states as terminal (done) states.\n",
    "\n",
    "        Defaults to False.\n",
    "\n",
    "        Args:\n",
    "            state: A state.\n",
    "\n",
    "        Returns:\n",
    "            is_terminal : A bool.\n",
    "        \"\"\"\n",
    "        return False\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        \"\"\"Return (deterministic) reward for executing action\n",
    "        in state.\n",
    "\n",
    "        Args:\n",
    "            state: A current state.\n",
    "            action: An action.\n",
    "            next_state: A next state.\n",
    "\n",
    "        Returns:\n",
    "            reward : Single time step reward.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_transition_distribution(self, state, action):\n",
    "        \"\"\"Return a distribution over next states.\n",
    "\n",
    "        Unless otherwise stated, assume that this returns\n",
    "        a dictionary mapping states to probabilities. For\n",
    "        example, if the state space were {0, 1, 2}, then\n",
    "        this function might return {0: 0.3, 1: 0.2, 2: 0.5}.\n",
    "\n",
    "        Args:\n",
    "            state: A current state.\n",
    "            action: An action.\n",
    "\n",
    "        Returns:\n",
    "            next_state_distribution: Distribution over next states.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    def sample_next_state(self, state, action, rng=np.random):\n",
    "        \"\"\"Sample a next state from the transition distribution.\n",
    "\n",
    "        This function may be overwritten by subclasses when the explicit\n",
    "        distribution is too large to enumerate.\n",
    "\n",
    "        Args:\n",
    "            state: A state from the state space.\n",
    "            action: An action from the action space.\n",
    "            rng: A random number generator.\n",
    "\n",
    "        Returns:\n",
    "            next_state: A sampled next state from the state space.\n",
    "        \"\"\"\n",
    "        next_state_dist = self.get_transition_distribution(state, action)\n",
    "        next_states, probs = zip(*next_state_dist.items())\n",
    "        next_state_index = rng.choice(len(next_states), p=probs)\n",
    "        next_state = next_states[next_state_index]\n",
    "        return next_state\n",
    "\n",
    "\n",
    "class SingleRowMDP(MDP):\n",
    "    \"\"\"A 1D grid MDP for debugging. The grid is 1x5\n",
    "    and the agent is meant to start off in the middle.\n",
    "    There is +10 reward on the rightmost square, -10 on\n",
    "    the left. Actions are left and right. An action effect\n",
    "    is reversed with 10% probability.\n",
    "    \"\"\"\n",
    "    @property\n",
    "    def state_space(self):\n",
    "        return {0, 1, 2, 3, 4}  # position in grid\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return {0, 1}  # left, right\n",
    "\n",
    "    def get_transition_distribution(self, state, action):\n",
    "        # Discrete distributions, represented with a dict\n",
    "        # mapping next states to probs.\n",
    "        delta = 1 if action == 1 else -1\n",
    "        intended_effect = min(max(state + delta, 0), 4)\n",
    "        opposite_effect = min(max(state - delta, 0), 4)\n",
    "        assert (intended_effect != opposite_effect)\n",
    "        return {intended_effect: 0.9, opposite_effect: 0.1}\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        if next_state == 0:\n",
    "          return -10\n",
    "        if next_state == 4:\n",
    "          return 10\n",
    "        return -1  # living penalty\n",
    "\n",
    "    def state_is_terminal(self, state):\n",
    "        return state in {0, 4}\n",
    "\n",
    "\n",
    "class ZitsMDP(MDP):\n",
    "    \"\"\"The Zits MDP described in lecture.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def state_space(self):\n",
    "        return {0, 1, 2, 3, 4}\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return {\"apply\", \"sleep\"}\n",
    "\n",
    "    @property\n",
    "    def temporal_discount_factor(self):\n",
    "        return 0.9\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        if action == \"apply\":\n",
    "            return -1 - next_state\n",
    "        assert action == \"sleep\"\n",
    "        return -next_state\n",
    "\n",
    "    def get_transition_distribution(self, state, action):\n",
    "        if action == \"apply\":\n",
    "            return {\n",
    "                0: 0.8,\n",
    "                4: 0.2\n",
    "            }\n",
    "        assert action == \"sleep\"\n",
    "        return {\n",
    "            min(state + 1, 4): 0.4,\n",
    "            max(state - 1, 0): 0.6\n",
    "        }\n",
    "\n",
    "\n",
    "class ChaseMDP(MDP):\n",
    "    \"\"\"A 2D grid bunny chasing MDP.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def obstacles(self):\n",
    "        return np.zeros((2, 3))  # by default, 2x3 grid with no obstacles\n",
    "\n",
    "    @property\n",
    "    def goal_reward(self):\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def living_reward(self):\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def height(self):\n",
    "        return self.obstacles.shape[0]\n",
    "\n",
    "    @property\n",
    "    def width(self):\n",
    "        return self.obstacles.shape[1]\n",
    "\n",
    "    @property\n",
    "    def state_space(self):\n",
    "        pos = [(r, c) for r in range(self.height) for c in range(self.width)]\n",
    "        return {(p1, p2) for p1 in pos for p2 in pos}\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return {'up', 'down', 'left', 'right'}\n",
    "\n",
    "    @property\n",
    "    def temporal_discount_factor(self):\n",
    "        return 0.9\n",
    "\n",
    "    def action_to_delta(self, action):\n",
    "        return {\n",
    "            'up': (-1, 0),  # up,\n",
    "            'down': (1, 0),  # down,\n",
    "            'left': (0, -1),  # left,\n",
    "            'right': (0, 1),  # right,\n",
    "        }[action]\n",
    "\n",
    "    def get_transition_distribution(self, state, action):\n",
    "        # Discrete distributions, represented with a dict\n",
    "        # mapping next states to probs.\n",
    "        next_state_dist = defaultdict(float)\n",
    "\n",
    "        agent_pos, goal_pos = state\n",
    "\n",
    "        # Get next agent state\n",
    "        row, col = agent_pos\n",
    "        dr, dc = self.action_to_delta(action)\n",
    "        r, c = row + dr, col + dc\n",
    "        # Stay in place if out of bounds or obstacle\n",
    "        if not (0 <= r < self.height and 0 <= c < self.width):\n",
    "            r, c = row, col\n",
    "        elif self.obstacles[r, c]:\n",
    "            r, c = row, col\n",
    "        next_agent_pos = (r, c)\n",
    "\n",
    "        # Get next bunny state\n",
    "        # Stay in same place with probability 0.5\n",
    "        next_state_dist[(next_agent_pos, goal_pos)] += 0.5\n",
    "        # Otherwise move\n",
    "        row, col = goal_pos\n",
    "        for (dr, dc) in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
    "            r, c = row + dr, col + dc\n",
    "            # Stay in place if out of bounds or obstacle\n",
    "            if not (0 <= r < self.height and 0 <= c < self.width):\n",
    "                r, c = row, col\n",
    "            elif self.obstacles[r, c]:\n",
    "                r, c = row, col\n",
    "            next_goal_pos = (r, c)\n",
    "            next_state_dist[(next_agent_pos, next_goal_pos)] += 0.5*0.25\n",
    "\n",
    "        return next_state_dist\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        agent_pos, goal_pos = next_state\n",
    "        if agent_pos == goal_pos:\n",
    "            return self.goal_reward\n",
    "        return self.living_reward\n",
    "\n",
    "    def state_is_terminal(self, state):\n",
    "        agent_pos, goal_pos = state\n",
    "        return agent_pos == goal_pos\n",
    "\n",
    "\n",
    "class RescueMDP(MDP):\n",
    "    \"\"\"A 2D grid Rescue MDP. We know where the person to be rescued is, and we\n",
    "    have to go get them.\"\"\"\n",
    "\n",
    "    _goal_location = (4, 5)\n",
    "    goal_reward = 100\n",
    "    living_reward = 0\n",
    "    hazard_cost = -100\n",
    "    hazards = set()\n",
    "    temporal_discount_factor = .9\n",
    "    goal_is_terminal = True\n",
    "\n",
    "    \"\"\"This is the probability that the action has the intended effect. It\n",
    "    needs to be <= 1. If it is less than 1, the rest of the probability mass\n",
    "    is distributed among the other 3 actions. \"\"\"\n",
    "\n",
    "    _correct_transition_probability = .97\n",
    "    _noise_transition_probability = .01\n",
    "\n",
    "    @property\n",
    "    def obstacles(self):\n",
    "      return np.zeros((2, 3))  # by default, 2x3 grid with no obstacles\n",
    "\n",
    "    @property\n",
    "    def goal_location(self):\n",
    "      return self._goal_location\n",
    "\n",
    "    @goal_location.setter\n",
    "    def goal_location(self, location):\n",
    "      assert location[0] < self.width\n",
    "      assert location[1] < self.height\n",
    "      assert not self.obstacles[location[0]][location[1]]\n",
    "      self._goal_location = location\n",
    "\n",
    "    @property\n",
    "    def height(self):\n",
    "      return self.obstacles.shape[0]\n",
    "\n",
    "    @property\n",
    "    def width(self):\n",
    "      return self.obstacles.shape[1]\n",
    "\n",
    "    @property\n",
    "    def state_space(self):\n",
    "      return {(r, c) for r in range(self.height) for c in range(self.width)\n",
    "              if not self.obstacles[r][c]}\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "      return {'up', 'down', 'left', 'right'}\n",
    "\n",
    "    def action_to_delta(self, action):\n",
    "      return {\n",
    "        'up': (-1, 0),  # up,\n",
    "        'down': (1, 0),  # down,\n",
    "        'left': (0, -1),  # left,\n",
    "        'right': (0, 1),  # right,\n",
    "      }[action]\n",
    "\n",
    "    def get_action(self, V, s):\n",
    "\n",
    "      best_value = -float(\"inf\")\n",
    "      best_action = False\n",
    "\n",
    "      for a in self.action_space:\n",
    "        qsa = 0.\n",
    "        for ns, p in self.get_transition_distribution(s, a).items():\n",
    "          r = self.get_reward(s, a, ns)\n",
    "          qsa += p * (r + self.temporal_discount_factor * V[ns])\n",
    "        if qsa > best_value:\n",
    "          best_value = qsa\n",
    "          best_action = a\n",
    "\n",
    "      return best_action\n",
    "\n",
    "\n",
    "    @property\n",
    "    def correct_transition_probability(self):\n",
    "      return self._correct_transition_probability\n",
    "\n",
    "    @correct_transition_probability.setter\n",
    "    def correct_transition_probability(self, prob):\n",
    "      assert prob >= 0 and prob <= 1\n",
    "\n",
    "      # This setter function allows the user to specify a probability of an\n",
    "      # action having the intended effect, e.g., the likelihood that the 'up'\n",
    "      # action moves the agent 'up'. The probability has to be between 0 and\n",
    "      # 1. If it is less than 1, all the remaining probability mass is equally\n",
    "      # distributed among the other three directions.\n",
    "\n",
    "      self._correct_transition_probability = prob\n",
    "      self._noise_transition_probability = (1.0 - prob) / 3.0\n",
    "\n",
    "    @property\n",
    "    def noise_transition_probability(self):\n",
    "      return self._noise_transition_probability\n",
    "\n",
    "    def get_transition_distribution(self, state, action):\n",
    "      # Discrete distributions, represented with a dict\n",
    "      # mapping next states to probs.\n",
    "      next_state_dist = defaultdict(float)\n",
    "\n",
    "      if self.state_is_terminal(state):\n",
    "        return {state: 1.0}\n",
    "\n",
    "      row, col = state\n",
    "      for (dr, dc) in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
    "        r, c = row + dr, col + dc\n",
    "        # Stay in place if out of bounds or obstacle\n",
    "        if not (0 <= r < self.height and 0 <= c < self.width):\n",
    "          r, c = row, col\n",
    "        elif self.obstacles[r, c]:\n",
    "          r, c = row, col\n",
    "        next_agent_pos = (r, c)\n",
    "        if self.action_to_delta(action) == (dr, dc):\n",
    "          next_state_dist[next_agent_pos] += self.correct_transition_probability\n",
    "        else:\n",
    "          next_state_dist[next_agent_pos] += self.noise_transition_probability\n",
    "\n",
    "      return next_state_dist\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "      agent_pos = next_state\n",
    "      if agent_pos == self.goal_location:\n",
    "        return self.goal_reward\n",
    "      if self.hazards and next_state in self.hazards:\n",
    "        return self.hazard_cost\n",
    "      return self.living_reward\n",
    "\n",
    "    def state_is_terminal(self, state):\n",
    "      if not self.goal_is_terminal:\n",
    "        return False\n",
    "      return state == self.goal_location\n",
    "\n",
    "class SmallRescueMDP(RescueMDP):\n",
    "  \"\"\"A small 2D grid MDP.\"\"\"\n",
    "\n",
    "  goal_location = (0, 0)\n",
    "  @property\n",
    "  def obstacles(self):\n",
    "    return np.array([\n",
    "      [0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 1, 0],\n",
    "      [0, 0, 0, 1, 0],\n",
    "      [0, 1, 0, 1, 1]\n",
    "      ])\n",
    "\n",
    "class LargeRescueMDP(RescueMDP):\n",
    "  \"\"\"A larger 2D grid MDP.\"\"\"\n",
    "\n",
    "  @property\n",
    "  def obstacles(self):\n",
    "    return np.array([\n",
    "      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 1, 0, 0, 0, 0, 1, 1],\n",
    "      [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 1, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "      [0, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "      [0, 1, 1, 0, 0, 0, 0, 1, 0, 0],\n",
    "      [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "      ])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63ba84d",
   "metadata": {},
   "source": [
    "## Wait, Bellman, Backup!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180f6fa1",
   "metadata": {},
   "source": [
    "### Question\n",
    "Complete the implementation of the bellman backup for an infinite or indefinite horizon MDP.\n",
    "\n",
    "For reference, our solution is **13** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19893604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman_backup(s, V, mdp):\n",
    "    \"\"\"Look ahead one step and propose an update for the value of s.\n",
    "\n",
    "    You can assume that the mdp is either infinite or indefinite\n",
    "    horizon (that is, mdp.horizon is inf).\n",
    "\n",
    "    It is possible to handle terminal states either here or in\n",
    "    value iteration. For consistency with our solution, please\n",
    "    handle terminal states in value iteration, not here.\n",
    "\n",
    "    Args:\n",
    "        s: A state.\n",
    "        V: A dict, V[state] -> value.\n",
    "        mdp: An MDP.\n",
    "\n",
    "    Returns:\n",
    "        vs: new value estimate for s.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca44252",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf6672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test1_bellman_backup():\n",
    "    mdp = SingleRowMDP()\n",
    "    s = 3\n",
    "    V = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}\n",
    "    new_V_s = bellman_backup(s, V, mdp)\n",
    "    # Bellman backup should not change V\n",
    "    assert V == {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}\n",
    "    assert new_V_s == 0.9 * 10 + 0.1 * -1\n",
    "    s = 2\n",
    "    new_V_s = bellman_backup(s, V, mdp)\n",
    "    assert new_V_s == -1.\n",
    "\n",
    "test1_bellman_backup()\n",
    "\n",
    "\n",
    "def test2_bellman_backup():\n",
    "    mdp = ZitsMDP()\n",
    "    V = {s : 0 for s in mdp.state_space}\n",
    "    assert bellman_backup(0, V, mdp) == -0.4\n",
    "    assert bellman_backup(1, V, mdp) == -0.8\n",
    "    assert bellman_backup(2, V, mdp) == -1.8\n",
    "    assert bellman_backup(3, V, mdp) == -1.8\n",
    "    assert bellman_backup(4, V, mdp) == -1.8\n",
    "\n",
    "test2_bellman_backup()\n",
    "\n",
    "\n",
    "def test3_bellman_backup():\n",
    "    mdp = ZitsMDP()\n",
    "    V = {0 : -0.1, 1: 0.1, 2: 5, 3: -4, 4: -2.2}\n",
    "    assert abs(bellman_backup(0, V, mdp) - -0.418) < 1e-5\n",
    "    assert abs(bellman_backup(1, V, mdp) - 0.946) < 1e-5\n",
    "    assert abs(bellman_backup(2, V, mdp) - -2.268) < 1e-5\n",
    "    assert abs(bellman_backup(3, V, mdp) - -0.892) < 1e-5\n",
    "    assert abs(bellman_backup(4, V, mdp) - -2.268) < 1e-5\n",
    "\n",
    "test3_bellman_backup()\n",
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4b24c5",
   "metadata": {},
   "source": [
    "## There's Value in that Iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb03bb1b",
   "metadata": {},
   "source": [
    "### Question\n",
    "Complete the implementation of value iteration for an infinite or indefinite horizon MDP.\n",
    "\n",
    "For reference, our solution is **20** line(s) of code.\n",
    "\n",
    "In addition to all the utilities defined at the top of the Colab notebook, the following functions are available in this question environment: `bellman_backup`. You may not need to use all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a5cf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp, max_num_iters=1000, change_threshold=1e-4):\n",
    "    \"\"\"Run value iteration for a certain number of iterations or until\n",
    "    the max change between iterations is below a threshold.\n",
    "\n",
    "    Specifically, you should terminate when:\n",
    "        (max_{s} |V(s) - V'(s)|) < change_threshold\n",
    "    where V is the old value function estimate, V' is the new one,\n",
    "    and |*| denotes absolute value.\n",
    "\n",
    "    You can assume that the mdp is either infinite or indefinite\n",
    "    horizon (that is, mdp.horizon is inf).\n",
    "\n",
    "    Make sure to handle terminal states! You will need to think about\n",
    "    what behavior we should expect from value iteration exactly to\n",
    "    deal with terminal states, and then implement that behavior.\n",
    "\n",
    "    Args:\n",
    "        mdp: An MDP.\n",
    "        max_num_iters: An int representing the maximum number of\n",
    "            iterations to run value iteration before giving up.\n",
    "        change_threshold: A float used to determine when value iteration\n",
    "            has converged and it is safe to terminate.\n",
    "\n",
    "    Returns: \n",
    "        V:  A dict, V[state] -> value.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72746cf3",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eae3571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test1_value_iteration():\n",
    "    mdp = SingleRowMDP()\n",
    "    V = value_iteration(mdp)\n",
    "    expected_V = {0: 0.0, 1: 5.58531, 2: 8.31706, 3: 9.73170, 4: 0.0}\n",
    "    for s in mdp.state_space:\n",
    "        assert abs(V[s] - expected_V[s]) < 1e-4\n",
    "\n",
    "test1_value_iteration()\n",
    "\n",
    "\n",
    "def test2_value_iteration():\n",
    "    mdp = ZitsMDP()\n",
    "    V = value_iteration(mdp)\n",
    "    expected_V = {0: -6.40530, 1: -7.07368, 2: -7.81918, 3: -7.81918, 4: -7.81918}\n",
    "    for s in mdp.state_space:\n",
    "        assert abs(V[s] - expected_V[s]) < 1e-4\n",
    "\n",
    "test2_value_iteration()\n",
    "\n",
    "\n",
    "def test3_value_iteration():\n",
    "    mdp = SingleRowMDP()\n",
    "    expected_V = {0: 0.0, 1: -1.9, 2: -1.0, 3: 8.9, 4: 0.0}\n",
    "    V = value_iteration(mdp, max_num_iters=1)\n",
    "    for s in mdp.state_space:\n",
    "        assert abs(V[s] - expected_V[s]) < 1e-4\n",
    "    V = value_iteration(mdp, change_threshold=float(\"inf\"))\n",
    "    for s in mdp.state_space:\n",
    "        assert abs(V[s] - expected_V[s]) < 1e-4\n",
    "\n",
    "test3_value_iteration()\n",
    "\n",
    "\n",
    "def test4_value_iteration():\n",
    "    mdp = ChaseMDP()\n",
    "    V = value_iteration(mdp)\n",
    "    partial_expected_V = {((0, 1), (0, 1)): 0.0, ((0, 1), (1, 0)): 0.87506,\n",
    "                          ((1, 0), (0, 2)): 0.80601, ((0, 2), (1, 2)): 0.96536,\n",
    "                          ((1, 1), (0, 1)): 0.94896}\n",
    "    for s in partial_expected_V:\n",
    "        assert abs(V[s] - partial_expected_V[s]) < 1e-4\n",
    "\n",
    "test4_value_iteration()\n",
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f09676",
   "metadata": {},
   "source": [
    "## MDP Question 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4793ef1e",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "We have provided a RescueMDP class for you, that models the robot's motion\n",
    "as noisy. This class is in many ways similar to the ChaseMDP class\n",
    "in that it is a maze MDP with obstacles. However, there's just the one\n",
    "agent (the robot, no bunny).\n",
    "\n",
    "Recall that an MDP is defined by the following tuple:\n",
    "* States: The state space is a set of coordinates. The set is defined over a\n",
    "grid, but states that aren't obstacles aren't in the state space.\n",
    "* Actions: The robot can take four actions, up, down, left, right.\n",
    "* A transition function: The probability the action succeeds at moving the\n",
    "robot in the given direction is given by the class field\n",
    "`correct_transition_probability`. If this probability is less than 1, then the\n",
    "rest of the probability mass is uniformly distributed among the other three\n",
    "directions. Any probability mass for a motion that would move the robot into\n",
    "an obstacle or out of the map is mapped to the robot staying the same\n",
    "place. If the robot is in the goal state and the MDP has the flag\n",
    "`goal_is_terminal = True`, it cannot transition out of the goal state. If the flag\n",
    "`goal_is_terminal = False`, the robot is free to leave the goal state and\n",
    "re-enter it.\n",
    "* Reward function: The robot gets a reward of `living_reward` for each action\n",
    "it takes. It gets reward of `goal_reward` every time it enters the goal\n",
    "state.\n",
    "\n",
    "Remember that arrays are row-major order, that is, the arrays are indexed by\n",
    "(row, column). The person to be rescued (goal_location) is at (0, 0).\n",
    "\n",
    "We also want you to be able to examine the policy that results from solving\n",
    "for the optimal value function. Please use the helper function\n",
    "`plot_value_function` to generate a plot of both the value function and the\n",
    "corresponding policy.\n",
    "\n",
    "Please create a LargeRescueMDP and compute the optimal value function.\n",
    "\n",
    "\n",
    "For reference, our solution is **4** line(s) of code.\n",
    "\n",
    "In addition to all the utilities defined at the top of the Colab notebook, the following functions are available in this question environment: `bellman_backup`, `value_iteration`. You may not need to use all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8634a43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDP_1():\n",
    "    \"\"\"Creates a LargeRescueMDP(), and returns the optimal value function.\n",
    "\n",
    "    Args:\n",
    "      None\n",
    "\n",
    "    Returns:\n",
    "      value function: a dict of states to values\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fa8d1e",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26927b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vi_test(function, V):\n",
    "  import random; random.seed(0)\n",
    "  import numpy.random as npr; npr.seed(0)\n",
    "  Vsub = function()\n",
    "  assert check_value_function(Vsub, V)\n",
    "\n",
    "vi_test(MDP_1, {(4, 0): 26.97240030571446, (3, 4): 89.28524333009564, (4, 9): 46.28196469917531, (3, 7): 63.866705918797976, (5, 4): 89.19904388402188, (8, 0): 41.56749854170138, (0, 2): 51.77167752199026, (8, 3): 57.77191141593772, (8, 9): 46.46228722379235, (0, 5): 71.33026555348314, (2, 2): 41.5179757146076, (1, 0): 37.29501402141274, (1, 6): 71.46317864388475, (0, 8): 51.577016904150774, (2, 5): 89.01900243377555, (8, 6): 64.24856551191186, (2, 8): 63.866697606203566, (7, 4): 71.6813569329961, (7, 1): 41.52806356045833, (7, 7): 64.1906123496321, (6, 5): 88.9353734615529, (6, 8): 63.99437878087923, (4, 2): 33.35853220876574, (3, 0): 29.997978602266883, (4, 5): 0.0, (3, 9): 51.378797571401286, (5, 0): 30.00062894204498, (4, 8): 51.477861619010575, (5, 6): 89.10343277693175, (5, 3): 79.95573398571852, (5, 9): 51.427184372629654, (8, 2): 51.784353648073704, (8, 5): 71.26442203181726, (0, 1): 46.36201563185604, (0, 7): 57.53968483878395, (2, 4): 80.11518850941486, (1, 2): 46.361654557652294, (0, 4): 64.49822454430208, (2, 1): 37.294652947208995, (2, 7): 71.31884930507499, (1, 5): 79.64569348621423, (8, 8): 51.73109931662876, (6, 1): 37.22526933032567, (7, 0): 37.29908558822458, (6, 4): 79.96208134752649, (7, 3): 64.38378339850985, (7, 9): 51.63515062834477, (6, 7): 71.52909705147924, (7, 6): 71.6037666010765, (3, 2): 37.21529472183023, (4, 1): 29.90300033148703, (4, 7): 57.24942466484467, (3, 5): 99.50024037525212, (4, 4): 99.50187546410399, (3, 8): 57.25614447733788, (5, 5): 99.40514065110256, (8, 4): 64.31801372707054, (0, 0): 41.55773396442767, (5, 8): 57.30901926222007, (8, 1): 46.37295740578368, (1, 1): 41.5626015183776, (0, 3): 57.81353882364696, (0, 9): 46.27619295995178, (2, 0): 33.465368707503046, (1, 4): 71.88481819474575, (0, 6): 64.12430323875384, (8, 7): 57.651153477926165, (2, 9): 57.24850935593198, (1, 7): 64.12419955940373, (2, 6): 79.71607202444552, (6, 0): 33.4683494865608, (6, 6): 79.87591094828778, (7, 5): 79.57152694202794, (6, 3): 71.75171813607103, (6, 9): 57.31044771948743, (7, 8): 57.5446502393624})\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda6f1b1",
   "metadata": {},
   "source": [
    "## MDP Question 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4051df6c",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "The default transition model for the LargeRescueMDP is pretty close\n",
    "to deterministic. There is a .97 chance that each action succeeds in the intended\n",
    "direction (unless there's an obstacle in the way or its the edge of the map)\n",
    "and only a .01 chance of ending up in one of the other 3 neighbouring grid\n",
    "cells.\n",
    "\n",
    "What happens if we make the transition model noisier? Setting the\n",
    "correct_transition_probability to .76 is not too noisy, but it has\n",
    "implications for our ability to solve for the optimal policy.\n",
    "\n",
    "Please create a LargeRescueMDP and set the correct_transition_probability to\n",
    "be .76. You can set the correct_transition_probability field of the\n",
    "LargeRescueMDP class directly, and the transition function will be adjusted\n",
    "according. (You can read the comment in the LargeRescueMDP setter function for\n",
    "more detail on how setting the correct_transition_probability works.)\n",
    "\n",
    "\n",
    "For reference, our solution is **5** line(s) of code.\n",
    "\n",
    "In addition to all the utilities defined at the top of the Colab notebook, the following functions are available in this question environment: `bellman_backup`, `value_iteration`. You may not need to use all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDP_2():\n",
    "    \"\"\"Creates a LargeRescueMDP(), sets the correct_transition_probability to\n",
    "    .76 and returns the optimal value function.\n",
    "\n",
    "    Args:\n",
    "      None\n",
    "\n",
    "    Returns:\n",
    "      value function: a dict of states to values\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e55c18e",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7219d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vi_test(function, V):\n",
    "  import random; random.seed(0)\n",
    "  import numpy.random as npr; npr.seed(0)\n",
    "  Vsub = function()\n",
    "  assert check_value_function(Vsub, V)\n",
    "\n",
    "vi_test(MDP_2, {(4, 0): 17.193656448413645, (3, 4): 82.66821248865962, (4, 9): 33.24527236077266, (3, 7): 48.38047730522361, (5, 4): 81.57596106315178, (8, 0): 29.485520779767217, (0, 2): 39.54500010633741, (8, 3): 46.729294692291475, (8, 9): 34.81853504895561, (0, 5): 57.71031630612815, (2, 2): 28.67493693106001, (1, 0): 25.425642667196094, (1, 6): 58.371006293552576, (0, 8): 38.3562257828573, (2, 5): 79.50785163099987, (8, 6): 51.83291668257821, (2, 8): 48.37609286765003, (7, 4): 60.490236730134086, (7, 1): 29.15147584428534, (7, 7): 51.413594478870216, (6, 5): 78.66538264988357, (6, 8): 49.519712872964426, (4, 2): 21.29292539099053, (3, 0): 19.184941573721872, (4, 5): 0.0, (3, 9): 36.70592766228915, (5, 0): 19.36445038655641, (4, 8): 37.33611818768401, (5, 6): 80.39156750674351, (5, 3): 70.35461243079723, (5, 9): 37.09563198565326, (8, 2): 40.21565223450838, (8, 5): 57.16960149114739, (0, 1): 33.657954212109814, (0, 7): 44.47885137481426, (2, 4): 71.94570365595851, (1, 2): 33.63013421547188, (0, 4): 54.116379728735176, (2, 1): 25.397822670558156, (2, 7): 56.82786088085701, (1, 5): 67.23721597706378, (8, 8): 39.50173149602546, (6, 1): 25.180061683772845, (7, 0): 25.734404278265206, (6, 4): 70.24275282935724, (7, 3): 53.62383581844224, (7, 9): 38.68727231509817, (6, 7): 58.8166743393827, (7, 6): 59.54114719666043, (3, 2): 24.704121310787116, (4, 1): 18.46056909239355, (4, 7): 41.79958505037646, (3, 5): 94.47917738184584, (4, 4): 94.63963415294967, (3, 8): 41.906350224012456, (5, 5): 93.32556960074426, (8, 4): 52.64659733120905, (0, 0): 29.033502757353006, (5, 8): 42.2743189133091, (8, 1): 34.191129369373066, (1, 1): 29.10269089376744, (0, 3): 46.568742068171915, (0, 9): 33.46384586176194, (2, 0): 22.19943115828667, (1, 4): 62.444310480187774, (0, 6): 50.95192651060614, (8, 7): 45.25820887982636, (2, 9): 41.7430747582296, (1, 7): 50.88346010072386, (2, 6): 67.54061291729131, (6, 0): 22.424055347153732, (6, 6): 69.17350279458515, (7, 5): 66.56559260006958, (6, 3): 61.46654014072949, (6, 9): 42.379150021338276, (7, 8): 44.3619253645486})\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503276dc",
   "metadata": {},
   "source": [
    "## MDP Question 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28886859",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "The default temporal_discount_factor is .9. This is in general quite a low\n",
    "discount factor. An action 20 steps away will have $.9^{20} \\approx .12$ impact on later\n",
    "actions.\n",
    "\n",
    "What happens if we increase the discount factor? Please create a\n",
    "LargeRescueMDP, set the temporal_discount_factor to be .99, and also set\n",
    "the correct_transition_probability to be .76. Then please solve for the\n",
    "optimal value function.\n",
    "\n",
    "\n",
    "For reference, our solution is **6** line(s) of code.\n",
    "\n",
    "In addition to all the utilities defined at the top of the Colab notebook, the following functions are available in this question environment: `bellman_backup`, `value_iteration`. You may not need to use all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019c2364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDP_3():\n",
    "    \"\"\"Creates a LargeRescueMDP(), sets the correct_transition_probability to\n",
    "    .76 and the temporal_discount_factor to .99 and returns the optimal value function.\n",
    "\n",
    "    Args:\n",
    "      None\n",
    "\n",
    "    Returns:\n",
    "      value function: a dict of states to values\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada23f18",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a3b754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vi_test(function, V):\n",
    "  import random; random.seed(0)\n",
    "  import numpy.random as npr; npr.seed(0)\n",
    "  Vsub = function()\n",
    "  assert check_value_function(Vsub, V)\n",
    "\n",
    "vi_test(MDP_3, {(4, 0): 84.0088571825647, (3, 4): 98.08785113514071, (4, 9): 89.54219638658633, (3, 7): 92.85684581202082, (5, 4): 97.92940667071218, (8, 0): 88.55907148092525, (0, 2): 91.15195158169959, (8, 3): 92.71010661836499, (8, 9): 90.02820552034618, (0, 5): 94.57557687282643, (2, 2): 88.2596929505786, (1, 0): 87.24220975351437, (1, 6): 94.66146525243018, (0, 8): 90.84234985893639, (2, 5): 97.64083406000782, (8, 6): 93.58873399735727, (2, 8): 92.85540170289251, (7, 4): 95.05380837452687, (7, 1): 88.4404389970416, (7, 7): 93.49470937074302, (6, 5): 97.51833426545349, (6, 8): 93.09749758158448, (4, 2): 85.70758382182571, (3, 0): 84.8709760689106, (4, 5): 0.0, (3, 9): 90.3793968171442, (5, 0): 84.979923830497, (4, 8): 90.54408402590663, (5, 6): 97.7688940677295, (5, 3): 96.51303468508517, (5, 9): 90.49294151104122, (8, 2): 91.34026747888913, (8, 5): 94.47412332570079, (0, 1): 89.68812885428089, (0, 7): 92.17514466317554, (2, 4): 96.75361408485887, (1, 2): 89.67672656160515, (0, 4): 94.06459700794483, (2, 1): 87.23080746083865, (2, 7): 94.36773185735558, (1, 5): 96.0088117943946, (8, 8): 91.131398346243, (6, 1): 87.17982482641227, (7, 0): 87.385190483955, (6, 4): 96.47708368340508, (7, 3): 93.97491750094179, (7, 9): 90.91508992377925, (6, 7): 94.7388560346764, (7, 6): 94.86949260973816, (3, 2): 86.97078685302058, (4, 1): 84.52926790610293, (4, 7): 91.53586068943369, (3, 5): 99.37197204832822, (4, 4): 99.39679280725164, (3, 8): 91.54877634429582, (5, 5): 99.22275748726061, (8, 4): 93.77006553865067, (0, 0): 88.39225620136497, (5, 8): 91.64279788012988, (8, 1): 89.85966251042251, (1, 1): 88.40199915122935, (0, 3): 92.67280996413947, (0, 9): 89.65081472852465, (2, 0): 86.08978494897735, (1, 4): 95.40750777342734, (0, 6): 93.41218842117453, (8, 7): 92.35290042089164, (2, 9): 91.51907133407133, (1, 7): 93.39431118581003, (2, 6): 96.04265401195448, (6, 0): 86.21161978985722, (6, 6): 96.30174912325705, (7, 5): 95.897070647761, (6, 3): 95.24349019068275, (6, 9): 91.67470906173104, (7, 8): 92.13682235192404})\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5772d4",
   "metadata": {},
   "source": [
    "## MDP Question 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73704382",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Now let's make the dynamics really noisy. Once again, please create a\n",
    "LargeRescueMDP with a hazard at `{(1, 4)}', but with\n",
    "correct_transition_probability set to 0.5. You should see a very substantial\n",
    "change in both the policy and value function.\n",
    "\n",
    "\n",
    "For reference, our solution is **6** line(s) of code.\n",
    "\n",
    "In addition to all the utilities defined at the top of the Colab notebook, the following functions are available in this question environment: `bellman_backup`, `value_iteration`. You may not need to use all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5204245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDP_4():\n",
    "    \"\"\"Creates a LargeRescueMDP(), and sets a hazard at `{(1, 4)}' and\n",
    "    correct_transition_probability to 0.5 and returns\n",
    "    the optimal value function.\n",
    "\n",
    "    Args:\n",
    "      None\n",
    "\n",
    "    Returns:\n",
    "      value function: a dict of states to values\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a495cbec",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1640553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vi_test(function, V):\n",
    "  import random; random.seed(0)\n",
    "  import numpy.random as npr; npr.seed(0)\n",
    "  Vsub = function()\n",
    "  assert check_value_function(Vsub, V)\n",
    "\n",
    "vi_test(MDP_4, {(4, 0): 3.9085329993652533, (3, 4): 58.805202984216365, (4, 9): 11.753646222356124, (3, 7): 14.723408455358632, (5, 4): 61.773838936434885, (8, 0): 10.878573027487132, (0, 2): -0.20781732009289544, (8, 3): 25.36339749449463, (8, 9): 15.238821727444563, (0, 5): 3.0643671550462237, (2, 2): 1.3841713034879826, (1, 0): 1.5389420673207934, (1, 6): 17.433410439493713, (0, 8): 9.345469276586272, (2, 5): 41.64410517348415, (8, 6): 27.34061864611354, (2, 8): 14.623495028538457, (7, 4): 36.3058589431468, (7, 1): 10.407323848359, (7, 7): 26.517051201155294, (6, 5): 53.88967949297427, (6, 8): 22.684327610380464, (4, 2): 2.317116216835473, (3, 0): 2.9670510404427017, (4, 5): 0.0, (3, 9): 10.690151502522395, (5, 0): 5.390778213003905, (4, 8): 13.032557691482545, (5, 6): 58.417081668751784, (5, 3): 48.1633084382923, (5, 9): 14.293773308570756, (8, 2): 19.299393310263866, (8, 5): 30.829638790982713, (0, 1): 0.8020744337498172, (0, 7): 11.988658922265932, (2, 4): 16.3730627200805, (1, 2): 0.9073556276657573, (0, 4): -21.315413465306385, (2, 1): 1.5783110450404056, (2, 7): 19.59916846774346, (1, 5): 3.022657227097599, (8, 8): 18.15310792716475, (6, 1): 8.20816411897359, (7, 0): 8.845736779684062, (6, 4): 47.243646625788344, (7, 3): 31.763122639369513, (7, 9): 16.655193698431397, (6, 7): 32.12324354829552, (7, 6): 33.91437634728944, (3, 2): 1.7861164466125632, (4, 1): 3.0091125874058258, (4, 7): 12.257729404123905, (3, 5): 76.54987697070835, (4, 4): 80.10218352076012, (3, 8): 12.377457524096181, (5, 5): 76.11208979327662, (8, 4): 29.13716588731284, (0, 0): 1.1610960979976075, (5, 8): 16.831639699010523, (8, 1): 13.973656429560908, (1, 1): 1.1974311585861637, (0, 3): -4.701281693660509, (0, 9): 7.646274417336397, (2, 0): 2.1208429059739875, (1, 4): -14.16789428451299, (0, 6): 11.885859730077536, (8, 7): 22.357412587191533, (2, 9): 11.691554230957815, (1, 7): 15.568152460906626, (2, 6): 28.582035217246688, (6, 0): 7.082840364895587, (6, 6): 44.27678096369422, (7, 5): 39.40783614141224, (6, 3): 39.44059244534516, (6, 9): 17.470929083719025, (7, 8): 20.556565474493652})\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5989deb",
   "metadata": {},
   "source": [
    "## Value from Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf85bbe",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13589e2",
   "metadata": {},
   "source": [
    "### Mountain Car MDP and Regressors\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b99aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "class MountainCar:\n",
    "    '''\n",
    "    Mountain Car MDP.\n",
    "\n",
    "    The state is a tuple of two floats (x, v), denoting the position and \n",
    "    velocity of the car.\n",
    "    '''\n",
    "    def __init__(self, goal_velocity=0, difficulty='hard', rng=np.random.default_rng(42)):\n",
    "        self.difficulty = difficulty\n",
    "        self.min_position = -1.2\n",
    "        self.max_position = 0.6\n",
    "        self.max_speed = 0.07\n",
    "        self.goal_position = 0.5\n",
    "        self.goal_velocity = goal_velocity\n",
    "\n",
    "        self.force = 0.001\n",
    "        self.gravity = 0.0025\n",
    "        self.force_noise = 0.0002\n",
    "\n",
    "        self.low = np.array([self.min_position, -self.max_speed], dtype=np.float32)\n",
    "        self.high = np.array([self.max_position, self.max_speed], dtype=np.float32)\n",
    "        self.actions = (0, 1, 2)          # (left_acc, none, right_acc)\n",
    "        self.discount_factor = 1.0\n",
    "        self.rng = rng\n",
    "\n",
    "        self.time_factor = 10.\n",
    "\n",
    "        self.init_state()\n",
    "\n",
    "\n",
    "    def init_state(self):\n",
    "        if self.difficulty == 'hard':\n",
    "            self.state = (self.rng.uniform(-0.6, -0.4), 0.0)\n",
    "        else:\n",
    "            self.state = (self.rng.uniform(0.0, 0.5), 0.0)\n",
    "        return self.state\n",
    "\n",
    "    def terminal(self, s):\n",
    "        position, velocity = s\n",
    "        return bool(position >= self.goal_position and velocity >= self.goal_velocity)\n",
    "\n",
    "    def sim_transition(self, action: int):\n",
    "        '''\n",
    "        Args:\n",
    "        - action : {0, 1, 2}, indicating left, none, right\n",
    "        Returns:\n",
    "        - reward : float\n",
    "        - state : (x_position : float, velocity : float)\n",
    "        '''\n",
    "        position, velocity = self.state\n",
    "        velocity += ((action - 1) * self.force + np.cos(3 * position) * (-self.gravity) + self.rng.normal(scale=self.force_noise)) * self.time_factor\n",
    "        velocity = np.clip(velocity, -self.max_speed, self.max_speed)\n",
    "        position += velocity * self.time_factor\n",
    "        position = np.clip(position, self.min_position, self.max_position)\n",
    "        if position == self.min_position and velocity < 0:\n",
    "            velocity = 0\n",
    "        reward = -1.0 \n",
    "\n",
    "        self.state = (position, velocity)\n",
    "\n",
    "        return reward, self.state\n",
    "\n",
    "\n",
    "    def sim_episode(self, policy, max_iters = 20):\n",
    "        traj = []\n",
    "        s = self.init_state()\n",
    "        for i in range(max_iters):\n",
    "            if self.terminal(s):\n",
    "                for a in self.actions:\n",
    "                    traj.append((s, a, 0, s))\n",
    "                return traj\n",
    "            a = policy(s)\n",
    "            (r, s_prime) = self.sim_transition(a)\n",
    "            traj.append((s, a, r, s_prime))\n",
    "            s = s_prime\n",
    "        return traj\n",
    "\n",
    "    def evaluate(self, n_play, traj_length, policy):\n",
    "        score = 0\n",
    "        for i in range(n_play):\n",
    "            score += sum(x[2] for x in self.sim_episode(policy=policy, max_iters=traj_length)) # reward\n",
    "        return score/n_play\n",
    "\n",
    "\n",
    "class QFRegressor:\n",
    "    def __init__(self, mdp, rng=np.random.default_rng(42)):\n",
    "        self.mdp = mdp\n",
    "        self.fitted = False\n",
    "        self.rng = rng\n",
    "    def fq_q_value(self, s, a):\n",
    "        raise NotImplementedError('Override me')\n",
    "\n",
    "    def fq_value(self, s):\n",
    "        return compute_value_from_q(self.mdp.actions, self.fq_q_value, s)\n",
    "\n",
    "    def fq_greedy(self, s):\n",
    "        if not self.fitted:\n",
    "            return self.rng.choice(self.mdp.actions)\n",
    "        return greedy_policy_from_q(self.mdp.actions, self.fq_q_value, s)\n",
    "\n",
    "    def fq_epsilon_greedy(self, s, eps):\n",
    "        if not self.fitted:\n",
    "            return self.rng.choice(self.mdp.actions)\n",
    "        return epsilon_greedy_policy_from_q(self.mdp.actions, self.fq_q_value, s, eps, self.rng)\n",
    "\n",
    "class NeuralRegressor(QFRegressor):\n",
    "    def initialize(self, max_iter=1000, hidden_layer_sizes=(40,40)):\n",
    "        self.fq_models = {\n",
    "            a: MLPRegressor(hidden_layer_sizes=hidden_layer_sizes,\n",
    "                            max_iter=max_iter, learning_rate_init=0.03)\n",
    "            for a in self.mdp.actions\n",
    "        }\n",
    "        self.fitted = False\n",
    "    def fq_q_value(self, s, a):\n",
    "        return self.fq_models[a].predict(np.array(s).reshape(1,-1))\n",
    "\n",
    "    def fit(self, a, X, Y):\n",
    "        self.fq_models[a].fit(X, Y)\n",
    "\n",
    "class KNNRegressor(QFRegressor):\n",
    "    def initialize(self, n_neighbors=3):\n",
    "        self.fq_models = {\n",
    "            a: KNeighborsRegressor(n_neighbors=n_neighbors)\n",
    "            for a in self.mdp.actions\n",
    "        }\n",
    "        self.fitted = False\n",
    "\n",
    "    def fq_q_value(self, s, a):\n",
    "        return self.fq_models[a].predict(np.array(s).reshape(1,-1))\n",
    "\n",
    "    def fit(self, a, X, Y):\n",
    "        self.fq_models[a].fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e8fbad",
   "metadata": {},
   "source": [
    "### Question\n",
    "Compute the value function from the Q function.\n",
    "\n",
    "For reference, our solution is **3** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff13c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_value_from_q(action_space, q_function, state):\n",
    "    \"\"\"Given the action space, a q_function and a state, compute the value \n",
    "    for this state using the q_function.\n",
    "    Args:\n",
    "    - action_space : tuple of actions\n",
    "    - q_function : (state, action) -> q_value : float\n",
    "    - state : state\n",
    "    Returns:\n",
    "    - value : float - the value of this state\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a08fbf",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250b14f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_from_q_problem_test():\n",
    "    actions = (0, 1, 2)\n",
    "    action_scores = [-3, 1, 5.]\n",
    "    test_state = 3\n",
    "    def q(state, action):\n",
    "        assert(state == test_state)\n",
    "        assert action in actions\n",
    "        return action_scores[action]\n",
    "    def rotate(arr):\n",
    "        arr.append(arr[0])\n",
    "        arr.pop(0)\n",
    "    assert(compute_value_from_q(actions, q, test_state) == 5.)\n",
    "    rotate(action_scores)\n",
    "    assert(compute_value_from_q(actions, q, test_state) == 5.)\n",
    "    rotate(action_scores)\n",
    "    assert(compute_value_from_q(actions, q, test_state) == 5.)\n",
    "\n",
    "value_from_q_problem_test()\n",
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e031395",
   "metadata": {},
   "source": [
    "## Greedy Policy from Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc2e1a9",
   "metadata": {},
   "source": [
    "### Question\n",
    "Write the greedy policy given a Q function.\n",
    "\n",
    "For reference, our solution is **6** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f92f262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy_from_q(action_space, q_function, state):\n",
    "    \"\"\"Given the action space, a q_function and a state, compute the action\n",
    "    taken by the greedy policy at this state, under the q_function.\n",
    "\n",
    "    Args:\n",
    "    - action_space : tuple of actions\n",
    "    - q_function : (state, action) -> q_value : float\n",
    "    - state : state\n",
    "\n",
    "    Return:\n",
    "    - action\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f7dc7e",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fe9457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy_from_q_test():\n",
    "    actions = (0, 1, 2)\n",
    "    action_scores = [-3, 1, 5.]\n",
    "    test_state = 3\n",
    "    def q(state, action):\n",
    "        assert(state == test_state)\n",
    "        assert action in actions\n",
    "        return action_scores[action]\n",
    "    def rotate(arr):\n",
    "        arr.append(arr[0])\n",
    "        arr.pop(0)\n",
    "    assert(greedy_policy_from_q(actions, q, test_state) == 2)\n",
    "    rotate(action_scores)\n",
    "    assert(greedy_policy_from_q(actions, q, test_state) == 1)\n",
    "    rotate(action_scores)\n",
    "    assert(greedy_policy_from_q(actions, q, test_state) == 0)\n",
    "\n",
    "greedy_policy_from_q_test()\n",
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6361853a",
   "metadata": {},
   "source": [
    "## Epsilon Greedy Policy from Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dc54c4",
   "metadata": {},
   "source": [
    "### Question\n",
    "Write the epsilon greedy policy given a Q function. With probability epsilon, the policy should pick a random action, and with probability 1-epsilon, pick the greedy action. You may use the `greed_policy_from_q` function. You may find `rng.choice` and `rng.random` useful.\n",
    "\n",
    "For reference, our solution is **6** line(s) of code.\n",
    "\n",
    "In addition to all the utilities defined at the top of the Colab notebook, the following functions are available in this question environment: `greedy_policy_from_q`. You may not need to use all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daa41eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy_from_q(action_space, q_function, state, eps, rng):\n",
    "    \"\"\"Given the action space, a q_function and a state, compute the action\n",
    "    taken by an epsilon-greedy policy at this state, under the q_function.\n",
    "\n",
    "    Args:\n",
    "    - action_space : tuple of actions\n",
    "    - q_function : (state, action) -> q_value : float\n",
    "    - state : state\n",
    "    - eps : [0, 1]\n",
    "    - rng : np.random.Generator\n",
    "\n",
    "    Return:\n",
    "    - action\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be88a454",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd1910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy_from_q_test():\n",
    "    actions = (0, 1, 2)\n",
    "    action_scores = [-3, 1, 5.]\n",
    "    test_state = 3\n",
    "    rng = np.random.default_rng(42)\n",
    "    eps = 0.5\n",
    "    N = 1000\n",
    "    def q(state, action):\n",
    "        assert(state == test_state)\n",
    "        assert action in actions\n",
    "        return action_scores[action]\n",
    "    def rotate(arr):\n",
    "        arr.append(arr[0])\n",
    "        arr.pop(0)\n",
    "    runs = [epsilon_greedy_policy_from_q(actions, q, test_state, eps, rng) for _ in range(N)]\n",
    "    counts = [sum([1 for j in runs if j == i]) for i in actions]\n",
    "    def binary_std(p):\n",
    "        return (p * (1-p) * N)**.5\n",
    "    assert(np.abs(counts[0] - N*eps/3) < binary_std(eps/3) * 3)\n",
    "    assert(np.abs(counts[1] - N*eps/3) < binary_std(eps/3) * 3)\n",
    "    assert(np.abs(counts[2] - N*eps/3 - N * (1-eps)) < binary_std(eps/3) * 3)\n",
    "    assert(sum(counts) == N)\n",
    "\n",
    "epsilon_greedy_policy_from_q_test()\n",
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec88e7e3",
   "metadata": {},
   "source": [
    "## Sampling points in grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659f2071",
   "metadata": {},
   "source": [
    "### Question\n",
    "Next, we'd want to sample points in the state space on which to perform Bellman backups. Write a function that samples the state space in an evenly spaced grid, and return the sampled points and their corresponding transitions.\n",
    "\n",
    "For reference, our solution is **18** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a447deba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_grid_points(x_divisions, v_divisions, mdp):\n",
    "    \"\"\"Samples x_divisons times v_divisions points in a grid across \n",
    "    the state space, defined by [mdp.min_position, mdp.max_position] x\n",
    "    [-mdp.max_speed, mdp.max_speed]. Then, sample their next transitions using the mdp.sim_transition method. Returns a list of tuples, each \n",
    "    tuple describing the sampled state and its transition.\n",
    "\n",
    "    Args:\n",
    "    - mdp : MountainCar\n",
    "    Return:\n",
    "    - memory : [tuple (state, action, reward, next_state)]\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cfff3f",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c9733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_grid_points_test():\n",
    "    mc = MountainCar()\n",
    "    mc.force_noise = 0.\n",
    "    results = sample_grid_points(3, 4, mc)\n",
    "    my_results = [((-1.2, -0.07), 0, -1.0, (-1.2, 0)), ((-1.2, -0.07), 1, -1.0, (-1.2, 0)), ((-1.2, -0.07), 2, -1.0, (-1.2, 0)), ((-1.2, -0.023333333333333338), 0, -1.0, (-1.2, 0)), ((-1.2, -0.023333333333333338), 1, -1.0, (-1.2, 0)), ((-1.2, -0.023333333333333338), 2, -1.0, (-1.1091437292497965, 0.009085627075020343)), ((-1.2, 0.02333333333333333), 0, -1.0, (-0.8424770625831298, 0.035752293741687015)), ((-1.2, 0.02333333333333333), 1, -1.0, (-0.7424770625831298, 0.04575229374168702)), ((-1.2, 0.02333333333333333), 2, -1.0, (-0.6424770625831299, 0.05575229374168701)), ((-1.2, 0.07), 0, -1.0, (-0.4999999999999999, 0.07)), ((-1.2, 0.07), 1, -1.0, (-0.4999999999999999, 0.07)), ((-1.2, 0.07), 2, -1.0, (-0.4999999999999999, 0.07)), ((-0.30000000000000004, -0.07), 0, -1.0, (-1.0, -0.07)), ((-0.30000000000000004, -0.07), 1, -1.0, (-1.0, -0.07)), ((-0.30000000000000004, -0.07), 2, -1.0, (-1.0, -0.07)), ((-0.30000000000000004, -0.023333333333333338), 0, -1.0, (-0.7887358254009995, -0.04887358254009995)), ((-0.30000000000000004, -0.023333333333333338), 1, -1.0, (-0.6887358254009995, -0.03887358254009995)), ((-0.30000000000000004, -0.023333333333333338), 2, -1.0, (-0.5887358254009996, -0.028873582540099946)), ((-0.30000000000000004, 0.02333333333333333), 0, -1.0, (-0.32206915873433284, -0.002206915873433281)), ((-0.30000000000000004, 0.02333333333333333), 1, -1.0, (-0.22206915873433283, 0.007793084126566721)), ((-0.30000000000000004, 0.02333333333333333), 2, -1.0, (-0.12206915873433283, 0.017793084126566723)), ((-0.30000000000000004, 0.07), 0, -1.0, (0.1445975079323339, 0.044459750793233395)), ((-0.30000000000000004, 0.07), 1, -1.0, (0.24459750793233392, 0.0544597507932334)), ((-0.30000000000000004, 0.07), 2, -1.0, (0.3445975079323339, 0.06445975079323339)), ((0.6, -0.07), 0, -1.0, (-0.10000000000000009, -0.07)), ((0.6, -0.07), 1, -1.0, (-0.04319947632672838, -0.06431994763267283)), ((0.6, -0.07), 2, -1.0, (0.0568005236732716, -0.05431994763267284)), ((0.6, -0.023333333333333338), 0, -1.0, (0.3234671903399383, -0.027653280966006166)), ((0.6, -0.023333333333333338), 1, -1.0, (0.42346719033993835, -0.017653280966006164)), ((0.6, -0.023333333333333338), 2, -1.0, (0.5234671903399384, -0.007653280966006166)), ((0.6, 0.02333333333333333), 0, 0.0, (0.6, 0.02333333333333333)), ((0.6, 0.02333333333333333), 1, 0.0, (0.6, 0.02333333333333333)), ((0.6, 0.02333333333333333), 2, 0.0, (0.6, 0.02333333333333333)), ((0.6, 0.07), 0, 0.0, (0.6, 0.07)), ((0.6, 0.07), 1, 0.0, (0.6, 0.07)), ((0.6, 0.07), 2, 0.0, (0.6, 0.07))]\n",
    "    def recur_match(a, b, fn):\n",
    "        if type(a) is tuple:\n",
    "            assert type(b) is tuple\n",
    "            assert len(a) == len(b)\n",
    "            for x, y in zip(a, b):\n",
    "                if not recur_match(x, y, fn):\n",
    "                    False\n",
    "            return True\n",
    "        else:\n",
    "            return fn(a, b)\n",
    "    for r in results:\n",
    "        found = False\n",
    "        for dr in my_results:\n",
    "            if recur_match(dr, r, lambda x, y: np.abs(x - y) < 1e-6):\n",
    "              found = True\n",
    "        assert(found)\n",
    "\n",
    "sample_grid_points_test()\n",
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10ac9ba",
   "metadata": {},
   "source": [
    "## Sampling points from policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8380fe",
   "metadata": {},
   "source": [
    "### Question\n",
    "Another way to sample points is to collect points by rolling out trajectories from a policy. Implement this, using the same output format.\n",
    "\n",
    "For reference, our solution is **8** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdee02b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_policy_points(policy, traj_length, num_traj, mdp):\n",
    "    \"\"\"Produce samples in the state space by rolling out a policy. Use `mdp.sim_episode`\n",
    "    to obtain rollouts.\n",
    "\n",
    "    Args:\n",
    "    - policy : state -> action\n",
    "    - traj_length : int  - length of rollout\n",
    "    - num_traj : int - number of trajectories to rollout\n",
    "    - mdp : MountainCar\n",
    "    Return:\n",
    "    - memory : [tuple (state, action, reward, next_state)]\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89e6828",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163adf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_policy_points_test():\n",
    "    rng = np.random.default_rng(3)\n",
    "    mc = MountainCar(rng=rng)\n",
    "    mc.state = 0.\n",
    "    policy = lambda s : 1+np.sign(s[1])\n",
    "    results = sample_policy_points(policy, 5, 2, mc)\n",
    "    my_results = [((-1.2, -0.07), 0, -1.0, (-1.2, 0)), ((-1.2, -0.07), 1, -1.0, (-1.2, 0)), ((-1.2, -0.07), 2, -1.0, (-1.2, 0)), ((-1.2, -0.023333333333333338), 0, -1.0, (-1.2, 0)), ((-1.2, -0.023333333333333338), 1, -1.0, (-1.2, 0)), ((-1.2, -0.023333333333333338), 2, -1.0, (-1.1091437292497965, 0.009085627075020343)), ((-1.2, 0.02333333333333333), 0, -1.0, (-0.8424770625831298, 0.035752293741687015)), ((-1.2, 0.02333333333333333), 1, -1.0, (-0.7424770625831298, 0.04575229374168702)), ((-1.2, 0.02333333333333333), 2, -1.0, (-0.6424770625831299, 0.05575229374168701)), ((-1.2, 0.07), 0, -1.0, (-0.4999999999999999, 0.07)), ((-1.2, 0.07), 1, -1.0, (-0.4999999999999999, 0.07)), ((-1.2, 0.07), 2, -1.0, (-0.4999999999999999, 0.07)), ((-0.30000000000000004, -0.07), 0, -1.0, (-1.0, -0.07)), ((-0.30000000000000004, -0.07), 1, -1.0, (-1.0, -0.07)), ((-0.30000000000000004, -0.07), 2, -1.0, (-1.0, -0.07)), ((-0.30000000000000004, -0.023333333333333338), 0, -1.0, (-0.7887358254009995, -0.04887358254009995)), ((-0.30000000000000004, -0.023333333333333338), 1, -1.0, (-0.6887358254009995, -0.03887358254009995)), ((-0.30000000000000004, -0.023333333333333338), 2, -1.0, (-0.5887358254009996, -0.028873582540099946)), ((-0.30000000000000004, 0.02333333333333333), 0, -1.0, (-0.32206915873433284, -0.002206915873433281)), ((-0.30000000000000004, 0.02333333333333333), 1, -1.0, (-0.22206915873433283, 0.007793084126566721)), ((-0.30000000000000004, 0.02333333333333333), 2, -1.0, (-0.12206915873433283, 0.017793084126566723)), ((-0.30000000000000004, 0.07), 0, -1.0, (0.1445975079323339, 0.044459750793233395)), ((-0.30000000000000004, 0.07), 1, -1.0, (0.24459750793233392, 0.0544597507932334)), ((-0.30000000000000004, 0.07), 2, -1.0, (0.3445975079323339, 0.06445975079323339)), ((0.6, -0.07), 0, -1.0, (-0.10000000000000009, -0.07)), ((0.6, -0.07), 1, -1.0, (-0.04319947632672838, -0.06431994763267283)), ((0.6, -0.07), 2, -1.0, (0.0568005236732716, -0.05431994763267284)), ((0.6, -0.023333333333333338), 0, -1.0, (0.3234671903399383, -0.027653280966006166)), ((0.6, -0.023333333333333338), 1, -1.0, (0.42346719033993835, -0.017653280966006164)), ((0.6, -0.023333333333333338), 2, -1.0, (0.5234671903399384, -0.007653280966006166)), ((0.6, 0.02333333333333333), 0, 0.0, (0.6, 0.02333333333333333)), ((0.6, 0.02333333333333333), 1, 0.0, (0.6, 0.02333333333333333)), ((0.6, 0.02333333333333333), 2, 0.0, (0.6, 0.02333333333333333)), ((0.6, 0.07), 0, 0.0, (0.6, 0.07)), ((0.6, 0.07), 1, 0.0, (0.6, 0.07)), ((0.6, 0.07), 2, 0.0, (0.6, 0.07))]\n",
    "    def recur_match(a, b, fn):\n",
    "        if type(a) is tuple:\n",
    "            assert type(b) is tuple\n",
    "            assert len(a) == len(b)\n",
    "            for x, y in zip(a, b):\n",
    "                if not recur_match(x, y, fn):\n",
    "                    False\n",
    "            return True\n",
    "        else:\n",
    "            return fn(a, b)\n",
    "    for r in results:\n",
    "        found = False\n",
    "        for dr in my_results:\n",
    "            if recur_match(dr, r, lambda x, y: np.abs(x - y) < 1e-6):\n",
    "              found = True\n",
    "        assert(found)\n",
    "\n",
    "sample_policy_points_test()\n",
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5a9206",
   "metadata": {},
   "source": [
    "## Fitted Q Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a3a45e",
   "metadata": {},
   "source": [
    "### Question\n",
    "Complete the `fitted_Q_learn` function given in the colab notebook. You're now ready to conduct fitted Q learning on the Mountain Car Problem! You may use either sampling method (grid or policy), and either regression method (`KNNRegressor` or `NeuralRegressor`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef443f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython import display as display\n",
    "matplotlib.rc('animation', html='jshtml')\n",
    "\n",
    "def visualize_qf(qf_regressor):\n",
    "    min_x, max_x = qf_regressor.mdp.min_position, qf_regressor.mdp.max_position\n",
    "    min_v, max_v = -qf_regressor.mdp.max_speed, qf_regressor.mdp.max_speed\n",
    "    vf = np.array([\n",
    "          [\n",
    "              qf_regressor.fq_value((x, v))\n",
    "            for x in np.linspace(min_x, max_x, 100)\n",
    "          ]\n",
    "        for v in np.linspace(min_v, max_v, 100)\n",
    "    ])\n",
    "    im = plt.imshow(vf, extent=(min_x, max_x, min_v, max_v), aspect='auto')\n",
    "    plt.colorbar(im)\n",
    "\n",
    "def visualize_traj(traj):\n",
    "    '''\n",
    "    Visualizes a trajectory. Call with the output of MountainCar.sim_episode\n",
    "\n",
    "    Args:\n",
    "    - traj : [tuple (state, action, reward, next_state)]\n",
    "    '''\n",
    "    # based off https://github.com/mpatacchiola/dissecting-reinforcement-learning/blob/master/environments/mountain_car.py#L105\n",
    "    mode = 'jupyter'\n",
    "    file_path='./mountain_car.mp4'\n",
    "    # Plot init\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, autoscale_on=False, xlim=(-1.2, 0.6), ylim=(-1.1, 1.1))\n",
    "    ax.grid(False)  # disable the grid\n",
    "    x_sin = np.linspace(start=-1.2, stop=0.6, num=100)\n",
    "    y_sin = np.sin(3 * x_sin)\n",
    "    # plt.plot(x, y)\n",
    "    ax.plot(x_sin, y_sin)  # plot the sine wave\n",
    "    # line, _ = ax.plot(x, y, 'o-', lw=2)\n",
    "    dot, = ax.plot([], [], 'ro')\n",
    "    time_text = ax.text(0.05, 0.9, '', transform=ax.transAxes)\n",
    "    _position_list = [s[0][0] for s in traj]\n",
    "    _delta_t = .6\n",
    "\n",
    "    def _init():\n",
    "        dot.set_data([], [])\n",
    "        time_text.set_text('')\n",
    "        return dot, time_text\n",
    "\n",
    "    def _animate(i):\n",
    "        x = _position_list[i]\n",
    "        y = np.sin(3 * x)\n",
    "        dot.set_data(x, y)\n",
    "        time_text.set_text(\"Time: \" + str(np.round(i*_delta_t, 1)) + \"s\" + '\\n' + \"Frame: \" + str(i))\n",
    "        return dot, time_text\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, _animate, np.arange(1, len(_position_list)),\n",
    "                                    blit=True, init_func=_init, repeat=True, interval=_delta_t * 1000)\n",
    "\n",
    "    if mode == 'gif':\n",
    "        ani.save(file_path, writer='imagemagick', fps=int(1/_delta_t))\n",
    "    elif mode == 'mp4':\n",
    "        ani.save(file_path, fps=int(1/_delta_t), writer='avconv', codec='libx264')\n",
    "    elif mode == 'jupyter':\n",
    "        video = ani.to_jshtml()\n",
    "        html = display.HTML(video)\n",
    "        display.display(html)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def fitted_Q_learn(mdp, sampler, qf_regressor, iters):\n",
    "    '''\n",
    "    Takes in a MountainCar instance, a sampling method, a fitted q\n",
    "    regression method, and the number of iterations. Runs fitted Q\n",
    "    learning with that many iterations.\n",
    "\n",
    "    Args:\n",
    "    - mdp : MountainCar\n",
    "    - sampler : (mdp : MountainCar) -> memory : [tuple (state, action, reward, next_state)]\n",
    "    - qf_regressor : QFRegressor\n",
    "    - iters : int\n",
    "    '''\n",
    "    PRINT_EPOCH = 6\n",
    "    qf_regressor.initialize()\n",
    "\n",
    "    for it in range(iters):\n",
    "        Xd = dict([(a, []) for a in mdp.actions])\n",
    "        Yd = dict([(a, []) for a in mdp.actions])\n",
    "        memory = sampler(mdp)\n",
    "        for (s, a, r, s_prime) in memory:\n",
    "            if it == 0 or mdp.terminal(s):\n",
    "                # TODO: IMPLEMENT ME\n",
    "                raise NotImplementedError('Set v = something here')\n",
    "            else:\n",
    "                # TODO: IMPLEMENT ME. You may find mdp.discount_factor \n",
    "                # and qf_regressor.fq_value useful.\n",
    "                raise NotImplementedError('Set v = something here')\n",
    "            Xd[a].append(s)\n",
    "            Yd[a].append(np.array([v]))\n",
    "        for a in mdp.actions:\n",
    "            X = np.vstack(Xd[a])\n",
    "            Y = np.vstack(Yd[a])\n",
    "            Y = Y[:, 0]\n",
    "            qf_regressor.fit(a, X, Y)\n",
    "        qf_regressor.fitted = True\n",
    "        print(f'Iteration {it}:  {mdp.evaluate(n_play=10, traj_length=100, policy=qf_regressor.fq_greedy)}')\n",
    "        if it % PRINT_EPOCH == PRINT_EPOCH-1:\n",
    "            visualize_qf(qf_regressor)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28482e5b",
   "metadata": {},
   "source": [
    "This shows an example of running fitted Q learning using KNNRegressor with policy sampling. Try it with different different regressors, sampling methods and sampling parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108dfed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fitted_q_example():\n",
    "    NUM_ITERS = 10\n",
    "    TRAJ_LENGTH = 40\n",
    "    NUM_ROLLOUTS = 20\n",
    "    EPSILON = 0.4\n",
    "    mc = MountainCar()\n",
    "    qf_regressor = KNNRegressor(mc)\n",
    "    sampler = partial(sample_policy_points, lambda s: qf_regressor.fq_epsilon_greedy(s, EPSILON), TRAJ_LENGTH, NUM_ROLLOUTS)\n",
    "    fitted_Q_learn(\n",
    "        mdp=mc, \n",
    "        sampler=sampler,\n",
    "        qf_regressor=qf_regressor,\n",
    "        iters=NUM_ITERS)\n",
    "    print('expected reward =', mc.evaluate(100, 100, qf_regressor.fq_greedy))\n",
    "    visualize_traj(mc.sim_episode(policy=qf_regressor.fq_greedy, max_iters=100))\n",
    "    plt.show()\n",
    "    visualize_qf(qf_regressor)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw08.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
